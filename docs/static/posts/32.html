<!doctype html>
<html lang="ru">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Душный NLP — пост #32</title>
  <meta name="description" content=" Branch-Train-MiX — метод создания MoE-моделей    Сегодня  рассмотрим статью,  в которой предложен метод получения MoE-модели (Mixture-of-Experts) из обычной, dense-модели. Авторы назвали его Branch-T" />
  <link rel="icon" href="../../favicon.ico" sizes="any" />
  <link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32.png" />
  <link rel="apple-touch-icon" href="../../apple-touch-icon.png" />

  <link rel="canonical" href="https://ml-brand.github.io/stuffyNLP/static/posts/32.html" />
  <meta property="og:type" content="article" />
  <meta property="og:title" content="Душный NLP — пост #32" />
  <meta property="og:description" content=" Branch-Train-MiX — метод создания MoE-моделей    Сегодня  рассмотрим статью,  в которой предложен метод получения MoE-модели (Mixture-of-Experts) из обычной, dense-модели. Авторы назвали его Branch-T" />
  <meta property="og:image" content="https://ml-brand.github.io/stuffyNLP/assets/media/thumbs/32_480.webp" />
  <meta property="og:image:alt" content="Душный NLP" />
  <meta property="article:published_time" content="2024-09-06T09:30:24+00:00" />
  <meta property="article:author" content="Душный NLP" />
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:image" content="https://ml-brand.github.io/stuffyNLP/assets/media/thumbs/32_480.webp" />
  <link rel="stylesheet" href="../../style.css" />
  <script src="../../metrika.js"></script>
</head>
<body data-index-href="../page-4.html">
  <header class="header">
    <div class="container">
      <div class="title-grid single-title">
        <a class="grid-avatar" href="#" target="_blank" rel="noopener">
          <img id="channelAvatar" class="channel-avatar" src="../../assets/channel_avatar.jpg" alt="Аватар канала"  />
        </a>
        <div class="grid-main">
          <div class="title-head">
            <a class="back-link" href="../page-4.html">← Ко всем постам</a>
            <a class="badge-chip" id="siteTitleWrap" href="#" target="_blank" rel="noopener"><h1 id="siteTitle">Душный NLP</h1></a>
            <div class="hero-actions">
              <a id="subscribeBtn" class="subscribe-btn" href="https://t.me/+1Z41UptsLwszZDE6" target="_blank" rel="noopener" >Подписаться</a>
              <a class="icon-btn" href="../../post.html?id=32" aria-label="Открыть динамическую страницу поста">↺</a>
              <button id="themeToggle" class="icon-btn" type="button" aria-label="Переключить тему"></button>
            </div>
          </div>
        </div>
      </div>
    </div>
  </header>

  
  <div id="promoBanner" class="promo-banner" hidden>
    <div class="container promo-inner">
      <span class="promo-text"><a href="https://t.me/addlist/5NH3RoVejEI1MGEy">Подпишись на все наши ML каналы. Они классные, отвечаем!</a></span>
      <button id="promoClose" class="promo-close" type="button" aria-label="Скрыть плашку">×</button>
    </div>
  </div>
  

  <main class="container single-page">
    <article id="postContainer" class="post post-page" data-post-id="32">
      <div class="post-header">
        <div class="right"><span class="post-date" data-iso-date="2024-09-06T09:30:24+00:00">2024-09-06 09:30 UTC</span></div>
      </div>
      <div class="post-body"><strong>Branch-Train-MiX — метод создания MoE-моделей </strong><br><br>Сегодня <a href="https://arxiv.org/abs/2403.07816" rel="nofollow noopener noreferrer">рассмотрим статью,</a> в которой предложен метод получения MoE-модели (Mixture-of-Experts) из обычной, dense-модели. Авторы назвали его Branch-Train-MiX (BTX).<br><br>Для начала напомним, что такое MoE. Это архитектурный подход, который предполагает использование в трансформенных слоях нескольких FNN-блоков — экспертов. У каждого из них предположительно есть собственная узкая специализация, поэтому над решением поставленной модели задачи они работают не одновременно. Роутер выбирает, какие эксперты лучше подходят для обработки токенов на каждом конкретном этапе. <br><br>Во время инференса модели, построенные на архитектуре MoE, задействуют меньше параметров, чем их dense-аналоги — и без сильной потери качества. Кроме того, они прекрасно масштабируются. Поэтому понятно стремление превратить обычную модель в MoE. <br><br>В основе идеи авторов лежит метод BTM (Branch-Train-Merge). Суть его заключается в том, чтобы взять несколько одинаковых LLM, параллельно обучить их на разных датасетах, а затем агрегировать предсказания каждой модели во время инференса. <br><br>Рассматриваемая статья предлагает после обучения слить все модели в MoE, чтобы FNN каждой базовой модели стал экспертом в соответствующем слое, и добавить роутер. Веса аттеншена при этом следует усреднить и дообучить. <br><br>Авторы взяли три копии Llama-2 7B и дообучили каждую на своём домене: математика (на тех же данных, что и Llemma), программирование (на тех же данных, что CodeLlama) и общие знания (на «Википедии»). К финальной модели добавили оригинальную Llama-2 7B, не дообученную на чём-то конкретном. Получившуюся MoE-модель файнтюнили на источниках, которые применялись для обучения всех четырёх экспертов. В результате модель не сильно уступает тем же Llemma и CodeLlama в вопросах математики и программирования. <br><br>Интересно и то, что BTX-модели обретают интерпретируемость. Авторы показывают, что в эксперта, обученного на определённом домене, попадают токены из сэмпла этого же домена.<br><br>Рассказывайте в комментариях, что думаете про BTX! <br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Александр Пацация</em><br><br><a href="https://t.me/+eA4YF9UCmLBiMTgy" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../../assets/media/thumbs/32_480.webp" srcset="../../assets/media/thumbs/32_480.webp 480w, ../../assets/media/32.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="32" data-image-index="0" /></div></div>
      <div class="actions">
        <span>5 243 просмотров · 42 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/32" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="../page-4.html">К списку постов</a> · <a href="./32.html">Ссылка на этот пост</a></span>
      </div>
    </article>

    <div class="pager single-nav">
      <a id="prevPost" class="nav-link" href="./33.html" style="visibility:visible">← Более новый</a>
      <a id="nextPost" class="nav-link" href="./30.html" style="visibility:visible">Более старый →</a>
    </div>
  </main>

  <footer class="footer">
    <div class="container">
      <div class="footer-inner">
        <span>based on <a href="https://github.com/ml-brand/tg-to-gh-pages" target="_blank" rel="noopener">tg-to-gh-pages</a> (created by <a href="https://github.com/ml-brand" target="_blank" rel="noopener">ML Brand</a>)</span>
        <a id="repoLink" href="https://github.com/ml-brand/tg-to-gh-pages" target="_blank" rel="noopener">Do the same with your channel.</a>
        <span class="footer-links">
          static copy ·
          <a href="../../feed.xml" target="_blank" rel="noopener">RSS</a> ·
          <a href="../../atom.xml" target="_blank" rel="noopener">Atom</a>
        </span>
      </div>
    </div>
  </footer>

  <script>
    window.__STATIC_POSTS = [{"id": 32, "media": [{"kind": "photo", "path": "../../assets/media/32.jpg", "thumb": "../../assets/media/thumbs/32_480.webp", "size": 103779, "mime": "image/jpeg", "name": null}]}];
    window.__STATIC_META = {"title": "Душный NLP", "username": "stuffyNLP", "channel": "stuffyNLP", "last_sync_utc": "2026-02-10T06:02:41Z", "posts_count": 113, "last_seen_message_id": 225, "stats": {"new": 105, "updated": 2, "media_downloaded": 105}, "avatar": "assets/channel_avatar.jpg", "meta_schema_version": "1.0.0", "posts_schema_version": "1.0.0"};
  </script>
  <script src="../../common.js"></script>
  <script src="../../static.js"></script>
</body>
</html>
