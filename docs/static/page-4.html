<!doctype html>
<html lang="ru">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Душный NLP — статическая версия (стр. 4/4)</title>
  <meta name="description" content="Статическая версия зеркала Telegram-канала" />
  <link rel="icon" href="../favicon.ico?v=2026-02-20T17%3A24%3A27Z" sizes="any" />
  <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32.png?v=2026-02-20T17%3A24%3A27Z" />
  <link rel="apple-touch-icon" href="../apple-touch-icon.png?v=2026-02-20T17%3A24%3A27Z" />

  <link rel="stylesheet" href="../style.css" />
  <script src="../metrika.js"></script>
</head>
<body>
  <header class="header">
    <div class="container">
      <div class="title-grid">
        <a class="grid-avatar" href="#" target="_blank" rel="noopener">
          <img id="channelAvatar" class="channel-avatar" src="../assets/channel_avatar.jpg" alt="Аватар канала"  />
        </a>
        <div class="grid-main">
          <div class="title-head">
            <div class="title-left">
              <a class="badge-chip" id="siteTitleWrap" href="#" target="_blank" rel="noopener"><h1 id="siteTitle">Душный NLP</h1></a>
            </div>
            <div class="hero-actions">
              <a id="subscribeBtn" class="subscribe-btn" href="https://t.me/+1Z41UptsLwszZDE6" target="_blank" rel="noopener" >Подписаться</a>
              <a class="icon-btn" href="../" aria-label="Перейти к динамической версии">↺</a>
              <button id="themeToggle" class="icon-btn" type="button" aria-label="Переключить тему"></button>
            </div>
          </div>
        </div>
        <div class="controls"></div>
      </div>
    </div>
  </header>

  
  <div id="promoBanner" class="promo-banner" hidden>
    <div class="container promo-inner">
      <span class="promo-text"><a href="https://t.me/addlist/5NH3RoVejEI1MGEy">Подпишись на все наши ML каналы. Они классные, отвечаем!</a></span>
      <button id="promoClose" class="promo-close" type="button" aria-label="Скрыть плашку">×</button>
    </div>
  </div>
  

  <main class="container">
    
    <div class="pager static-pager" style="justify-content:center">
      <div class="page-links">
        <a class="nav-link" href="page-3.html">←</a>
        <a class="page-link" href="index.html">1</a> <a class="page-link" href="page-2.html">2</a> <a class="page-link" href="page-3.html">3</a> <a class="page-link current" href="page-4.html">4</a>
        <a class="nav-link disabled" href="#">→</a>
      </div>
    </div>
    
    <div id="posts" class="posts">
      
    <article class="post" data-post-id="38" data-search="архитектура llama 3.1 продолжаем разбирать llama 3.1. в прошлый раз речь шла о претрейн-датасете, а в этот раз — об архитектуре модели. llama 3 использует стандартную архитектуру трансформера, которая не сильно отличается от того, что было в llama и llama 2. однако отличия есть. скажем, если в llama 2 grouped query attention (gqa) с восемью ключевыми головами внимания использовались только в моделях на 34b+, то здесь gqa применяется для всех моделей llama 3.1. это позволило повысить скорость вывода и уменьшить объём данных, необходимых для кеширования во время декодирования. ещё одно важное изменение — увеличение контекстного окна до 128 тысяч токенов. это стало возможным благодаря увеличению гиперпараметра базовой частоты rope до 500 тысяч. такой подход позволяет модели эффективно решать задачи, связанные с большими объёмами текстов. модель также использует словарь на 128 тысяч токенов. разработчики внедрили четырёхмерный параллелизм (4d parallelism), который включает тензорный, пайплайновый, контекстный и параллелизм данных. этот подход позволяет значительно улучшить утилизацию ресурсов при обучении на тысячах gpu. например, для обучения модели с 405 миллиардами параметров использовалось до 16 тысяч gpu, а средняя утилизация вычислительных ресурсов составила около 41%​. контекстный параллелизм позволяет разбивать длинные строки на части. в отличие от классических методов, такой параллелизм синхронизирует только ключи и значения в attention-слое, что минимизирует задержки при обработке длинных последовательностей. помимо этого, в архитектуре llama 3.1 активно используется fp8-квантизация, которая значительно ускоряет вычисления без значительных потерь в точности. это позволяет экономить до 50% времени на вычисления по сравнению с традиционными методами, что критично для моделей с миллиардами параметров. fp8-квантизация используется не для всех слоев, потому что она может вызвать ошибки при вычислении в attention-слоях. подход доказал свою эффективность при решении большинства задач​. разбор подготовил ❣ михаил хрущев душный nlp архитектура llama 3.1 продолжаем разбирать llama 3.1. в прошлый раз речь шла о претрейн-датасете , а в этот раз — об архитектуре модели. llama 3 использует стандартную архитектуру трансформера, которая не сильно отличается от того, что было в llama и llama 2. однако отличия есть. скажем, если в llama 2 grouped query attention (gqa) с восемью ключевыми головами внимания использовались только в моделях на 34b+, то здесь gqa применяется для всех моделей llama 3.1. это позволило повысить скорость вывода и уменьшить объём данных, необходимых для кеширования во время декодирования. ещё одно важное изменение — увеличение контекстного окна до 128 тысяч токенов. это стало возможным благодаря увеличению гиперпараметра базовой частоты rope до 500 тысяч. такой подход позволяет модели эффективно решать задачи, связанные с большими объёмами текстов. модель также использует словарь на 128 тысяч токенов. разработчики внедрили четырёхмерный параллелизм (4d parallelism), который включает тензорный, пайплайновый, контекстный и параллелизм данных. этот подход позволяет значительно улучшить утилизацию ресурсов при обучении на тысячах gpu. например, для обучения модели с 405 миллиардами параметров использовалось до 16 тысяч gpu, а средняя утилизация вычислительных ресурсов составила около 41%​. контекстный параллелизм позволяет разбивать длинные строки на части. в отличие от классических методов, такой параллелизм синхронизирует только ключи и значения в attention-слое, что минимизирует задержки при обработке длинных последовательностей. помимо этого, в архитектуре llama 3.1 активно используется fp8-квантизация, которая значительно ускоряет вычисления без значительных потерь в точности. это позволяет экономить до 50% времени на вычисления по сравнению с традиционными методами, что критично для моделей с миллиардами параметров. fp8-квантизация используется не для всех слоев, потому что она может вызвать ошибки при вычислении в attention-слоях. подход доказал свою эффективность при решении большинства задач​. разбор подготовил ❣ михаил хрущев душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-10-18T10:55:22+00:00" href="./posts/38.html">2024-10-18 10:55 UTC</a></div>
      </div>
      <div class="post-body"><strong>Архитектура LLaMA 3.1</strong><br><br>Продолжаем <a href="https://arxiv.org/abs/2407.21783" rel="nofollow noopener noreferrer">разбирать LLaMA 3.1.</a> В прошлый раз речь шла о <a href="https://t.me/stuffyNLP/37" rel="nofollow noopener noreferrer">претрейн-датасете</a>, а в этот раз — об архитектуре модели. <br><br>Llama 3 использует стандартную архитектуру трансформера, которая не сильно отличается от того, что было в LLaMA и LLaMA 2. Однако отличия есть. Скажем, если в LLaMA 2 Grouped Query Attention (GQA) с восемью ключевыми головами внимания использовались только в моделях на 34B+, то здесь GQA применяется для всех моделей LLaMA 3.1. Это позволило повысить скорость вывода и уменьшить объём данных, необходимых для кеширования во время декодирования.<br><br>Ещё одно важное изменение — увеличение контекстного окна до 128 тысяч токенов. Это стало возможным благодаря увеличению гиперпараметра базовой частоты RoPE до 500 тысяч. Такой подход позволяет модели эффективно решать задачи, связанные с большими объёмами текстов. Модель также использует словарь на 128 тысяч токенов.<br><br>Разработчики внедрили четырёхмерный параллелизм (4D Parallelism), который включает тензорный, пайплайновый, контекстный и параллелизм данных. Этот подход позволяет значительно улучшить утилизацию ресурсов при обучении на тысячах GPU. Например, для обучения модели с 405 миллиардами параметров использовалось до 16 тысяч GPU, а средняя утилизация вычислительных ресурсов составила около 41%​.<br><br>Контекстный параллелизм позволяет разбивать длинные строки на части. В отличие от классических методов, такой параллелизм синхронизирует только ключи и значения в attention-слое, что минимизирует задержки при обработке длинных последовательностей.<br><br>Помимо этого, в архитектуре LLaMA 3.1 активно используется FP8-квантизация, которая значительно ускоряет вычисления без значительных потерь в точности. Это позволяет экономить до 50% времени на вычисления по сравнению с традиционными методами, что критично для моделей с миллиардами параметров. FP8-квантизация используется не для всех слоев, потому что она может вызвать ошибки при вычислении в attention-слоях. Подход доказал свою эффективность при решении большинства задач​.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Михаил Хрущев</em><br><br><a href="https://t.me/+yAHD2sNNZUw0MTRi" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/38_480.webp" srcset="../assets/media/thumbs/38_480.webp 480w, ../assets/media/38.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="38" data-image-index="0" /></div></div>
      <div class="actions">
        <span>5 524 просмотров · 81 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/38" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/38.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="37" data-search="о претрейн-датасете llama 3.1 сегодня расскажем о том, как устроен претрейн-датасет для llama 3.1. разработчики этой llm поделились множеством подробностей, поэтому будет интересно. одна из важнейших характеристик llama 3.1 — объём данных, на которых она обучалась. для этой модели использовался претрейн-датасет из 15 триллионов токенов. это один из самых больших наборов данных для языковых моделей, превосходящий, например, llama 2 с её 2 триллионами токенов. модель также включает специальный набор данных для длинных контекстов. сбор данных для llama 3.1 построен на стандартном пайплайне для обработки текста из html. один из его ключевых аспектов — это фильтрация данных на ранних стадиях. она включает как дедупликацию данных, так и использование эвристических методов для удаления нежелательных токенов. кроме того, команда разработчиков использовала кастомные парсеры для специфических доменов, таких как математика и программирование. это позволило улучшить качество обработки данных для узкоспециализированных задач. кастомный парсер способен сохранить точное форматирование математических формул, тогда как обычные парсеры теряют важную информацию. llama 3.1 демонстрирует хорошие результаты благодаря специальным пайплайнам для математических задач и задач, связанных с программированием. объем данных по коду и математике в датасете llama 3.1 превосходит весь датасет оригинальной llama. для оценки качества данных использовались различные классификаторы, полученные дистилляцией llama 2. другие классификаторы отбирали документы, опираясь на вероятность их принадлежности к определённому домену. один из интересных моментов — использование классификаторов для оценки доменов с целью дальнейшего перевзвешивания различных частей датасета. например, данные из области искусства и развлечений могут быть уменьшены в весе по сравнению с математикой. фильтрация включает несколько уровней дедупликации. во-первых, стандартная дедупликация с использованием метода minhash. есть также локальная дедупликация — метод, при котором данные разбиваются на блоки, и дубликаты удаляются только внутри этих блоков. ещё одной важной частью фильтрации данных стала проверка на безопасность. разработчики llama 3.1 реализовали специальные фильтры для удаления вредоносного или небезопасного контента. эти фильтры основаны на классификаторах и могут удалять не только нежелательные токены, но и данные с персональной информацией. llama 3.1 также выделяется своими экспериментами в области data mixer — процесса, в котором данные разделяются по доменам, а затем перевзвешиваются для оптимальной работы модели. важным этапом оптимизации стал так называемое «сведение». это метод, при котором на последних этапах обучения модель дообучается на малом наборе данных, включающем специфичные задачи. это позволяет значительно повысить качество ответов модели на конечных тестах. в ходе экспериментов установили, что даже небольшой объём высококачественных данных может существенно улучшить результаты. разбор подготовил ❣ дмитрий мокеев душный nlp о претрейн-датасете llama 3.1 сегодня расскажем о том, как устроен претрейн-датасет для llama 3.1 . разработчики этой llm поделились множеством подробностей, поэтому будет интересно. одна из важнейших характеристик llama 3.1 — объём данных, на которых она обучалась. для этой модели использовался претрейн-датасет из 15 триллионов токенов. это один из самых больших наборов данных для языковых моделей, превосходящий, например, llama 2 с её 2 триллионами токенов. модель также включает специальный набор данных для длинных контекстов. сбор данных для llama 3.1 построен на стандартном пайплайне для обработки текста из html. один из его ключевых аспектов — это фильтрация данных на ранних стадиях. она включает как дедупликацию данных, так и использование эвристических методов для удаления нежелательных токенов. кроме того, команда разработчиков использовала кастомные парсеры для специфических доменов, таких как математика и программирование. это позволило улучшить качество обработки данных для узкоспециализированных задач. кастомный парсер способен сохранить точное форматирование математических формул, тогда как обычные парсеры теряют важную информацию. llama 3.1 демонстрирует хорошие результаты благодаря специальным пайплайнам для математических задач и задач, связанных с программированием. объем данных по коду и математике в датасете llama 3.1 превосходит весь датасет оригинальной llama. для оценки качества данных использовались различные классификаторы, полученные дистилляцией llama 2. другие классификаторы отбирали документы, опираясь на вероятность их принадлежности к определённому домену. один из интересных моментов — использование классификаторов для оценки доменов с целью дальнейшего перевзвешивания различных частей датасета. например, данные из области искусства и развлечений могут быть уменьшены в весе по сравнению с математикой. фильтрация включает несколько уровней дедупликации. во-первых, стандартная дедупликация с использованием метода minhash. есть также локальная дедупликация — метод, при котором данные разбиваются на блоки, и дубликаты удаляются только внутри этих блоков. ещё одной важной частью фильтрации данных стала проверка на безопасность. разработчики llama 3.1 реализовали специальные фильтры для удаления вредоносного или небезопасного контента. эти фильтры основаны на классификаторах и могут удалять не только нежелательные токены, но и данные с персональной информацией. llama 3.1 также выделяется своими экспериментами в области data mixer — процесса, в котором данные разделяются по доменам, а затем перевзвешиваются для оптимальной работы модели. важным этапом оптимизации стал так называемое «сведение». это метод, при котором на последних этапах обучения модель дообучается на малом наборе данных, включающем специфичные задачи. это позволяет значительно повысить качество ответов модели на конечных тестах. в ходе экспериментов установили, что даже небольшой объём высококачественных данных может существенно улучшить результаты. разбор подготовил ❣ дмитрий мокеев душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-10-11T13:15:58+00:00" href="./posts/37.html">2024-10-11 13:15 UTC</a></div>
      </div>
      <div class="post-body"><strong>О претрейн-датасете LLaMA 3.1</strong><br><br>Сегодня расскажем о том, как устроен <a href="https://arxiv.org/abs/2407.21783" rel="nofollow noopener noreferrer">претрейн-датасет для LLaMA 3.1</a>. Разработчики этой LLM поделились множеством подробностей, поэтому будет интересно. <br><br>Одна из важнейших характеристик LLaMA 3.1 — объём данных, на которых она обучалась. Для этой модели использовался претрейн-датасет из 15 триллионов токенов. Это один из самых больших наборов данных для языковых моделей, превосходящий, например, LLaMA 2 с её 2 триллионами токенов. Модель также включает специальный набор данных для длинных контекстов.<br><br>Сбор данных для LLaMA 3.1 построен на стандартном пайплайне для обработки текста из HTML. Один из его ключевых аспектов — это фильтрация данных на ранних стадиях. Она включает как дедупликацию данных, так и использование эвристических методов для удаления нежелательных токенов. <br><br>Кроме того, команда разработчиков использовала кастомные парсеры для специфических доменов, таких как математика и программирование. Это позволило улучшить качество обработки данных для узкоспециализированных задач. Кастомный парсер способен сохранить точное форматирование математических формул, тогда как обычные парсеры теряют важную информацию.<br><br>LLaMA 3.1 демонстрирует хорошие результаты благодаря специальным пайплайнам для математических задач и задач, связанных с программированием. Объем данных по коду и математике в датасете LLaMA 3.1 превосходит весь датасет оригинальной LLaMA.<br><br>Для оценки качества данных использовались различные классификаторы, полученные дистилляцией LLaMA 2. Другие классификаторы отбирали документы, опираясь на вероятность их принадлежности к определённому домену. Один из интересных моментов — использование классификаторов для оценки доменов с целью дальнейшего перевзвешивания различных частей датасета. Например, данные из области искусства и развлечений могут быть уменьшены в весе по сравнению с математикой.<br><br>Фильтрация включает несколько уровней дедупликации. Во-первых, стандартная дедупликация с использованием метода MinHash. Есть также локальная дедупликация — метод, при котором данные разбиваются на блоки, и дубликаты удаляются только внутри этих блоков. <br><br>Ещё одной важной частью фильтрации данных стала проверка на безопасность. Разработчики LLaMA 3.1 реализовали специальные фильтры для удаления вредоносного или небезопасного контента. Эти фильтры основаны на классификаторах и могут удалять не только нежелательные токены, но и данные с персональной информацией.<br><br>LLaMA 3.1 также выделяется своими экспериментами в области Data Mixer — процесса, в котором данные разделяются по доменам, а затем перевзвешиваются для оптимальной работы модели. <br><br>Важным этапом оптимизации стал так называемое «сведение». Это метод, при котором на последних этапах обучения модель дообучается на малом наборе данных, включающем специфичные задачи. Это позволяет значительно повысить качество ответов модели на конечных тестах. В ходе экспериментов установили, что даже небольшой объём высококачественных данных может существенно улучшить результаты.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Дмитрий Мокеев</em><br><br><a href="https://t.me/+ufd0OK8SBSE3ZmFi" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/37_480.webp" srcset="../assets/media/thumbs/37_480.webp 480w, ../assets/media/37.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="37" data-image-index="0" /></div></div>
      <div class="actions">
        <span>6 140 просмотров · 63 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/37" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/37.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="36" data-search="remax как альтернатива ppo сегодняшняя статья — об интересном методе обучения с подкреплением, который называется remax. авторы предлагают его как замену популярному алгоритму proximal policy optimization (ppo). remax основывается на алгоритме обучения с подкреплением, который называется reinforce — отсюда и приставка re. в reinforce, в отличие от ppo, нет value-модели. она выступает в роли бейзлайна для снижения дисперсии оценки градиента и представляет собой среднюю награду, которую наберёт генератор, если будет отвечать на конкретный запрос. вместо отдельной value-модели в remax предлагают использовать другой бейзлайн — то, сколько награды набирает greedy-генерация обучаемой моделью на запросе. отсюда окончание max. такой бейзлайн тоже отлично подходит с точки зрения теории, и не требует хранения дополнительной модели в памяти — авторы сообщают, что снижение потребления gpu-памяти составляет 46%. а поскольку число моделей, необходимых алгоритму алайнмента, уменьшилось, то уменьшилось и число гиперпараметров, которые нужно подобрать. использование remax для обучения модели mistral-7b показало значительные улучшения. модель достигла 94,78% успеха на leaderboard alpacaeval и установила новый стандарт для моделей с 7 миллиардами параметров. эти результаты демонстрируют, что remax может стать отличной альтернативой ppo для rlhf-задач, значительно сокращая вычислительные затраты и повышая эффективность обучения крупных языковых моделей. разбор подготовил ❣ павел темирчев душный nlp remax как альтернатива ppo сегодняшняя статья — об интересном методе обучения с подкреплением, который называется remax. авторы предлагают его как замену популярному алгоритму proximal policy optimization (ppo). remax основывается на алгоритме обучения с подкреплением, который называется reinforce — отсюда и приставка re. в reinforce, в отличие от ppo, нет value-модели. она выступает в роли бейзлайна для снижения дисперсии оценки градиента и представляет собой среднюю награду, которую наберёт генератор, если будет отвечать на конкретный запрос. вместо отдельной value-модели в remax предлагают использовать другой бейзлайн — то, сколько награды набирает greedy-генерация обучаемой моделью на запросе. отсюда окончание max. такой бейзлайн тоже отлично подходит с точки зрения теории, и не требует хранения дополнительной модели в памяти — авторы сообщают, что снижение потребления gpu-памяти составляет 46%. а поскольку число моделей, необходимых алгоритму алайнмента, уменьшилось, то уменьшилось и число гиперпараметров, которые нужно подобрать. использование remax для обучения модели mistral-7b показало значительные улучшения. модель достигла 94,78% успеха на leaderboard alpacaeval и установила новый стандарт для моделей с 7 миллиардами параметров. эти результаты демонстрируют, что remax может стать отличной альтернативой ppo для rlhf-задач, значительно сокращая вычислительные затраты и повышая эффективность обучения крупных языковых моделей. разбор подготовил ❣ павел темирчев душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-10-04T08:57:19+00:00" href="./posts/36.html">2024-10-04 08:57 UTC</a></div>
      </div>
      <div class="post-body"><strong>ReMax как альтернатива PPO</strong><br><br>Сегодняшняя статья — об интересном методе обучения с подкреплением, <a href="https://arxiv.org/abs/2310.10505" rel="nofollow noopener noreferrer">который называется ReMax.</a> Авторы предлагают его как замену популярному алгоритму Proximal Policy Optimization (PPO).<br><br>ReMax основывается на алгоритме обучения с подкреплением, который называется REINFORCE — отсюда и приставка Re. В REINFORCE, в отличие от PPO, нет value-модели. Она выступает в роли бейзлайна для снижения дисперсии оценки градиента и представляет собой среднюю награду, которую наберёт генератор, если будет отвечать на конкретный запрос.<br><br>Вместо отдельной value-модели в ReMax предлагают использовать другой бейзлайн — то, сколько награды набирает greedy-генерация обучаемой моделью на запросе. Отсюда окончание Max. Такой бейзлайн тоже отлично подходит с точки зрения теории, и не требует хранения дополнительной модели в памяти — авторы сообщают, что снижение потребления GPU-памяти составляет 46%. А поскольку число моделей, необходимых алгоритму алайнмента, уменьшилось, то уменьшилось и число гиперпараметров, которые нужно подобрать.<br><br>Использование ReMax для обучения модели Mistral-7B показало значительные улучшения. Модель достигла 94,78% успеха на leaderboard AlpacaEval и установила новый стандарт для моделей с 7 миллиардами параметров. Эти результаты демонстрируют, что ReMax может стать отличной альтернативой PPO для RLHF-задач, значительно сокращая вычислительные затраты и повышая эффективность обучения крупных языковых моделей.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Павел Темирчев</em><br><br><a href="https://t.me/+HJcT-kMS_sYwNDJi" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/36_480.webp" srcset="../assets/media/thumbs/36_480.webp 480w, ../assets/media/36.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="36" data-image-index="0" /></div></div>
      <div class="actions">
        <span>12 765 просмотров · 60 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/36" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/36.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="35" data-search="self-rewarding language models в сегодняшней статье — концепция self-rewarding language models. они не только генерируют ответы, но и оценивают их качество в процессе обучения. традиционные подходы к обучению, такие как обучение с подкреплением на основе обратной связи от людей (rlhf), ограничены качеством и объемом человеческих данных. авторы статьи предлагают модель, которая сама создает инструкции, генерирует ответы и оценивает их качество, используя технику llm-as-a-judge. обучение проводится через итерации, каждая из которых состоит из следующих этапов: 1. генерация ответов на основе текущего датасета. на первой итерации — созданного человеком. 2. оценка ответов — модель сама оценивает качество ответов по определённым критериям, таким как релевантность, полезность, чёткость и уровень экспертизы. 3. создание самоинструкций для следующего шага — модель создает новые инструкции на основе исходного набора данных. 4. следующая итерация — дообученные модели используются для последующих итераций обучения. это ведёт к улучшению качества как генерации ответов, так и их оценки. в экспериментах использовали llama 2-70b, обученную на датасете open assistant. модель прошла через три итерации обучения с самосозданными данными. — первичная модель, файнтюненная на ift (instruction fine-tuning) и eft (evaluation fine-tuning)-данных с помощью sft, показала результаты, сопоставимые с базовой моделью. — вторая итерация, обученная на aift (ai feedback training) c помощью dpo, выдавала лучший ответ в 55,5% случаев, а в 32,8% — того же качества. — третья итерация в 47,7% случаев выдавала лучший ответ, а в 39,8% — того же качества. модель, прошедшая три итерации обучения, продемонстрировала улучшения в тестах alpacaeval 2.0, превзойдя модели claude 2, gemini pro и gpt-4 0613. модель третьей итерации показала 20,44% выигрышей в тестах, что значительно выше предыдущих итераций (9,94% для первой и 15,38% для второй). разбор подготовил ❣ валентин шубин душный nlp self-rewarding language models в сегодняшней статье — концепция self-rewarding language models. они не только генерируют ответы, но и оценивают их качество в процессе обучения. традиционные подходы к обучению, такие как обучение с подкреплением на основе обратной связи от людей (rlhf), ограничены качеством и объемом человеческих данных. авторы статьи предлагают модель, которая сама создает инструкции, генерирует ответы и оценивает их качество, используя технику llm-as-a-judge. обучение проводится через итерации, каждая из которых состоит из следующих этапов: 1. генерация ответов на основе текущего датасета. на первой итерации — созданного человеком. 2. оценка ответов — модель сама оценивает качество ответов по определённым критериям, таким как релевантность, полезность, чёткость и уровень экспертизы. 3. создание самоинструкций для следующего шага — модель создает новые инструкции на основе исходного набора данных. 4. следующая итерация — дообученные модели используются для последующих итераций обучения. это ведёт к улучшению качества как генерации ответов, так и их оценки. в экспериментах использовали llama 2-70b, обученную на датасете open assistant. модель прошла через три итерации обучения с самосозданными данными. — первичная модель , файнтюненная на ift (instruction fine-tuning) и eft (evaluation fine-tuning)-данных с помощью sft, показала результаты, сопоставимые с базовой моделью. — вторая итерация , обученная на aift (ai feedback training) c помощью dpo, выдавала лучший ответ в 55,5% случаев, а в 32,8% — того же качества. — третья итерация в 47,7% случаев выдавала лучший ответ, а в 39,8% — того же качества. модель, прошедшая три итерации обучения, продемонстрировала улучшения в тестах alpacaeval 2.0, превзойдя модели claude 2, gemini pro и gpt-4 0613. модель третьей итерации показала 20,44% выигрышей в тестах, что значительно выше предыдущих итераций (9,94% для первой и 15,38% для второй). разбор подготовил ❣ валентин шубин душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-09-20T09:02:01+00:00" href="./posts/35.html">2024-09-20 09:02 UTC</a></div>
      </div>
      <div class="post-body"><strong>Self-Rewarding Language Models</strong><br><br>В <a href="https://arxiv.org/abs/2401.10020" rel="nofollow noopener noreferrer">сегодняшней статье</a> — концепция Self-Rewarding Language Models. Они не только генерируют ответы, но и оценивают их качество в процессе обучения. <br><br>Традиционные подходы к обучению, такие как обучение с подкреплением на основе обратной связи от людей (RLHF), ограничены качеством и объемом человеческих данных. Авторы статьи предлагают модель, которая сама создает инструкции, генерирует ответы и оценивает их качество, используя технику LLM-as-a-Judge. <br><br>Обучение проводится через итерации, каждая из которых состоит из следующих этапов:<br><br><strong>1. Генерация ответов</strong> на основе текущего датасета. На первой итерации — созданного человеком.<br><strong>2. Оценка ответов</strong> — модель сама оценивает качество ответов по определённым критериям, таким как релевантность, полезность, чёткость и уровень экспертизы.<br><strong>3. Создание самоинструкций для следующего шага</strong> — модель создает новые инструкции на основе исходного набора данных.<br><strong>4. Следующая итерация</strong> — дообученные модели используются для последующих итераций обучения. Это ведёт к улучшению качества как генерации ответов, так и их оценки.<br><br>В экспериментах использовали Llama 2-70B, обученную на датасете Open Assistant. Модель прошла через три итерации обучения с самосозданными данными.<br><br><strong>— Первичная модель</strong>, файнтюненная на IFT (Instruction Fine-Tuning) и EFT (Evaluation Fine-Tuning)-данных с помощью SFT, показала результаты, сопоставимые с базовой моделью.<br><strong>— Вторая итерация</strong>, обученная на AIFT (AI Feedback Training) c помощью DPO, выдавала лучший ответ в 55,5% случаев, а в 32,8% — того же качества. <br><strong>— Третья итерация</strong> в 47,7% случаев выдавала лучший ответ, а в 39,8% — того же качества.<br><br>Модель, прошедшая три итерации обучения, продемонстрировала улучшения в тестах AlpacaEval 2.0, превзойдя модели Claude 2, Gemini Pro и GPT-4 0613. Модель третьей итерации показала 20,44% выигрышей в тестах, что значительно выше предыдущих итераций (9,94% для первой и 15,38% для второй).<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Валентин Шубин</em><br><br><a href="https://t.me/+Vs1Q1Fc2KW8zNjZi" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/35_480.webp" srcset="../assets/media/thumbs/35_480.webp 480w, ../assets/media/35.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="35" data-image-index="0" /></div></div>
      <div class="actions">
        <span>6 199 просмотров · 53 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/35" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/35.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="33" data-search="rest — метод ускорения генерации токенов сегодняшняя статья посвящена методу retrieval-based speculative decoding (rest). по словам авторов, он позволяет ускорить генерацию токенов более чем в два раза. а в основе подхода лежит спекулятивное декодирование. о нём сперва коротко и расскажем. спекулятивное декодирование (speculative decoding) — это метод, который направлен на ускорение генерации за счёт использования дополнительной, более компактной и быстрой модели. она предлагает варианты продолжения цепочек токенов, которые затем проверяются основной моделью. если предсказания обеих моделей совпадают, токены принимаются, в противном случае — отбрасываются. однако у этого метода есть ограничения и недостатки. необходимо найти такую вспомогательную модель, которая обладала бы высокой точностью, но при этом не тратила слишком много вычислительных ресурсов. кроме того, малые модели требуют дополнительного обучения. авторы статьи предлагают использовать вместо меньшей модели базу данных, построенную на суффиксном массиве и включающую корпусы кодов и текстов — например, диалогов ultrachat. из базы извлекаются: контексты (contexts) — последовательности токенов, которые служат отправной точкой для поиска продолжений. продолжения (continuations) — следующие за контекстом токены, представляющие возможные варианты развития текста или кода. подходящие продолжения организуются в древовидную trie-структуру, где каждый узел — это токен. далее выбираются те последовательности токенов, которые повторяются чаще всего. их называют кандидатами (candidates). кандидаты проверяются с помощью маски tree attention. её особенность состоит в том, что всё осуществляется всего за один проход трансформера. оптимизация достигается за счёт того, что у многих продолжений одинаковые префиксы. модель принимает все токены, которые успешно прошли проверку с самого начала, а то, что следует за первой ошибкой — отбрасывает. после этого процесс начинается снова для оставшихся токенов. результаты тестов на codellama и vicuna показали прирост в скорости генерации токенов с использованием rest в 1,62–2,36 раза. однако методу ещё есть, куда развиваться. так, авторы отмечают, что результат сильно зависит от полноты и качества базы данных, и рекомендуют собирать её, используя llm. кроме того, возникают трудности с пониманием контекста — например, при генерации персонализированных названий переменных в программировании. рассказывайте в комментариях, а что вы думаете о rest? разбор подготовил ❣ дмитрий васильев душный nlp rest — метод ускорения генерации токенов сегодняшняя статья посвящена методу retrieval-based speculative decoding (rest). по словам авторов, он позволяет ускорить генерацию токенов более чем в два раза. а в основе подхода лежит спекулятивное декодирование. о нём сперва коротко и расскажем. спекулятивное декодирование (speculative decoding) — это метод, который направлен на ускорение генерации за счёт использования дополнительной, более компактной и быстрой модели. она предлагает варианты продолжения цепочек токенов, которые затем проверяются основной моделью. если предсказания обеих моделей совпадают, токены принимаются, в противном случае — отбрасываются. однако у этого метода есть ограничения и недостатки. необходимо найти такую вспомогательную модель, которая обладала бы высокой точностью, но при этом не тратила слишком много вычислительных ресурсов. кроме того, малые модели требуют дополнительного обучения. авторы статьи предлагают использовать вместо меньшей модели базу данных, построенную на суффиксном массиве и включающую корпусы кодов и текстов — например, диалогов ultrachat. из базы извлекаются: контексты (contexts) — последовательности токенов, которые служат отправной точкой для поиска продолжений. продолжения (continuations) — следующие за контекстом токены, представляющие возможные варианты развития текста или кода. подходящие продолжения организуются в древовидную trie-структуру, где каждый узел — это токен. далее выбираются те последовательности токенов, которые повторяются чаще всего. их называют кандидатами (candidates). кандидаты проверяются с помощью маски tree attention. её особенность состоит в том, что всё осуществляется всего за один проход трансформера. оптимизация достигается за счёт того, что у многих продолжений одинаковые префиксы. модель принимает все токены, которые успешно прошли проверку с самого начала, а то, что следует за первой ошибкой — отбрасывает. после этого процесс начинается снова для оставшихся токенов. результаты тестов на codellama и vicuna показали прирост в скорости генерации токенов с использованием rest в 1,62–2,36 раза. однако методу ещё есть, куда развиваться. так, авторы отмечают, что результат сильно зависит от полноты и качества базы данных, и рекомендуют собирать её, используя llm. кроме того, возникают трудности с пониманием контекста — например, при генерации персонализированных названий переменных в программировании. рассказывайте в комментариях, а что вы думаете о rest? разбор подготовил ❣ дмитрий васильев душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-09-13T08:37:57+00:00" href="./posts/33.html">2024-09-13 08:37 UTC</a></div>
      </div>
      <div class="post-body"><strong>REST — метод ускорения генерации токенов</strong><br><br><a href="https://arxiv.org/abs/2311.08252" rel="nofollow noopener noreferrer">Сегодняшняя статья</a> посвящена методу Retrieval-Based Speculative Decoding (REST). По словам авторов, он позволяет ускорить генерацию токенов более чем в два раза. А в основе подхода лежит спекулятивное декодирование. О нём сперва коротко и расскажем. <br><br>Спекулятивное декодирование (Speculative Decoding) — это метод, который направлен на ускорение генерации за счёт использования дополнительной, более компактной и быстрой модели. Она предлагает варианты продолжения цепочек токенов, которые затем проверяются основной моделью. Если предсказания обеих моделей совпадают, токены принимаются, в противном случае — отбрасываются. <br><br>Однако у этого метода есть ограничения и недостатки. Необходимо найти такую вспомогательную модель, которая обладала бы высокой точностью, но при этом не тратила слишком много вычислительных ресурсов. Кроме того, малые модели требуют дополнительного обучения.<br><br>Авторы статьи предлагают использовать вместо меньшей модели базу данных, построенную на суффиксном массиве и включающую корпусы кодов и текстов — например, диалогов UltraChat. Из базы извлекаются:<br><br><strong>Контексты (contexts) </strong>— последовательности токенов, которые служат отправной точкой для поиска продолжений.<br><strong>Продолжения (continuations)</strong> — следующие за контекстом токены,  представляющие возможные варианты развития текста или кода. <br><br>Подходящие продолжения организуются в древовидную Trie-структуру, где каждый узел — это токен. Далее выбираются те последовательности токенов, которые повторяются чаще всего. Их называют кандидатами (candidates).<br><br>Кандидаты проверяются с помощью маски Tree Attention. Её особенность состоит в том, что всё осуществляется всего за один проход трансформера. Оптимизация достигается за счёт того, что у многих продолжений одинаковые префиксы. <br><br>Модель принимает все токены, которые успешно прошли проверку с самого начала, а то, что следует за первой ошибкой — отбрасывает. После этого процесс начинается снова для оставшихся токенов.<br><br>Результаты тестов на CodeLlama и Vicuna показали прирост в скорости генерации токенов с использованием REST в 1,62–2,36 раза. Однако методу ещё есть, куда развиваться. Так, авторы отмечают, что результат сильно зависит от полноты и качества базы данных, и рекомендуют собирать её, используя LLM. Кроме того, возникают трудности с пониманием контекста — например, при генерации персонализированных названий переменных в программировании. <br><br>Рассказывайте в комментариях, а что вы думаете о REST?<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Дмитрий Васильев</em><br><br><a href="https://t.me/+iqcQY1lZ0RBkOGYy" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/33_480.webp" srcset="../assets/media/thumbs/33_480.webp 480w, ../assets/media/33.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="33" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/34_480.webp" srcset="../assets/media/thumbs/34_480.webp 480w, ../assets/media/34.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="33" data-image-index="1" /></div></div>
      <div class="actions">
        <span>5 590 просмотров · 52 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/33" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/33.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="32" data-search="branch-train-mix — метод создания moe-моделей сегодня рассмотрим статью, в которой предложен метод получения moe-модели (mixture-of-experts) из обычной, dense-модели. авторы назвали его branch-train-mix (btx). для начала напомним, что такое moe. это архитектурный подход, который предполагает использование в трансформенных слоях нескольких fnn-блоков — экспертов. у каждого из них предположительно есть собственная узкая специализация, поэтому над решением поставленной модели задачи они работают не одновременно. роутер выбирает, какие эксперты лучше подходят для обработки токенов на каждом конкретном этапе. во время инференса модели, построенные на архитектуре moe, задействуют меньше параметров, чем их dense-аналоги — и без сильной потери качества. кроме того, они прекрасно масштабируются. поэтому понятно стремление превратить обычную модель в moe. в основе идеи авторов лежит метод btm (branch-train-merge). суть его заключается в том, чтобы взять несколько одинаковых llm, параллельно обучить их на разных датасетах, а затем агрегировать предсказания каждой модели во время инференса. рассматриваемая статья предлагает после обучения слить все модели в moe, чтобы fnn каждой базовой модели стал экспертом в соответствующем слое, и добавить роутер. веса аттеншена при этом следует усреднить и дообучить. авторы взяли три копии llama-2 7b и дообучили каждую на своём домене: математика (на тех же данных, что и llemma), программирование (на тех же данных, что codellama) и общие знания (на «википедии»). к финальной модели добавили оригинальную llama-2 7b, не дообученную на чём-то конкретном. получившуюся moe-модель файнтюнили на источниках, которые применялись для обучения всех четырёх экспертов. в результате модель не сильно уступает тем же llemma и codellama в вопросах математики и программирования. интересно и то, что btx-модели обретают интерпретируемость. авторы показывают, что в эксперта, обученного на определённом домене, попадают токены из сэмпла этого же домена. рассказывайте в комментариях, что думаете про btx! разбор подготовил ❣ александр пацация душный nlp branch-train-mix — метод создания moe-моделей сегодня рассмотрим статью, в которой предложен метод получения moe-модели (mixture-of-experts) из обычной, dense-модели. авторы назвали его branch-train-mix (btx). для начала напомним, что такое moe. это архитектурный подход, который предполагает использование в трансформенных слоях нескольких fnn-блоков — экспертов. у каждого из них предположительно есть собственная узкая специализация, поэтому над решением поставленной модели задачи они работают не одновременно. роутер выбирает, какие эксперты лучше подходят для обработки токенов на каждом конкретном этапе. во время инференса модели, построенные на архитектуре moe, задействуют меньше параметров, чем их dense-аналоги — и без сильной потери качества. кроме того, они прекрасно масштабируются. поэтому понятно стремление превратить обычную модель в moe. в основе идеи авторов лежит метод btm (branch-train-merge). суть его заключается в том, чтобы взять несколько одинаковых llm, параллельно обучить их на разных датасетах, а затем агрегировать предсказания каждой модели во время инференса. рассматриваемая статья предлагает после обучения слить все модели в moe, чтобы fnn каждой базовой модели стал экспертом в соответствующем слое, и добавить роутер. веса аттеншена при этом следует усреднить и дообучить. авторы взяли три копии llama-2 7b и дообучили каждую на своём домене: математика (на тех же данных, что и llemma), программирование (на тех же данных, что codellama) и общие знания (на «википедии»). к финальной модели добавили оригинальную llama-2 7b, не дообученную на чём-то конкретном. получившуюся moe-модель файнтюнили на источниках, которые применялись для обучения всех четырёх экспертов. в результате модель не сильно уступает тем же llemma и codellama в вопросах математики и программирования. интересно и то, что btx-модели обретают интерпретируемость. авторы показывают, что в эксперта, обученного на определённом домене, попадают токены из сэмпла этого же домена. рассказывайте в комментариях, что думаете про btx! разбор подготовил ❣ александр пацация душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-09-06T09:30:24+00:00" href="./posts/32.html">2024-09-06 09:30 UTC</a></div>
      </div>
      <div class="post-body"><strong>Branch-Train-MiX — метод создания MoE-моделей </strong><br><br>Сегодня <a href="https://arxiv.org/abs/2403.07816" rel="nofollow noopener noreferrer">рассмотрим статью,</a> в которой предложен метод получения MoE-модели (Mixture-of-Experts) из обычной, dense-модели. Авторы назвали его Branch-Train-MiX (BTX).<br><br>Для начала напомним, что такое MoE. Это архитектурный подход, который предполагает использование в трансформенных слоях нескольких FNN-блоков — экспертов. У каждого из них предположительно есть собственная узкая специализация, поэтому над решением поставленной модели задачи они работают не одновременно. Роутер выбирает, какие эксперты лучше подходят для обработки токенов на каждом конкретном этапе. <br><br>Во время инференса модели, построенные на архитектуре MoE, задействуют меньше параметров, чем их dense-аналоги — и без сильной потери качества. Кроме того, они прекрасно масштабируются. Поэтому понятно стремление превратить обычную модель в MoE. <br><br>В основе идеи авторов лежит метод BTM (Branch-Train-Merge). Суть его заключается в том, чтобы взять несколько одинаковых LLM, параллельно обучить их на разных датасетах, а затем агрегировать предсказания каждой модели во время инференса. <br><br>Рассматриваемая статья предлагает после обучения слить все модели в MoE, чтобы FNN каждой базовой модели стал экспертом в соответствующем слое, и добавить роутер. Веса аттеншена при этом следует усреднить и дообучить. <br><br>Авторы взяли три копии Llama-2 7B и дообучили каждую на своём домене: математика (на тех же данных, что и Llemma), программирование (на тех же данных, что CodeLlama) и общие знания (на «Википедии»). К финальной модели добавили оригинальную Llama-2 7B, не дообученную на чём-то конкретном. Получившуюся MoE-модель файнтюнили на источниках, которые применялись для обучения всех четырёх экспертов. В результате модель не сильно уступает тем же Llemma и CodeLlama в вопросах математики и программирования. <br><br>Интересно и то, что BTX-модели обретают интерпретируемость. Авторы показывают, что в эксперта, обученного на определённом домене, попадают токены из сэмпла этого же домена.<br><br>Рассказывайте в комментариях, что думаете про BTX! <br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Александр Пацация</em><br><br><a href="https://t.me/+eA4YF9UCmLBiMTgy" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/32_480.webp" srcset="../assets/media/thumbs/32_480.webp 480w, ../assets/media/32.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="32" data-image-index="0" /></div></div>
      <div class="actions">
        <span>5 244 просмотров · 42 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/32" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/32.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="30" data-search="icml 2024 — ещё интересные статьи соскучились? специально для душного nlp роман горб из команды ускорения инференса yandexgpt поделился релевантными его рабочей области статьями с icml 2024. towards efficient generative large language model serving отлично структурированная статья и рассказ об llm serving — о челленджах, их причинах и решениях. сначала описывается генеративный сценарий и его природа. авторы отмечают, что корпорации вкладываются в llm, размеры моделей растут и появляются новые семейства. но есть и платформы, на которых модели хочется деплоить, что требует инженерных усилий. где-то между находится llm serving. задача области — поиск компромисса между качеством и скоростью (compute budget), о чём и рассказывает статья. на первой картинке — 2 подкласса методов: алгоритмические и системные. с подробностями предлагаю ознакомиться самостоятельно, но выделю часть из них. speculative decoding базово для этого метода нужны 2 модели: draft и verifier. первая быстро и дёшево генерирует гипотезы; вторая — выбирает из них валидное продолжение так, чтобы генерация происходила из распределения verifier-модели. чем больше токенов «угадывает» draft-модель, тем выше ускорение. в продвинутых версиях растёт утилизация ресурсов gpu. свежие работы (eagle и medusa) предлагают уменьшить draft-модель для большего ускорения при сохранении качества. авторы дообучают небольшие головы поверх verifier-модели, что снижает оверхэд и ускоряет процесс даже в сложных сценариях с высоким throughput токенов. draft-головы генерируют гипотезы в виде дерева, а не списка (см. картинку 2), что повышает точность принятия токенов. архитектурная оптимизация в mobilellm и rethinking optimization and architecture for tiny language models исследуют вопрос оптимальной архитектуры моделей до 1b для мобильных устройств. авторы за тот же compute получили значительные + 4 пп качества на бенчмарках с помощью swiglu-активаций, глубоких, а не широких сети, shared-эмбеддингов, grouped query attention, init из весов более крупных моделей и прунинга. душный nlp icml 2024 — ещё интересные статьи соскучились? специально для душного nlp роман горб из команды ускорения инференса yandexgpt поделился релевантными его рабочей области статьями с icml 2024. towards efficient generative large language model serving отлично структурированная статья и рассказ об llm serving — о челленджах, их причинах и решениях. сначала описывается генеративный сценарий и его природа. авторы отмечают, что корпорации вкладываются в llm, размеры моделей растут и появляются новые семейства. но есть и платформы, на которых модели хочется деплоить, что требует инженерных усилий. где-то между находится llm serving. задача области — поиск компромисса между качеством и скоростью (compute budget), о чём и рассказывает статья. на первой картинке — 2 подкласса методов: алгоритмические и системные. с подробностями предлагаю ознакомиться самостоятельно, но выделю часть из них. speculative decoding базово для этого метода нужны 2 модели: draft и verifier. первая быстро и дёшево генерирует гипотезы; вторая — выбирает из них валидное продолжение так, чтобы генерация происходила из распределения verifier-модели. чем больше токенов «угадывает» draft-модель, тем выше ускорение. в продвинутых версиях растёт утилизация ресурсов gpu. свежие работы ( eagle и medusa ) предлагают уменьшить draft-модель для большего ускорения при сохранении качества. авторы дообучают небольшие головы поверх verifier-модели, что снижает оверхэд и ускоряет процесс даже в сложных сценариях с высоким throughput токенов. draft-головы генерируют гипотезы в виде дерева, а не списка (см. картинку 2), что повышает точность принятия токенов. архитектурная оптимизация в mobilellm и rethinking optimization and architecture for tiny language models исследуют вопрос оптимальной архитектуры моделей до 1b для мобильных устройств. авторы за тот же compute получили значительные + 4 пп качества на бенчмарках с помощью swiglu-активаций, глубоких, а не широких сети, shared-эмбеддингов, grouped query attention, init из весов более крупных моделей и прунинга. душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-08-29T11:33:16+00:00" href="./posts/30.html">2024-08-29 11:33 UTC</a></div>
      </div>
      <div class="post-body"><strong>ICML 2024 — ещё интересные статьи</strong><br><br>Соскучились? Специально для Душного NLP Роман Горб из команды ускорения инференса YandexGPT поделился релевантными его рабочей области статьями с ICML 2024.<br><br><a href="https://icml.cc/virtual/2024/tutorial/35229" rel="nofollow noopener noreferrer"><strong>Towards Efficient Generative Large Language Model Serving</strong></a><br><br>Отлично структурированная статья и рассказ об LLM Serving — о челленджах, их причинах и решениях. Сначала описывается генеративный сценарий и его природа. Авторы отмечают, что корпорации вкладываются в LLM, размеры моделей растут и появляются новые семейства. Но есть и платформы, на которых модели хочется деплоить, что требует инженерных усилий. Где-то между находится LLM Serving. Задача области — поиск компромисса между качеством и скоростью (compute budget), о чём и рассказывает статья.<br><br>На первой картинке — 2 подкласса методов: алгоритмические и системные. С подробностями предлагаю ознакомиться самостоятельно, но выделю часть из них.<br><br><strong>Speculative Decoding</strong><br><br>Базово для этого метода нужны 2 модели: draft и verifier. Первая быстро и дёшево генерирует гипотезы; вторая — выбирает из них валидное продолжение так, чтобы генерация происходила из распределения verifier-модели. Чем больше токенов «угадывает» draft-модель, тем выше ускорение. В продвинутых версиях растёт утилизация ресурсов GPU.<br><br>Свежие работы (<a href="https://arxiv.org/abs/2401.15077" rel="nofollow noopener noreferrer"><strong>EAGLE</strong></a> и <a href="https://arxiv.org/abs/2401.10774" rel="nofollow noopener noreferrer"><strong>MEDUSA</strong></a>) предлагают уменьшить draft-модель для большего ускорения при сохранении качества. Авторы дообучают небольшие головы поверх verifier-модели, что снижает оверхэд и ускоряет процесс даже в сложных сценариях с высоким throughput токенов. Draft-головы генерируют гипотезы в виде дерева, а не списка (см. картинку 2), что повышает точность принятия токенов.<br><br><strong>Архитектурная Оптимизация</strong><br><br>В <a href="https://arxiv.org/abs/2402.14905" rel="nofollow noopener noreferrer"><strong>MobileLLM</strong></a> и <a href="https://arxiv.org/abs/2402.02791" rel="nofollow noopener noreferrer"><strong>Rethinking Optimization and Architecture for Tiny Language Models</strong></a> исследуют вопрос оптимальной архитектуры моделей до 1B для мобильных устройств. Авторы за тот же compute получили значительные + 4 пп качества на бенчмарках с помощью SwiGLU-активаций, глубоких, а не широких сети, shared-эмбеддингов, Grouped Query Attention, init из весов более крупных моделей и прунинга.<br><br><a href="https://t.me/+iYqdqLFuNLU2Y2Qy" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/30_480.webp" srcset="../assets/media/thumbs/30_480.webp 480w, ../assets/media/30.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="30" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/31_480.webp" srcset="../assets/media/thumbs/31_480.webp 480w, ../assets/media/31.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="30" data-image-index="1" /></div></div>
      <div class="actions">
        <span>12 263 просмотров · 39 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/30" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/30.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="29" data-search="впечатления от icml 2024 и несколько интересных статей отгремела конференция icml, но делиться впечатлениями от мероприятия никогда не поздно. руководитель команды yandexgpt alignment андрей бут рассказал нашему каналу, чем ему запомнилось событие и на какие статьи стоит обратить внимание. первое и главное впечатление — llm добрались и до icml. им была посвящена примерно треть работ. ну что же, о времена, о нравы. с другой стороны, большой плюс конференции в том, что организаторы смогли поддержать разнообразие работ по всем темам. можно было найти и «хардкорную математику» и обзорные position-статьи. приятно порадовали работы по rl — причём за счёт того, что работы по rlhf были вперемешку со «взрослым rl», можно было оценить применимость к области llm. несмотря на рост числа работ, по-прежнему остаётся опция пообщаться за постером (привет, neurips). хотя охватить все интересные статьи — скорее невозможная задача. интересные статьи a closer look at the limitations of instruction tuning в этой работе авторы задаются вопросом: какие есть минусы у supervised finetuning (sft) и может ли обучение с lora побороть их? исследователи опираются на тот факт, что при sft модель чаще галлюцинирует и подстраивается под sft-датасет. проводятся различные эксперименты, чтобы доказать эту гипотезу. в статье показано, что lora выдаёт лучший результат на нескольких общепризнанных датасетах — например, sft-alpaca-52k, sft-wizard-lm и других. self-rewarding language models в этой работе исследователи задаются вопросом: можно ли отказаться от дорогой preference-разметки? авторы предлагают подход, в котором исходная модель используется для генерации новых инструкций и ответов, а также для их оценки! за три итерации такой метод выдаёт сильные результаты, превосходящие claude 2, gemini pro и gpt-4 0613. is dpo superior to ppo for llm alignment? a comprehensive study авторы этой публикации рассматривают преимущества двух популярных алгоритмов для rlhf: dpo и ppo. в работе есть как интересные теоретические находки (область решений ppo строго вложена в область решений dpo), так и практические советы о том, как добиться лучших результатов для каждого из подходов. своими замерами авторы демонстрируют, что при правильном «приготовлении» ppo превосходит dpo. душный nlp впечатления от icml 2024 и несколько интересных статей отгремела конференция icml, но делиться впечатлениями от мероприятия никогда не поздно. руководитель команды yandexgpt alignment андрей бут рассказал нашему каналу, чем ему запомнилось событие и на какие статьи стоит обратить внимание. первое и главное впечатление — llm добрались и до icml. им была посвящена примерно треть работ. ну что же, о времена, о нравы. с другой стороны, большой плюс конференции в том, что организаторы смогли поддержать разнообразие работ по всем темам. можно было найти и «хардкорную математику» и обзорные position-статьи. приятно порадовали работы по rl — причём за счёт того, что работы по rlhf были вперемешку со «взрослым rl», можно было оценить применимость к области llm. несмотря на рост числа работ, по-прежнему остаётся опция пообщаться за постером (привет, neurips). хотя охватить все интересные статьи — скорее невозможная задача. интересные статьи a closer look at the limitations of instruction tuning в этой работе авторы задаются вопросом: какие есть минусы у supervised finetuning (sft) и может ли обучение с lora побороть их? исследователи опираются на тот факт, что при sft модель чаще галлюцинирует и подстраивается под sft-датасет. проводятся различные эксперименты, чтобы доказать эту гипотезу. в статье показано, что lora выдаёт лучший результат на нескольких общепризнанных датасетах — например, sft-alpaca-52k, sft-wizard-lm и других. self-rewarding language models в этой работе исследователи задаются вопросом: можно ли отказаться от дорогой preference-разметки? авторы предлагают подход, в котором исходная модель используется для генерации новых инструкций и ответов, а также для их оценки! за три итерации такой метод выдаёт сильные результаты, превосходящие claude 2, gemini pro и gpt-4 0613. is dpo superior to ppo for llm alignment? a comprehensive study авторы этой публикации рассматривают преимущества двух популярных алгоритмов для rlhf: dpo и ppo. в работе есть как интересные теоретические находки (область решений ppo строго вложена в область решений dpo), так и практические советы о том, как добиться лучших результатов для каждого из подходов. своими замерами авторы демонстрируют, что при правильном «приготовлении» ppo превосходит dpo. душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-08-22T09:40:43+00:00" href="./posts/29.html">2024-08-22 09:40 UTC</a></div>
      </div>
      <div class="post-body"><strong>Впечатления от ICML 2024 и несколько интересных статей</strong><br><br>Отгремела конференция ICML, но делиться впечатлениями от мероприятия никогда не поздно. Руководитель команды YandexGPT Alignment Андрей Бут рассказал нашему каналу, чем ему запомнилось событие и на какие статьи стоит обратить внимание. <br><br><blockquote>Первое и главное впечатление — LLM добрались и до ICML. Им была посвящена примерно треть работ. Ну что же, о времена, о нравы. <br><br>С другой стороны, большой плюс конференции в том, что организаторы смогли поддержать разнообразие работ по всем темам. Можно было найти и «хардкорную математику» и обзорные position-статьи. Приятно порадовали работы по RL — причём за счёт того, что работы по RLHF были вперемешку со «взрослым RL», можно было оценить применимость к области LLM.<br><br>Несмотря на рост числа работ, по-прежнему остаётся опция пообщаться за постером (привет, NeurIPS). Хотя охватить все интересные статьи — скорее невозможная задача.</blockquote><br><br><strong>Интересные статьи</strong><br><br><a href="https://arxiv.org/abs/2402.05119" rel="nofollow noopener noreferrer"><strong>A Closer Look at the Limitations of Instruction Tuning</strong></a><br><br>В этой работе авторы задаются вопросом: какие есть минусы у Supervised Finetuning (SFT) и может ли обучение с LoRA побороть их? Исследователи  опираются на тот факт, что при SFT модель чаще галлюцинирует и подстраивается под SFT-датасет. Проводятся различные эксперименты, чтобы доказать эту гипотезу. В статье показано, что LoRA выдаёт лучший результат на нескольких общепризнанных датасетах — например, SFT-Alpaca-52k, SFT-Wizard-LM и других.<br><br><a href="https://arxiv.org/abs/2401.10020" rel="nofollow noopener noreferrer"><strong>Self-Rewarding Language Models</strong></a><br><br>В этой работе исследователи задаются вопросом: можно ли отказаться от дорогой preference-разметки? Авторы предлагают подход, в котором исходная модель используется для генерации новых инструкций и ответов, а также для их оценки! За три итерации такой метод выдаёт сильные результаты, превосходящие Claude 2, Gemini Pro и GPT-4 0613. <br><br><a href="https://arxiv.org/abs/2404.10719" rel="nofollow noopener noreferrer"><strong>Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study</strong></a><br><br>Авторы этой публикации рассматривают преимущества двух популярных алгоритмов для RLHF: DPO и PPO. В работе есть как интересные теоретические находки (область решений PPO строго вложена в область решений DPO), так и практические советы о том, как добиться лучших результатов для каждого из подходов. Своими замерами авторы демонстрируют, что при правильном «приготовлении» PPO превосходит DPO.<br><br><a href="https://t.me/+8iDcirAXhmBkMDEy" rel="nofollow noopener noreferrer">Душный NLP</a></div>
      <div class="actions">
        <span>6 120 просмотров · 52 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/29" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/29.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="28" data-search="масштабирование и параметризация сохранение стабильности гиперпараметров при масштабировании модели позволяет подбирать гиперпараметры вроде lr или масштаба инициализации на маленьких моделях, не тратя ресурсы на дорогое обучение больших моделей. это важная задача, решению которой посвящены две сегодняшних публикации. авторы статьи tensor programs v предлагают использовать maximal update parametrization (µp) — перенос параметров с маленькой модели на большую без дополнительной настройки. традиционные методы параметризации приводят к изменению оптимальных гиперпараметров при увеличении масштаба сетей. впрочем, существуют способы избежать этого. чтобы достичь стабильности гиперпараметров, нужно правильно масштабировать спектральную норму матриц весов — показатель максимально возможного растяжения или сжатия вектора при его умножении на матрицу. авторы статьи отмечают, что добиться стабильности можно двумя способами: правильным масштабированием инициализаций и послойных lr, либо напрямую спектральной нормализацией матриц весов и их обновлений в процессе обучения. благодаря такому решению масштаб признаков и их изменений на каждом шаге сохраняется при увеличении размера сети — этого оказывается достаточно для стабильности гиперпараметров. в статье a spectral condition for feature learning предполагается, что обновления весов в градиентном спуске имеют низкий ранг и хорошо согласуются с векторами активаций. однако предположение о такой согласованности на самом деле выполняется не всегда, поэтому в более свежей статье scaling exponents across parameterizations and optimizers авторы предлагают дальнейшее улучшение метода с поправкой на это. расскажите в комментариях, что думаете по поводу этих методов! разбор подготовил ❣ дмитрий лунин душный nlp масштабирование и параметризация сохранение стабильности гиперпараметров при масштабировании модели позволяет подбирать гиперпараметры вроде lr или масштаба инициализации на маленьких моделях, не тратя ресурсы на дорогое обучение больших моделей. это важная задача, решению которой посвящены две сегодняшних публикации. авторы статьи tensor programs v предлагают использовать maximal update parametrization (µp) — перенос параметров с маленькой модели на большую без дополнительной настройки. традиционные методы параметризации приводят к изменению оптимальных гиперпараметров при увеличении масштаба сетей. впрочем, существуют способы избежать этого. чтобы достичь стабильности гиперпараметров, нужно правильно масштабировать спектральную норму матриц весов — показатель максимально возможного растяжения или сжатия вектора при его умножении на матрицу. авторы статьи отмечают, что добиться стабильности можно двумя способами: правильным масштабированием инициализаций и послойных lr, либо напрямую спектральной нормализацией матриц весов и их обновлений в процессе обучения. благодаря такому решению масштаб признаков и их изменений на каждом шаге сохраняется при увеличении размера сети — этого оказывается достаточно для стабильности гиперпараметров. в статье a spectral condition for feature learning предполагается, что обновления весов в градиентном спуске имеют низкий ранг и хорошо согласуются с векторами активаций. однако предположение о такой согласованности на самом деле выполняется не всегда, поэтому в более свежей статье scaling exponents across parameterizations and optimizers авторы предлагают дальнейшее улучшение метода с поправкой на это. расскажите в комментариях, что думаете по поводу этих методов! разбор подготовил ❣ дмитрий лунин душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-08-13T10:02:13+00:00" href="./posts/28.html">2024-08-13 10:02 UTC</a></div>
      </div>
      <div class="post-body"><strong>Масштабирование и параметризация</strong><br><br>Сохранение стабильности гиперпараметров при масштабировании модели позволяет подбирать гиперпараметры вроде LR или масштаба инициализации на маленьких моделях, не тратя ресурсы на дорогое обучение больших моделей. Это важная задача, решению которой посвящены <a href="https://arxiv.org/abs/2310.17813" rel="nofollow noopener noreferrer">две</a> <a href="https://arxiv.org/abs/2203.03466" rel="nofollow noopener noreferrer">сегодняшних</a> публикации. <br><br>Авторы статьи Tensor Programs V предлагают использовать Maximal Update Parametrization (µP) — перенос параметров с маленькой модели на большую без дополнительной настройки.  <br><br>Традиционные методы параметризации приводят к изменению оптимальных гиперпараметров при увеличении масштаба сетей. Впрочем, существуют способы избежать этого.<br><br>Чтобы достичь стабильности гиперпараметров, нужно правильно масштабировать спектральную норму матриц весов — показатель максимально возможного растяжения или сжатия вектора при его умножении на матрицу. Авторы статьи отмечают, что добиться стабильности можно двумя способами: правильным масштабированием инициализаций и послойных LR, либо напрямую спектральной нормализацией матриц весов и их обновлений в процессе обучения.<br><br>Благодаря такому решению масштаб признаков и их изменений на каждом шаге сохраняется при увеличении размера сети — этого оказывается достаточно для стабильности гиперпараметров. В статье <a href="https://arxiv.org/abs/2310.17813" rel="nofollow noopener noreferrer">A Spectral Condition for Feature Learning</a> предполагается, что обновления весов в градиентном спуске имеют низкий ранг и хорошо согласуются с векторами активаций. Однако предположение о такой согласованности на самом деле выполняется не всегда, поэтому в более свежей статье <a href="https://arxiv.org/abs/2407.05872" rel="nofollow noopener noreferrer">Scaling Exponents Across Parameterizations and Optimizers</a> авторы предлагают дальнейшее улучшение метода с поправкой на это.<br><br>Расскажите в комментариях, что думаете по поводу этих методов! <br><br><em>Разбор подготовил </em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> <em>Дмитрий Лунин</em><br><br><a href="https://t.me/+H0mJtChyAWc1ZGNi" rel="nofollow noopener noreferrer">Душный NLP</a></div>
      <div class="actions">
        <span>14 755 просмотров · 31 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/28" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/28.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="27" data-search="deepseek-v2 — moe-модель с технологией mla компания deepseek представила moe-модель deepseek-v2 на 236 миллиардов параметров. сегодня мы разберём статью, которая рассказывает об особенностях этой модели. модель состоит из 236 миллиардов параметров, однако на каждый токен активно используется лишь 27 миллиардов. это позволило значительно увеличить длину контекста до 128 тысяч токенов. для сравнения и оценки результатов в deepseek использовали dense-модель на 67b. авторы смогли существенно снизить стоимость обучения и уменьшить размер kv-кэша, что позволило увеличить скорость генерации токенов почти в 8 раз. эти достижения обеспечили модели преимущество в производительности. в deepseek-v2 внедрён модифицированный attention, который получил название mla (multi-head latent attention). этот механизм позволяет кэшировать уменьшенный вектор с последующим его восстановлением, что значительно снижает объём ресурсов, необходимый для работы модели. как устроен mla в сравнении с mha, gqa и mqa, вы можете увидеть на изображении выше. при таком подходе нельзя напрямую использовать rope (rotary position embedding) — в противном случае матрица rope становится неразделимой с остальными операциями, что значительно снижает эффективность вывода. в deepseek предложили стратегию так называемого «развязанного rope» (decoupled rope), в которой дополнительные ключи несут позиционную информацию. модель показала хорошие результаты в бенчмарках. в mmlu она добилась показателя в 78,3%, а в тестах на китайском языке, таких как cluewsc, c-eval и cmmlu, deepseek-v2 показала лучшие результаты среди всех открытых моделей. разбор подготовил ❣ сергей горбунов душный nlp deepseek-v2 — moe-модель с технологией mla компания deepseek представила moe-модель deepseek-v2 на 236 миллиардов параметров. сегодня мы разберём статью , которая рассказывает об особенностях этой модели. модель состоит из 236 миллиардов параметров, однако на каждый токен активно используется лишь 27 миллиардов. это позволило значительно увеличить длину контекста до 128 тысяч токенов. для сравнения и оценки результатов в deepseek использовали dense-модель на 67b. авторы смогли существенно снизить стоимость обучения и уменьшить размер kv-кэша, что позволило увеличить скорость генерации токенов почти в 8 раз. эти достижения обеспечили модели преимущество в производительности. в deepseek-v2 внедрён модифицированный attention, который получил название mla (multi-head latent attention). этот механизм позволяет кэшировать уменьшенный вектор с последующим его восстановлением, что значительно снижает объём ресурсов, необходимый для работы модели. как устроен mla в сравнении с mha, gqa и mqa, вы можете увидеть на изображении выше. при таком подходе нельзя напрямую использовать rope (rotary position embedding) — в противном случае матрица rope становится неразделимой с остальными операциями, что значительно снижает эффективность вывода. в deepseek предложили стратегию так называемого «развязанного rope» (decoupled rope), в которой дополнительные ключи несут позиционную информацию. модель показала хорошие результаты в бенчмарках. в mmlu она добилась показателя в 78,3%, а в тестах на китайском языке, таких как cluewsc, c-eval и cmmlu, deepseek-v2 показала лучшие результаты среди всех открытых моделей. разбор подготовил ❣ сергей горбунов душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-08-01T12:21:10+00:00" href="./posts/27.html">2024-08-01 12:21 UTC</a></div>
      </div>
      <div class="post-body"><strong>DeepSeek-V2 — MoE-модель с технологией MLA</strong><br><br>Компания DeepSeek представила MoE-модель DeepSeek-V2 на 236 миллиардов параметров. Сегодня мы разберём <a href="https://arxiv.org/abs/2405.04434" rel="nofollow noopener noreferrer">статью</a>, которая рассказывает об особенностях этой модели. <br><br>Модель состоит из 236 миллиардов параметров, однако на каждый токен активно используется лишь 27 миллиардов. Это позволило значительно увеличить длину контекста до 128 тысяч токенов. <br><br>Для сравнения и оценки результатов в DeepSeek использовали dense-модель на 67b. Авторы смогли существенно снизить стоимость обучения и уменьшить размер KV-кэша, что позволило увеличить скорость генерации токенов почти в 8 раз. Эти достижения обеспечили модели преимущество в производительности.<br><br>В DeepSeek-V2 внедрён модифицированный attention, который получил название MLA (Multi-Head Latent Attention). Этот механизм позволяет кэшировать уменьшенный вектор с последующим его восстановлением, что значительно снижает объём ресурсов, необходимый для работы модели. Как устроен MLA в сравнении с MHA, GQA и MQA, вы можете увидеть на изображении выше. <br><br>При таком подходе нельзя напрямую использовать RoPE (Rotary Position Embedding) — в противном случае матрица RoPE становится неразделимой с остальными операциями, что значительно снижает эффективность вывода. В DeepSeek предложили стратегию так называемого «развязанного RoPE» (decoupled RoPE), в которой дополнительные ключи несут позиционную информацию. <br><br>Модель показала хорошие результаты в бенчмарках. В MMLU она добилась показателя в 78,3%, а в тестах на китайском языке, таких как CLUEWSC, C-Eval и CMMLU, DeepSeek-V2 показала лучшие результаты среди всех открытых моделей.<br><br><em>Разбор подготовил </em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> <em>Сергей Горбунов</em><br><br><a href="https://t.me/+L_HwcBzYf7E0OWRi" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/27_480.webp" srcset="../assets/media/thumbs/27_480.webp 480w, ../assets/media/27.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="27" data-image-index="0" /></div></div>
      <div class="actions">
        <span>7 577 просмотров · 53 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/27" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/27.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="26" data-search="как устроена nemotron-4 340b компания nvidia выпустила одну из самых крупных и качественных open source моделей — nemotron-4 340b. сегодня разберём сразу две статьи, в которых рассказывается об её устройстве. на этапе претрейна nvidia использовала классические эвристики для дедупликации и отбора лучших текстов, что позволило создать качественный корпус данных. основная его часть составлена из источников на английском, однако есть тексты и на других языках. вопреки современным трендам, nvidia использовала функцию активации squared relu, но при этом отказалась от гейта. параметризация layer-norm также нестандартна: вместо инициализации единицами применяется инициализация нулями с добавлением единицы на каждом форварде. в датасетах alignment nvidia использовала небольшое количество человеческой разметки. датасет обучения reward-модели включает в себя всего лишь 10 тысяч пар ответов, размеченных по пяти параметрам: от общей пользы до связности и сложности. sft также включал лишь 10 тысяч написанных человеком примеров. при этом суммарно в sft-датасетах вышло порядка миллиона примеров, из которых почти все были сгенерированы. интересно, что сгенерированы были и сами промты — для этого nvidia использовала mixtral 8x7b. по итогу получилось два sft-датасета и два раунда обучения. сперва модель обучали code sft на 800 тысячах примеров. а затем ту же модель — дообучали general sft ещё на 200 тысячах примеров. далее следовал один раунд dpo и три раунда собственного метода nvidia — rpo (reward-aware preference optimization). для обучения стандартного dpo используется только порядок внутри пары: оптимальная политика максимизирует расстояние между наградой за выбранный и отклонённый ответ. такой подход может приводить к переобучению, поэтому в rpo авторы предложили аппроксимировать саму разницу между наградами, которые считают reward-моделью. после всех этапов обучения и тестирования модель nemotron показала хорошие результаты на бенчмарках вроде mmlu. интересно, что модель почти не росла после раунда dpo, но последующие rpo способствовали улучшению. nematron опережает большинство других open source-решений и тягается почти на равных с закрытыми. скажем, в arena hard модель nvidia показала результат 54,2, что выше чем у claude 3 sonnet (46,8) и qwen2 72b (48,1). в сравнении, проведённом людьми, у nemotron практически паритет c gpt-4 по всем задачам, исключая переписывание текста. здесь модель nvidia справлялась лишь в 3,03% случаев. однако стоит учитывать, что «живые» замеры проводилось на небольшом количестве примеров — их было всего 136. рассказывайте в комментариях, что думаете о nemotron-4? разбор подготовил ❣ руслан васильев душный nlp как устроена nemotron-4 340b компания nvidia выпустила одну из самых крупных и качественных open source моделей — nemotron-4 340b. сегодня разберём сразу две статьи , в которых рассказывается об её устройстве. на этапе претрейна nvidia использовала классические эвристики для дедупликации и отбора лучших текстов, что позволило создать качественный корпус данных. основная его часть составлена из источников на английском, однако есть тексты и на других языках. вопреки современным трендам, nvidia использовала функцию активации squared relu, но при этом отказалась от гейта. параметризация layer-norm также нестандартна: вместо инициализации единицами применяется инициализация нулями с добавлением единицы на каждом форварде. в датасетах alignment nvidia использовала небольшое количество человеческой разметки. датасет обучения reward-модели включает в себя всего лишь 10 тысяч пар ответов, размеченных по пяти параметрам: от общей пользы до связности и сложности. sft также включал лишь 10 тысяч написанных человеком примеров. при этом суммарно в sft-датасетах вышло порядка миллиона примеров, из которых почти все были сгенерированы. интересно, что сгенерированы были и сами промты — для этого nvidia использовала mixtral 8x7b . по итогу получилось два sft-датасета и два раунда обучения. сперва модель обучали code sft на 800 тысячах примеров. а затем ту же модель — дообучали general sft ещё на 200 тысячах примеров. далее следовал один раунд dpo и три раунда собственного метода nvidia — rpo (reward-aware preference optimization). для обучения стандартного dpo используется только порядок внутри пары: оптимальная политика максимизирует расстояние между наградой за выбранный и отклонённый ответ. такой подход может приводить к переобучению, поэтому в rpo авторы предложили аппроксимировать саму разницу между наградами, которые считают reward-моделью. после всех этапов обучения и тестирования модель nemotron показала хорошие результаты на бенчмарках вроде mmlu. интересно, что модель почти не росла после раунда dpo, но последующие rpo способствовали улучшению. nematron опережает большинство других open source-решений и тягается почти на равных с закрытыми. скажем, в arena hard модель nvidia показала результат 54,2, что выше чем у claude 3 sonnet (46,8) и qwen2 72b (48,1). в сравнении, проведённом людьми, у nemotron практически паритет c gpt-4 по всем задачам, исключая переписывание текста. здесь модель nvidia справлялась лишь в 3,03% случаев. однако стоит учитывать, что «живые» замеры проводилось на небольшом количестве примеров — их было всего 136. рассказывайте в комментариях, что думаете о nemotron-4? разбор подготовил ❣ руслан васильев душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-07-25T11:21:57+00:00" href="./posts/26.html">2024-07-25 11:21 UTC</a></div>
      </div>
      <div class="post-body"><strong>Как устроена Nemotron-4 340b</strong><br><br>Компания NVIDIA выпустила одну из самых крупных и качественных open source моделей — Nemotron-4 340B. Сегодня разберём сразу <a href="https://research.nvidia.com/publication/2024-06_nemotron-4-340b" rel="nofollow noopener noreferrer">две статьи</a>, в которых <a href="https://arxiv.org/abs/2402.16819" rel="nofollow noopener noreferrer">рассказывается</a> об её устройстве. <br><br>На этапе претрейна NVIDIA использовала классические эвристики для дедупликации и отбора лучших текстов, что позволило создать качественный корпус данных. Основная его часть составлена из источников на английском, однако есть тексты и на других языках. <br><br>Вопреки современным трендам, NVIDIA использовала функцию активации squared ReLU, но при этом отказалась от гейта. Параметризация layer-norm также нестандартна: вместо инициализации единицами применяется инициализация нулями с добавлением единицы на каждом форварде. <br><br>В датасетах alignment NVIDIA использовала небольшое количество человеческой разметки. <a href="https://arxiv.org/abs/2406.08673" rel="nofollow noopener noreferrer">Датасет обучения</a> reward-модели включает в себя всего лишь 10 тысяч пар ответов, размеченных по пяти параметрам: от общей пользы до связности и сложности.<br><br>SFT также включал лишь 10 тысяч написанных человеком примеров. При этом суммарно в SFT-датасетах вышло порядка миллиона примеров, из которых почти все были сгенерированы. Интересно, что сгенерированы были и сами промты — для этого NVIDIA использовала <a href="https://arxiv.org/abs/2401.04088" rel="nofollow noopener noreferrer">Mixtral 8x7B</a>. <br><br>По итогу получилось два SFT-датасета и два раунда обучения. Сперва модель обучали Code SFT на 800 тысячах примеров. А затем ту же модель — дообучали General SFT ещё на 200 тысячах примеров. <br><br>Далее следовал один раунд DPO и три раунда собственного метода NVIDIA — RPO (Reward-aware Preference Optimization). Для обучения стандартного DPO используется только порядок внутри пары: оптимальная политика максимизирует расстояние между наградой за выбранный и отклонённый ответ. Такой подход может приводить к переобучению, поэтому в RPO авторы предложили аппроксимировать саму разницу между наградами, которые считают Reward-моделью.<br><br>После всех этапов обучения и тестирования модель Nemotron показала хорошие результаты на бенчмарках вроде MMLU. Интересно, что модель почти не росла после раунда DPO, но последующие RPO способствовали улучшению. <br><br>Nematron опережает большинство других open source-решений и тягается почти на равных с закрытыми. Скажем, в Arena Hard модель NVIDIA показала результат 54,2, что выше чем у Claude 3 Sonnet (46,8) и Qwen2 72b (48,1). <br><br>В сравнении, проведённом людьми, у Nemotron практически паритет c GPT-4 по всем задачам, исключая переписывание текста. Здесь модель NVIDIA справлялась лишь в 3,03% случаев. Однако стоит учитывать, что «живые» замеры проводилось на небольшом количестве примеров — их было всего 136. <br><br>Рассказывайте в комментариях, что думаете о Nemotron-4?<br><br><em>Разбор подготовил </em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> <em>Руслан Васильев</em><br><br><a href="https://t.me/+y9m_2tGV5AQ4NzVi" rel="nofollow noopener noreferrer">Душный NLP</a></div>
      <div class="actions">
        <span>6 277 просмотров · 44 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/26" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/26.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="25" data-search="mixture-of-agents — простой способ улучшения ответов llm сегодня рассмотрим статью, которая описывает метод улучшения результатов llm на разных бенчмарках без дообучения. он называется mixture-of-agents (moa). суть метода заключается в использовании нескольких llm для генерации ответов. авторы статьи создали многослойную структуру с несколькими агентами — собственно, моделями — на каждом слое. на вход подавали один вопрос. каждый из агентов давал ответ. затем полученные данные агрегировались и вместе с промптом передавались на следующий слой, где процесс запускался заново. в итоге получался ответ, который превосходит по качеству все предыдущие. интересно то, что модели показывают лучшие результаты, когда имеют доступ к выходным данным других llm — даже если ответы последних не слишком качественные. этот феномен авторы назвали «коллаборативностью llm» (сollaborativeness of llms). эксперименты показали, что использование разных llm на разных слоях улучшает результаты. агрегаторы тоже играют важную роль — если пропоузеры могут быть относительно простыми и легкими, то агрегаторы требуют значительных вычислительных ресурсов. бенчмарки подтвердили, что moa — эффективный метод. скажем, на alpacaeval 2.0 и mt-bench применение такой архитектуры дало прирост производительности до 8% по сравнению с gpt-4 omni. впрочем, moa есть куда расти. например, в области уменьшения времени до первого токена. из-за итеративной агрегации конечному пользователю приходится долго ждать ответа на вопрос. авторы статьи намерены бороться с этим недостатком. рассказывайте в комментариях, что думаете о moa? разбор подготовил ❣ никита шевченко душный nlp mixture-of-agents — простой способ улучшения ответов llm сегодня рассмотрим статью , которая описывает метод улучшения результатов llm на разных бенчмарках без дообучения. он называется mixture-of-agents (moa) . суть метода заключается в использовании нескольких llm для генерации ответов. авторы статьи создали многослойную структуру с несколькими агентами — собственно, моделями — на каждом слое. на вход подавали один вопрос. каждый из агентов давал ответ. затем полученные данные агрегировались и вместе с промптом передавались на следующий слой, где процесс запускался заново. в итоге получался ответ, который превосходит по качеству все предыдущие. интересно то, что модели показывают лучшие результаты, когда имеют доступ к выходным данным других llm — даже если ответы последних не слишком качественные. этот феномен авторы назвали «коллаборативностью llm» (сollaborativeness of llms). эксперименты показали, что использование разных llm на разных слоях улучшает результаты. агрегаторы тоже играют важную роль — если пропоузеры могут быть относительно простыми и легкими, то агрегаторы требуют значительных вычислительных ресурсов. бенчмарки подтвердили, что moa — эффективный метод. скажем, на alpacaeval 2.0 и mt-bench применение такой архитектуры дало прирост производительности до 8% по сравнению с gpt-4 omni. впрочем, moa есть куда расти. например, в области уменьшения времени до первого токена. из-за итеративной агрегации конечному пользователю приходится долго ждать ответа на вопрос. авторы статьи намерены бороться с этим недостатком. рассказывайте в комментариях, что думаете о moa? разбор подготовил ❣ никита шевченко душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-07-17T09:48:06+00:00" href="./posts/25.html">2024-07-17 09:48 UTC</a></div>
      </div>
      <div class="post-body"><strong>Mixture-of-Agents — простой способ улучшения ответов LLM</strong><br><br>Сегодня рассмотрим <a href="https://www.researchgate.net/publication/381294672_Mixture-of-Agents_Enhances_Large_Language_Model_Capabilities" rel="nofollow noopener noreferrer">статью</a>, которая описывает метод улучшения результатов LLM на разных бенчмарках без дообучения. Он называется <a href="https://github.com/togethercomputer/MoA/tree/main" rel="nofollow noopener noreferrer">Mixture-of-Agents (MoA)</a>.<br><br>Суть метода заключается в использовании нескольких LLM для генерации ответов. Авторы статьи создали многослойную структуру с несколькими агентами — собственно, моделями — на каждом слое. На вход подавали один вопрос. Каждый из агентов давал ответ. Затем полученные данные агрегировались и вместе с промптом передавались на следующий слой, где процесс запускался заново. <br><br>В итоге получался ответ, который превосходит по качеству все предыдущие. Интересно то, что модели показывают лучшие результаты, когда имеют доступ к выходным данным других LLM — даже если ответы последних не слишком качественные. Этот феномен авторы назвали «коллаборативностью LLM» (Сollaborativeness of LLMs).<br><br>Эксперименты показали, что использование разных LLM на разных слоях улучшает результаты. Агрегаторы тоже играют важную роль — если пропоузеры могут быть относительно простыми и легкими, то агрегаторы требуют значительных вычислительных ресурсов. <br><br>Бенчмарки подтвердили, что MoA — эффективный метод. Скажем, на AlpacaEval 2.0 и MT-Bench применение такой архитектуры дало прирост производительности до 8% по сравнению с GPT-4 Omni. <br><br>Впрочем, MoA есть куда расти. Например, в области уменьшения времени до первого токена. Из-за итеративной агрегации конечному пользователю приходится долго ждать ответа на вопрос. Авторы статьи намерены бороться с этим недостатком.<br><br>Рассказывайте в комментариях, что думаете о MoA?<br><br>Разбор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Никита Шевченко<br><br><a href="https://t.me/+mbHFn1WXH0sxNzIy" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/25_480.webp" srcset="../assets/media/thumbs/25_480.webp 480w, ../assets/media/25.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="25" data-image-index="0" /></div></div>
      <div class="actions">
        <span>10 376 просмотров · 85 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/25" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/25.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="24" data-search="mamba и трансформеры ssm вроде mamba — неплохая альтернатива трансформерам, но с ними всё не так просто. сегодняшняя статья об этом. трансформеры, такие как bert и gpt, показывают отличные результаты в nlp-задачах, но испытывают сложности при работе с большими контекстами —при увеличении длины входного текста качество начинает деградировать. причина в росте вычислительных затрат и сложности поддержания высоких скоростей обучения и инференса. решают проблему разными оптимизациями: линейными аттеншн-механизмами, структурированными масками токенов. однако у этих способов есть ограничения. более удачная альтернатива — mamba и mamba 2. о ней мы уже писали в этом посте. mamba и mamba 2 используют иной подход к обработке длинных текстов, основываясь на структурированных состояниях. в отличие от трансформеров, они не требуют обработки всех токенов одновременно. авторы статьи сравнили mamba, mamba 2 и классический трансформер на нескольких задачах. в некоторых результаты mamba были сопоставимы с результатами трансформеров, но mamba тоже испытывает трудности при работе с большими контекстами. для решения проблемы авторы предложили гибридную модель — смесь mamba 2, селф-атеншн и mlp-слоёв. у первой модели было 130м параметров с 24 слоями. доля селф-аттеншн и mlp среди них менялась. наименьшей потери валидации удалось достичь при 8% селф-аттеншн слоёв. эти данные подтвердили эксперименты и с моделью на 840м параметров. при этом доля mlp достигала около 50%, что позволило ускорить инференс на 20%. авторы натренировали 8b-модель mamba-2-hybrid. она сравнялась по качеству с трансформерами. благодаря замене слоёв аттеншена ssm-слоями в теории можно добиться ускорения x7. также есть гипотеза, что уменьшение кэша key-value позволит модели использовать более крупные батчи, чем трансформерам, и ещё сильнее ускорить инференс. как вам такой подход? делитесь впечатлениями в комментариях! разбор подготовил ❣ никита шаповалов душный nlp mamba и трансформеры ssm вроде mamba — неплохая альтернатива трансформерам, но с ними всё не так просто. сегодняшняя статья об этом. трансформеры, такие как bert и gpt, показывают отличные результаты в nlp-задачах, но испытывают сложности при работе с большими контекстами —при увеличении длины входного текста качество начинает деградировать. причина в росте вычислительных затрат и сложности поддержания высоких скоростей обучения и инференса. решают проблему разными оптимизациями: линейными аттеншн-механизмами, структурированными масками токенов. однако у этих способов есть ограничения. более удачная альтернатива — mamba и mamba 2. о ней мы уже писали в этом посте . mamba и mamba 2 используют иной подход к обработке длинных текстов, основываясь на структурированных состояниях. в отличие от трансформеров, они не требуют обработки всех токенов одновременно. авторы статьи сравнили mamba, mamba 2 и классический трансформер на нескольких задачах. в некоторых результаты mamba были сопоставимы с результатами трансформеров, но mamba тоже испытывает трудности при работе с большими контекстами. для решения проблемы авторы предложили гибридную модель — смесь mamba 2, селф-атеншн и mlp-слоёв. у первой модели было 130м параметров с 24 слоями. доля селф-аттеншн и mlp среди них менялась. наименьшей потери валидации удалось достичь при 8% селф-аттеншн слоёв. эти данные подтвердили эксперименты и с моделью на 840м параметров. при этом доля mlp достигала около 50%, что позволило ускорить инференс на 20%. авторы натренировали 8b-модель mamba-2-hybrid. она сравнялась по качеству с трансформерами. благодаря замене слоёв аттеншена ssm-слоями в теории можно добиться ускорения x7. также есть гипотеза, что уменьшение кэша key-value позволит модели использовать более крупные батчи, чем трансформерам, и ещё сильнее ускорить инференс. как вам такой подход? делитесь впечатлениями в комментариях! разбор подготовил ❣ никита шаповалов душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-07-12T10:51:49+00:00" href="./posts/24.html">2024-07-12 10:51 UTC</a></div>
      </div>
      <div class="post-body"><strong>Mamba и трансформеры</strong><br><br>SSM вроде Mamba — неплохая альтернатива трансформерам, но с ними всё не так просто. Сегодняшняя <a href="https://arxiv.org/abs/2406.07887" rel="nofollow noopener noreferrer">статья</a> об этом. <br><br>Трансформеры, такие как BERT и GPT, показывают отличные результаты в NLP-задачах, но испытывают сложности при работе с большими контекстами —при увеличении длины входного текста качество начинает деградировать. Причина в росте вычислительных затрат и сложности поддержания высоких скоростей обучения и инференса.<br><br>Решают проблему разными оптимизациями: линейными аттеншн-механизмами, структурированными масками токенов. Однако у этих способов есть ограничения. Более удачная альтернатива — Mamba и Mamba 2. О ней мы уже писали <a href="https://t.me/stuffyNLP/21" rel="nofollow noopener noreferrer">в этом посте</a>. <br><br>Mamba и Mamba 2 используют иной подход к обработке длинных текстов, основываясь на структурированных состояниях. В отличие от трансформеров, они не требуют обработки всех токенов одновременно.<br><br>Авторы статьи сравнили Mamba, Mamba 2 и классический трансформер на нескольких задачах. В некоторых результаты Mamba были сопоставимы с результатами трансформеров, но Mamba тоже испытывает трудности при работе с большими контекстами. <br><br>Для решения проблемы авторы предложили гибридную модель — смесь Mamba 2, селф-атеншн и MLP-слоёв. У первой модели было 130М параметров с 24 слоями. Доля селф-аттеншн и MLP среди них менялась. Наименьшей потери валидации удалось достичь при 8% селф-аттеншн слоёв. Эти данные подтвердили эксперименты и с моделью на 840М параметров. При этом доля MLP достигала около 50%, что позволило ускорить инференс на 20%.<br><br>Авторы натренировали 8B-модель Mamba-2-Hybrid. Она сравнялась по качеству с трансформерами. Благодаря замене слоёв аттеншена SSM-слоями в теории можно добиться ускорения x7. Также есть гипотеза, что уменьшение кэша key-value позволит модели использовать более крупные батчи, чем трансформерам, и ещё сильнее ускорить инференс. <br><br>Как вам такой подход? Делитесь впечатлениями в комментариях! <br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Никита Шаповалов </em><br><br><a href="https://t.me/+HyJnGHP84ddlNjgy" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/24_480.webp" srcset="../assets/media/thumbs/24_480.webp 480w, ../assets/media/24.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="24" data-image-index="0" /></div></div>
      <div class="actions">
        <span>6 024 просмотров · 58 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/24" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/24.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="23" data-search="о файнтюнинге после квантизации сжатие языковых моделей трансформеров, таких как llama, opt и gpt — важно для повышения их эффективности и скорости работы. этому аспекту посвящена статья, которую мы сегодня разберём. результаты в этой публикации оказались лучше, чем в январской статье extreme compression of large language models via additive quantization благодаря улучшению шага файнтюнинга после основной квантизации. наиболее популярный и эффективный метод сжатия — квантизация. это уменьшение количества бит, используемых для представления весов модели, что позволяет снизить размер llm и ускорить её работу. при этом важно поддерживать баланс между степенью сжатия и качеством, поскольку экстремальное сжатие (например, до двух или одного бита) может значительно ухудшить качество. квантизация без файнтюнинга часто приводит к снижению качества. поэтому после квантизации модель следует дообучать для приближения к результатам неквантизованной модели. это особенно важно при экстремальных уровнях сжатия — до одного или полутора бит. популярная техника файнюнинга квантизированных весов — straight-through estimation. однако в её рамках улучшение может идти непредсказуемо плохими градиентами. авторы статьи предлагают другой способ. достаточно обновлять лишь небольшую долю весов в рамках одного шага — те, у которых самый большой градиент. эти весы нужно обучить, а остальные — «заморозить». затем следует пройтись большими шагами между «выжившими» весами. таким образом, обновятся все или почти все веса. этот метод позволяет достичь оптимума по парето, например, для моделей семейства llama-2 при 2 битах. а для моделей сжатых до 1-1,5 бита на параметр результаты оказались лучше чем у аналогов. рассказывайте в комментариях, что вы думаете об этом методе и делитесь опытом! разбор подготовил ❣ владимир малиновский душный nlp о файнтюнинге после квантизации сжатие языковых моделей трансформеров, таких как llama, opt и gpt — важно для повышения их эффективности и скорости работы. этому аспекту посвящена статья , которую мы сегодня разберём. результаты в этой публикации оказались лучше, чем в январской статье extreme compression of large language models via additive quantization благодаря улучшению шага файнтюнинга после основной квантизации. наиболее популярный и эффективный метод сжатия — квантизация. это уменьшение количества бит, используемых для представления весов модели, что позволяет снизить размер llm и ускорить её работу. при этом важно поддерживать баланс между степенью сжатия и качеством, поскольку экстремальное сжатие (например, до двух или одного бита) может значительно ухудшить качество. квантизация без файнтюнинга часто приводит к снижению качества. поэтому после квантизации модель следует дообучать для приближения к результатам неквантизованной модели. это особенно важно при экстремальных уровнях сжатия — до одного или полутора бит. популярная техника файнюнинга квантизированных весов — straight-through estimation. однако в её рамках улучшение может идти непредсказуемо плохими градиентами. авторы статьи предлагают другой способ. достаточно обновлять лишь небольшую долю весов в рамках одного шага — те, у которых самый большой градиент. эти весы нужно обучить, а остальные — «заморозить». затем следует пройтись большими шагами между «выжившими» весами. таким образом, обновятся все или почти все веса. этот метод позволяет достичь оптимума по парето, например, для моделей семейства llama-2 при 2 битах. а для моделей сжатых до 1-1,5 бита на параметр результаты оказались лучше чем у аналогов. рассказывайте в комментариях, что вы думаете об этом методе и делитесь опытом! разбор подготовил ❣ владимир малиновский душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-07-04T10:27:59+00:00" href="./posts/23.html">2024-07-04 10:27 UTC</a></div>
      </div>
      <div class="post-body"><strong>О файнтюнинге после квантизации</strong><br><br>Сжатие языковых моделей трансформеров, таких как LLaMA, OPT и GPT — важно для повышения их эффективности и скорости работы. Этому аспекту посвящена <a href="https://arxiv.org/abs/2405.14852" rel="nofollow noopener noreferrer">статья</a>, которую мы сегодня разберём. Результаты в этой публикации оказались лучше, чем в январской статье <a href="https://arxiv.org/html/2401.06118v2" rel="nofollow noopener noreferrer">Extreme Compression of Large Language Models via Additive Quantization</a> благодаря улучшению шага файнтюнинга после основной квантизации. <br><br>Наиболее популярный и эффективный метод сжатия — квантизация. Это уменьшение количества бит, используемых для представления весов модели, что позволяет снизить размер LLM и ускорить её работу. При этом важно поддерживать баланс между степенью сжатия и качеством, поскольку экстремальное сжатие (например, до двух или одного бита) может значительно ухудшить качество.<br><br>Квантизация без файнтюнинга часто приводит к снижению качества. Поэтому после квантизации модель следует дообучать для приближения к результатам неквантизованной модели. Это особенно важно при экстремальных уровнях сжатия — до одного или полутора бит.<br><br>Популярная техника файнюнинга квантизированных весов — straight-through estimation. Однако в её рамках улучшение может идти непредсказуемо плохими градиентами. <br><br>Авторы статьи предлагают другой способ. Достаточно обновлять лишь небольшую  долю весов в рамках одного шага —  те, у которых самый большой градиент. Эти весы нужно обучить, а остальные — «заморозить». Затем следует пройтись большими шагами между «выжившими» весами. Таким образом, обновятся все или почти все веса. <br><br>Этот метод позволяет достичь оптимума по Парето, например, для моделей семейства LLaMA-2 при 2 битах. А для моделей сжатых до 1-1,5 бита на параметр результаты оказались лучше чем у аналогов.<br><br>Рассказывайте в комментариях, что вы думаете об этом методе и делитесь опытом! <br><br>Разбор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Владимир Малиновский<br><br><a href="https://t.me/stuffyNLP" rel="nofollow noopener noreferrer">Душный NLP</a></div>
      <div class="actions">
        <span>6 354 просмотров · 73 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/23" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/23.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="22" data-search="specexec: cпекулятивное декодирование для запуска больших моделей на потребительских gpu генерация текста llm на gpu потребительского класса — сложная задача. стандартные алгоритмы требуют исполнения всех слоёв модели для получения каждого токена. модели размером в 10+b не помещаются в память gpu — приходится прибегать к офлодингу (offloading), поочерёдно подгружая слои из основной памяти в vram. это долго: одна итерация загрузки и генерации одного токена llama-2-70b в 16-битном режиме с pcie gen 4 может занять &gt; 5 секунд. спекулятивное декодирование ускоряет генерацию. это достигается за счёт дополнительной «черновой» модели — более компактной и быстрой. она предлагает варианты продолжения цепочек токенов. основная модель проверяет эти варианты, выбирая один с помощью стохастического алгоритма выборки. производительность измеряется числом токенов, сгенерированных за итерацию. specexec — самый производительный метод в классе. он генерирует до 20 токенов за итерацию и достигает ускорения x15 при офлодинге. вместо стохастического метода, specexec создаёт «кеш» в форме дерева продолжений, используя не случайные, а самые вероятные токены из модели-черновика. целевая модель проверяет их за один проход. алгоритм specexec производительнее, т.к. использует высокую пиковость распределений вероятностей токенов в современных llm. например, в llama-2-70b высочайшая вероятность токена составляет более 90%. сравнение производительности в выгрузке specexec превосходит specinfer — особенно с большими бюджетами токенов. в то время как производительность specinfer перестаёт расти с ростом бюджета, наш метод генерирует более 20 токенов за шаг при бюджетах 1000+ токенов. в тестах с офлодингом specexec показывает стабильный прирост скорости на видеокартах от высококлассных исследовательских gpu a100/h100 до потребительских gpu: 4090 и даже 2080. с квантованными моделями specexec достигает ускорения от 4.6x до 10.6x, генерируя от 3 до 6 токенов в секунду. *** specexec упрощает доступ к мощным llm и обеспечивает инференс тяжёлых моделей на оборудовании, считавшемся недостаточно мощным. узнать о нём больше можно в статье, а пощупать — на github. разбор подготовил ❣ руслан свирщевский душный nlp specexec: cпекулятивное декодирование для запуска больших моделей на потребительских gpu генерация текста llm на gpu потребительского класса — сложная задача. стандартные алгоритмы требуют исполнения всех слоёв модели для получения каждого токена. модели размером в 10+b не помещаются в память gpu — приходится прибегать к офлодингу (offloading), поочерёдно подгружая слои из основной памяти в vram. это долго: одна итерация загрузки и генерации одного токена llama-2-70b в 16-битном режиме с pcie gen 4 может занять &amp;gt; 5 секунд. спекулятивное декодирование ускоряет генерацию. это достигается за счёт дополнительной «черновой» модели — более компактной и быстрой. она предлагает варианты продолжения цепочек токенов. основная модель проверяет эти варианты, выбирая один с помощью стохастического алгоритма выборки. производительность измеряется числом токенов, сгенерированных за итерацию. specexec — самый производительный метод в классе. он генерирует до 20 токенов за итерацию и достигает ускорения x15 при офлодинге. вместо стохастического метода, specexec создаёт «кеш» в форме дерева продолжений, используя не случайные, а самые вероятные токены из модели-черновика. целевая модель проверяет их за один проход. алгоритм specexec производительнее, т.к. использует высокую пиковость распределений вероятностей токенов в современных llm. например, в llama-2-70b высочайшая вероятность токена составляет более 90%. сравнение производительности в выгрузке specexec превосходит specinfer — особенно с большими бюджетами токенов. в то время как производительность specinfer перестаёт расти с ростом бюджета, наш метод генерирует более 20 токенов за шаг при бюджетах 1000+ токенов. в тестах с офлодингом specexec показывает стабильный прирост скорости на видеокартах от высококлассных исследовательских gpu a100/h100 до потребительских gpu: 4090 и даже 2080. с квантованными моделями specexec достигает ускорения от 4.6x до 10.6x, генерируя от 3 до 6 токенов в секунду. *** specexec упрощает доступ к мощным llm и обеспечивает инференс тяжёлых моделей на оборудовании, считавшемся недостаточно мощным. узнать о нём больше можно в статье , а пощупать — на github . разбор подготовил ❣ руслан свирщевский душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-06-27T09:51:40+00:00" href="./posts/22.html">2024-06-27 09:51 UTC</a></div>
      </div>
      <div class="post-body"><strong>SpecExec: cпекулятивное декодирование для запуска больших моделей на потребительских GPU</strong><br><br>Генерация текста LLM на GPU потребительского класса — сложная задача. Стандартные алгоритмы требуют исполнения всех слоёв модели для получения каждого токена. Модели размером в 10+B не помещаются в память GPU — приходится прибегать к офлодингу (offloading), поочерёдно подгружая слои из основной памяти в VRAM. Это долго: одна итерация загрузки и генерации одного токена Llama-2-70B в 16-битном режиме с PCIe gen 4 может занять &gt; 5 секунд.<br><br>Спекулятивное декодирование ускоряет генерацию. Это достигается за счёт дополнительной «черновой» модели — более компактной и быстрой. Она предлагает варианты продолжения цепочек токенов. Основная модель проверяет эти варианты, выбирая один с помощью стохастического алгоритма выборки. Производительность измеряется числом токенов, сгенерированных за итерацию.<br><br>SpecExec — самый производительный метод в классе. Он генерирует до 20 токенов за итерацию и достигает ускорения x15 при офлодинге. Вместо стохастического метода, SpecExec создаёт «кеш» в форме дерева продолжений, используя не случайные, а самые вероятные токены из модели-черновика. Целевая модель проверяет их за один проход. <br><br>Алгоритм SpecExec производительнее, т.к. использует высокую пиковость распределений вероятностей токенов в современных LLM. Например, в Llama-2-70B высочайшая вероятность токена составляет более 90%. <br><br><strong>Сравнение производительности</strong><br><br>В выгрузке SpecExec превосходит SpecInfer — особенно с большими бюджетами токенов. в то время как  производительность SpecInfer перестаёт расти с ростом бюджета, наш метод генерирует более 20 токенов за шаг при бюджетах 1000+ токенов.<br><br>В тестах с офлодингом SpecExec показывает стабильный прирост скорости на видеокартах от высококлассных исследовательских GPU A100/H100 до потребительских GPU: 4090 и даже 2080. С квантованными моделями SpecExec достигает ускорения от 4.6x до 10.6x, генерируя от 3 до 6 токенов в секунду.<br><br><strong>***</strong><br><br>SpecExec упрощает доступ к мощным LLM и обеспечивает инференс тяжёлых моделей на оборудовании, считавшемся недостаточно мощным. Узнать о нём больше можно <a href="https://arxiv.org/abs/2406.02532" rel="nofollow noopener noreferrer">в статье</a>, а пощупать — <a href="https://github.com/yandex-research/specexec" rel="nofollow noopener noreferrer">на GitHub</a>.<br><br>Разбор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Руслан Свирщевский<br><br><a href="https://t.me/+wusTNmGO86Q1ZWIy" rel="nofollow noopener noreferrer">Душный NLP</a></div>
      <div class="actions">
        <span>13 019 просмотров · 55 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/22" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/22.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="21" data-search="mamba — что это такое и как себя показывает mamba — это llm, которая основана на ssm (state space models). о ней рассказывается в статье, которую мы сегодня и разберем. для начала напомним, что такое ssm. это модели, которые позволяют эффективно работать с длинными последовательностями данных. что-то вроде rnn, но без нелинейности, благодаря чему запись можно осуществлять в виде одной большой матричной операции. mamba сочетает в себе преимущества ssm и gated mlp. вот, как это работает. сначала к последовательности применяется in projection, увеличивается количество каналов. затем используется 1d-конволюция и нелинейность. после этого, как в gated mlp, происходит поэлементное умножение параллельного входа и выходной projection. таких блоков может быть несколько. какие преимущества даёт такой подход • mamba использует recomputation во время backward pass, что позволяет эффективно утилизировать памяти. это соответствует таким подходам, как flash attention, которые разработаны для оптимизации памяти. • на длинных последовательностях mamba превосходит transformer++, особенно при увеличении числа flops и размера модели. при этом на коротких последовательностях результаты сравнимы с transformer++. • модель может работать с контекстом до одного миллиона токенов, что демонстрирует её способность обрабатывать очень длинные последовательности. • mamba показывает превосходные результаты на различных бенчмарках, таких как lambada и piqa, даже при сравнении с моделями в два раза больше. • в экспериментах mamba демонстрирует высокую эффективность на задачах копирования текста. на этом всё. рассказываете, что вы думаете о mamba в комментариях! разбор подготовил ❣ александр мазитов душный nlp mamba — что это такое и как себя показывает mamba — это llm, которая основана на ssm (state space models). о ней рассказывается в статье , которую мы сегодня и разберем. для начала напомним, что такое ssm. это модели, которые позволяют эффективно работать с длинными последовательностями данных. что-то вроде rnn, но без нелинейности, благодаря чему запись можно осуществлять в виде одной большой матричной операции. mamba сочетает в себе преимущества ssm и gated mlp. вот, как это работает. сначала к последовательности применяется in projection, увеличивается количество каналов. затем используется 1d-конволюция и нелинейность. после этого, как в gated mlp, происходит поэлементное умножение параллельного входа и выходной projection. таких блоков может быть несколько. какие преимущества даёт такой подход • mamba использует recomputation во время backward pass, что позволяет эффективно утилизировать памяти. это соответствует таким подходам, как flash attention, которые разработаны для оптимизации памяти. • на длинных последовательностях mamba превосходит transformer++, особенно при увеличении числа flops и размера модели. при этом на коротких последовательностях результаты сравнимы с transformer++. • модель может работать с контекстом до одного миллиона токенов, что демонстрирует её способность обрабатывать очень длинные последовательности. • mamba показывает превосходные результаты на различных бенчмарках, таких как lambada и piqa, даже при сравнении с моделями в два раза больше. • в экспериментах mamba демонстрирует высокую эффективность на задачах копирования текста. на этом всё. рассказываете, что вы думаете о mamba в комментариях! разбор подготовил ❣ александр мазитов душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-06-25T14:11:48+00:00" href="./posts/21.html">2024-06-25 14:11 UTC</a></div>
      </div>
      <div class="post-body"><strong>Mamba — что это такое и как себя показывает</strong><br><br>Mamba — это LLM, которая основана на SSM (State Space Models). О ней рассказывается <a href="https://arxiv.org/abs/2312.00752" rel="nofollow noopener noreferrer">в статье</a>, которую мы сегодня и разберем. <br><br>Для начала напомним, что такое SSM. Это модели, которые позволяют эффективно работать с длинными последовательностями данных. Что-то вроде RNN, но без нелинейности, благодаря чему запись можно осуществлять в виде одной большой матричной операции. <br><br>Mamba сочетает в себе преимущества SSM и Gated MLP. Вот, как это работает. Сначала к последовательности применяется in projection, увеличивается количество каналов. Затем используется 1D-конволюция и нелинейность. После этого, как в Gated MLP, происходит поэлементное умножение параллельного входа и выходной projection. Таких блоков может быть несколько.<br><br><strong>Какие преимущества даёт такой подход</strong><br><br>• Mamba использует recomputation во время backward pass, что позволяет эффективно утилизировать памяти. Это соответствует таким подходам, как Flash Attention, которые разработаны для оптимизации памяти.<br><br>• На длинных последовательностях Mamba превосходит Transformer++, особенно при увеличении числа FLOPS и размера модели. При этом на коротких последовательностях результаты сравнимы с Transformer++.<br><br>• Модель может работать с контекстом до одного миллиона токенов, что демонстрирует её способность обрабатывать очень длинные последовательности.<br><br>• Mamba показывает превосходные результаты на различных бенчмарках, таких как LAMBADA и PIQA, даже при сравнении с моделями в два раза больше.<br><br>• В экспериментах Mamba демонстрирует высокую эффективность на задачах копирования текста. <br>На этом всё. Рассказываете, что вы думаете о Mamba в комментариях! <br><br>Разбор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Александр Мазитов<br><br><a href="https://t.me/+1SlPReu1q4NlMmIy" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/21_480.webp" srcset="../assets/media/thumbs/21_480.webp 480w, ../assets/media/21.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="21" data-image-index="0" /></div></div>
      <div class="actions">
        <span>5 327 просмотров · 71 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/21" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/21.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="19" data-search="ускорить обучение llm 70b на 25%? легко! yafsdp сегодня мы выĸладываем в опенсорс наш новый инструмент — библиотеку yafsdp. она значительно ускоряет обучение больших языковых моделей — как собственной разработки, так и сторонних, с открытым исходным кодом. библиотека даёт ускорение до 25% — результат зависит от архитектуры и параметров нейросети. с помощью yafsdp также можно расходовать до 20% меньше ресурсов графических процессоров (gpu), которые требуются для обучения. несколько подходов нашего метода: — выделить два буфера под все собираемые веса, чтобы не отдавать их на отĸуп аллоĸатору памяти torch. каждый нечётный слой будет использовать первый буфер, ĸаждый чётный — второй. это уменьшит нагрузку на память и сделает её использование более предсказуемым; — не делать чеĸпоинт аĸтиваций для ĸаĸ можно большего числа слоёв. это позволит убрать избыточные вычисления за счёт сэкономленной памяти; — выделить два стрима: вычислений и ĸоммуниĸаций, а синхронизацию построить таким образом, чтобы forward не начинался до завершения all_gather того же слоя, а all_gather не начинался до освобождения соответствующего буффера на предыдущем слое; — разово собирать rmsnorm/layernorm в начале итерации и тольĸо в ĸонце усреднить градиенты; — вынести predivide в самый ĸонец backward, таĸ ĸаĸ при reduce_scatter в bf16 или fp32 рисĸа переполнения нет. более подробно про проблемы обучения на множестве gpu можно почитать на хабре. приходите в репозиторий библиотеки yafsdp, ставьте лайк и приносите вопросы в issues. а ещё — делитесь своими впечатлениями здесь в комментариях. душный nlp ускорить обучение llm 70b на 25%? легко! yafsdp сегодня мы выĸладываем в опенсорс наш новый инструмент — библиотеку yafsdp . она значительно ускоряет обучение больших языковых моделей — как собственной разработки, так и сторонних, с открытым исходным кодом. библиотека даёт ускорение до 25% — результат зависит от архитектуры и параметров нейросети. с помощью yafsdp также можно расходовать до 20% меньше ресурсов графических процессоров (gpu), которые требуются для обучения. несколько подходов нашего метода: — выделить два буфера под все собираемые веса, чтобы не отдавать их на отĸуп аллоĸатору памяти torch. каждый нечётный слой будет использовать первый буфер, ĸаждый чётный — второй. это уменьшит нагрузку на память и сделает её использование более предсказуемым; — не делать чеĸпоинт аĸтиваций для ĸаĸ можно большего числа слоёв. это позволит убрать избыточные вычисления за счёт сэкономленной памяти; — выделить два стрима: вычислений и ĸоммуниĸаций, а синхронизацию построить таким образом, чтобы forward не начинался до завершения all_gather того же слоя, а all_gather не начинался до освобождения соответствующего буффера на предыдущем слое; — разово собирать rmsnorm/layernorm в начале итерации и тольĸо в ĸонце усреднить градиенты; — вынести predivide в самый ĸонец backward, таĸ ĸаĸ при reduce_scatter в bf16 или fp32 рисĸа переполнения нет. более подробно про проблемы обучения на множестве gpu можно почитать на хабре . приходите в репозиторий библиотеки yafsdp , ставьте лайк и приносите вопросы в issues. а ещё — делитесь своими впечатлениями здесь в комментариях. душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-06-11T09:00:47+00:00" href="./posts/19.html">2024-06-11 09:00 UTC</a></div>
      </div>
      <div class="post-body"><strong>Ускорить обучение LLM 70B на 25%? Легко! YaFSDP </strong><br><br>Сегодня мы выĸладываем в опенсорс наш новый инструмент — библиотеку<a href="https://github.com/yandex/YaFSDP" rel="nofollow noopener noreferrer"> YaFSDP</a>. Она значительно ускоряет обучение больших языковых моделей — как собственной разработки, так и сторонних, с открытым исходным кодом. <br><br>Библиотека даёт ускорение до 25% — результат зависит от архитектуры и параметров нейросети. С помощью YaFSDP также можно расходовать до 20% меньше ресурсов графических процессоров (GPU), которые требуются для обучения. <br><br><strong>Несколько подходов нашего метода:</strong><br><br>— выделить два буфера под все собираемые веса, чтобы не отдавать их на отĸуп аллоĸатору памяти torch. Каждый нечётный слой будет использовать первый буфер, ĸаждый чётный — второй. Это уменьшит нагрузку на память и сделает её использование более предсказуемым;<br>— не делать чеĸпоинт аĸтиваций для ĸаĸ можно большего числа слоёв. Это позволит убрать избыточные вычисления за счёт сэкономленной памяти;<br>— выделить два стрима: вычислений и ĸоммуниĸаций, а синхронизацию построить таким образом, чтобы forward не начинался до завершения all_gather того же слоя, а all_gather не начинался до освобождения соответствующего буффера на предыдущем слое;<br>— разово собирать RMSNorm/LayerNorm в начале итерации и тольĸо в ĸонце усреднить градиенты;<br>— вынести predivide в самый ĸонец backward, таĸ ĸаĸ при reduce_scatter в bf16 или fp32 рисĸа переполнения нет.<br><br>Более подробно про проблемы обучения на множестве GPU можно <a href="https://habr.com/ru/companies/yandex/articles/817509/" rel="nofollow noopener noreferrer">почитать на Хабре</a>. Приходите в <a href="https://github.com/yandex/YaFSDP" rel="nofollow noopener noreferrer">репозиторий библиотеки YaFSDP</a>, ставьте лайк и приносите вопросы в Issues. А ещё — делитесь своими впечатлениями здесь в комментариях.<br><br><a href="https://t.me/stuffyNLP" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/19_480.webp" srcset="../assets/media/thumbs/19_480.webp 480w, ../assets/media/19.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="19" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/20_480.webp" srcset="../assets/media/thumbs/20_480.webp 480w, ../assets/media/20.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="19" data-image-index="1" /></div></div>
      <div class="actions">
        <span>14 887 просмотров · 98 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/19" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/19.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="18" data-search="instruction tuning для moe-моделей техника mixture of experts (moe) предполагает использование в сети нескольких специализированных блоков — экспертов — для решения одной задачи. модели на архитектуре moe сильно выигрывают от грамотного файнтюнинга. об этом и говорится в статье, которую мы сегодня разберём. преимущества инстракшн-тюнинга секрет, как утверждают авторы, в предварительном инстракшн-тюнинге — после претрейна и до файнтюнинга. проверяют эту гипотезу на нескольких сетапах. первый — switch transformer, у которого каждый слой moe и только топ-1 гейтинг. далее — gshard с топ-2 гейтингом, expert-choice, дающий возможность экспертам выбирать токены и st-moe. на графике можно увидеть результаты файнтьюнинга моделей с инстракшн-тюнингом (приставка flan) и без него, сразу после претрейна. в первом случае моe выбивается вперёд, а вот во втором её показатели хуже, чем у т5. это происходит потому, что при обычном файнтюнинге мое-моделей они теряют преимущество, ведь «эксперты» начинают сходиться во мнениях. как устроен инстракшн-тюнинг авторы использовали задачи из flan-датасета и применяли auxillary loss. это нужно, чтобы все токены не отправлялись одному «эксперту». также применяли дропаут (0,05) и экспертный дропаут (0,2). коэффициент скорости обучения — на уровне 1e −4. результаты авторы статьи утверждают, что их flan-moe, требующий 32 миллиарда операций при форварде (32.1 gflops/token), показывает себя лучше, чем flan-palm, у которого в 3 раза больше количество операций 91.6 gflops/token. в итоге после инстракшн-тюнинга показатели модели выросли в три с лишним раза. выглядит слегка подозрительно, но впечатляет. более того, при росте модели сохраняется тенденция к росту показателей после инстракшн-тюнинга. но авторы подчёркивают, что количество данных важнее, чем число «экспертов». разбор подготовил ❣ александр мазитов душный nlp instruction tuning для moe-моделей техника mixture of experts (moe) предполагает использование в сети нескольких специализированных блоков — экспертов — для решения одной задачи. модели на архитектуре moe сильно выигрывают от грамотного файнтюнинга. об этом и говорится в статье, которую мы сегодня разберём. преимущества инстракшн-тюнинга секрет, как утверждают авторы, в предварительном инстракшн-тюнинге — после претрейна и до файнтюнинга. проверяют эту гипотезу на нескольких сетапах. первый — switch transformer, у которого каждый слой moe и только топ-1 гейтинг. далее — gshard с топ-2 гейтингом, expert-choice, дающий возможность экспертам выбирать токены и st-moe. на графике можно увидеть результаты файнтьюнинга моделей с инстракшн-тюнингом (приставка flan) и без него, сразу после претрейна. в первом случае моe выбивается вперёд, а вот во втором её показатели хуже, чем у т5. это происходит потому, что при обычном файнтюнинге мое-моделей они теряют преимущество, ведь «эксперты» начинают сходиться во мнениях. как устроен инстракшн-тюнинг авторы использовали задачи из flan-датасета и применяли auxillary loss. это нужно, чтобы все токены не отправлялись одному «эксперту». также применяли дропаут (0,05) и экспертный дропаут (0,2). коэффициент скорости обучения — на уровне 1e −4. результаты авторы статьи утверждают, что их flan-moe, требующий 32 миллиарда операций при форварде (32.1 gflops/token), показывает себя лучше, чем flan-palm, у которого в 3 раза больше количество операций 91.6 gflops/token. в итоге после инстракшн-тюнинга показатели модели выросли в три с лишним раза. выглядит слегка подозрительно, но впечатляет. более того, при росте модели сохраняется тенденция к росту показателей после инстракшн-тюнинга. но авторы подчёркивают, что количество данных важнее, чем число «экспертов». разбор подготовил ❣ александр мазитов душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-06-04T10:03:51+00:00" href="./posts/18.html">2024-06-04 10:03 UTC</a></div>
      </div>
      <div class="post-body"><strong>Instruction Tuning для MoE-моделей<br><br></strong>Техника Mixture of Experts (MoE) предполагает использование в сети нескольких специализированных блоков — экспертов — для решения одной задачи. Модели на архитектуре MoE сильно выигрывают от грамотного файнтюнинга. Об этом и говорится <a href="https://arxiv.org/abs/2305.14705" rel="nofollow noopener noreferrer">в статье,</a> которую мы сегодня разберём. <br><br><strong>Преимущества инстракшн-тюнинга<br><br></strong>Секрет, как утверждают авторы, в предварительном инстракшн-тюнинге — после претрейна и до файнтюнинга. Проверяют эту гипотезу на нескольких сетапах. Первый — Switch Transformer, у которого каждый слой MoE и только топ-1 гейтинг. Далее — GShard с топ-2 гейтингом, expert-choice, дающий возможность экспертам выбирать токены и ST-MoE. <br><br>На графике можно увидеть результаты файнтьюнинга моделей с инстракшн-тюнингом (приставка Flan) и без него, сразу после претрейна. В первом случае МоE выбивается вперёд, а вот во втором её показатели хуже, чем у Т5. Это происходит потому, что при обычном файнтюнинге МоЕ-моделей они теряют преимущество, ведь «эксперты» начинают сходиться во мнениях. <br><br><strong>Как устроен инстракшн-тюнинг<br><br></strong>Авторы использовали задачи из FLAN-датасета и применяли auxillary loss. Это нужно, чтобы все токены не отправлялись одному «эксперту». Также применяли дропаут (0,05) и экспертный дропаут (0,2). Коэффициент скорости обучения — на уровне 1e −4. <br><br><strong>Результаты<br><br></strong>Авторы статьи утверждают, что их FLAN-MoE, требующий  32 миллиарда операций при форварде (32.1 GFLOPs/token), показывает себя лучше, чем FLAN-PALM, у которого в 3 раза больше количество операций 91.6 GFLOPs/token. В итоге после инстракшн-тюнинга показатели модели выросли в три с лишним раза. Выглядит слегка подозрительно, но впечатляет. Более того, при росте модели сохраняется тенденция к росту показателей после инстракшн-тюнинга. Но авторы подчёркивают, что количество данных важнее, чем число «экспертов». <br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Александр Мазитов<br><br></em><a href="https://t.me/stuffyNLP" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/18_480.webp" srcset="../assets/media/thumbs/18_480.webp 480w, ../assets/media/18.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="18" data-image-index="0" /></div></div>
      <div class="actions">
        <span>5 142 просмотров · 44 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/18" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/18.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="11" data-search="самые интересные статьи с iclr 2024 в начале мая прошла iclr 2024 — конференция, которая собирает специалистов в области машинного обучения со всего мира. мы побывали на мероприятии и теперь в карточках рассказываем о статьях, которые запомнились больше всего. а вот ссылки на все тексты: 1. fast-detectgpt: efficient zero-shot detection of machine-generated text via conditional probability curvature 2. longlora: efficient fine-tuning of long-context large language models 3. dola: decoding by contrasting layers improves factuality in large language models 4. never train from scratch: fair comparison of long-sequence models requires data-driven priors 5. toolllm: facilitating large language models to master 16000+ real-world apis 6. towards a statistical theory of data selection under weak supervision рассказывайте в комментариях, какие статьи понравились вам, и делитесь впечатлениями! душный nlp самые интересные статьи с iclr 2024 в начале мая прошла iclr 2024 — конференция, которая собирает специалистов в области машинного обучения со всего мира. мы побывали на мероприятии и теперь в карточках рассказываем о статьях, которые запомнились больше всего. а вот ссылки на все тексты: 1. fast-detectgpt: efficient zero-shot detection of machine-generated text via conditional probability curvature 2. longlora: efficient fine-tuning of long-context large language models 3. dola: decoding by contrasting layers improves factuality in large language models 4. never train from scratch: fair comparison of long-sequence models requires data-driven priors 5. toolllm: facilitating large language models to master 16000+ real-world apis 6. towards a statistical theory of data selection under weak supervision рассказывайте в комментариях, какие статьи понравились вам, и делитесь впечатлениями! душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-05-31T12:57:32+00:00" href="./posts/11.html">2024-05-31 12:57 UTC</a></div>
      </div>
      <div class="post-body"><strong>Самые интересные статьи с ICLR 2024<br><br></strong>В начале мая прошла ICLR 2024 — конференция, которая собирает специалистов в области машинного обучения со всего мира. Мы побывали на мероприятии и теперь в карточках рассказываем о статьях, которые запомнились больше всего. <br><br>А вот ссылки на все тексты:<br><br>1. <a href="https://arxiv.org/abs/2310.05130" rel="nofollow noopener noreferrer">Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature<br><br></a>2. <a href="https://arxiv.org/abs/2309.12307" rel="nofollow noopener noreferrer">LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models<br><br></a>3. <a href="https://arxiv.org/abs/2309.03883" rel="nofollow noopener noreferrer">DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models<br><br></a>4. <a href="https://arxiv.org/abs/2310.02980" rel="nofollow noopener noreferrer">Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors<br><br></a>5. <a href="https://arxiv.org/abs/2307.16789" rel="nofollow noopener noreferrer">ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs<br><br></a>6. <a href="https://arxiv.org/abs/2309.14563" rel="nofollow noopener noreferrer">Towards a statistical theory of data selection under weak supervision<br><br></a>Рассказывайте в комментариях, какие статьи понравились вам, и делитесь впечатлениями!<br><br><a href="https://t.me/+5jfeAMZpXMs5Nzdi" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/11_480.webp" srcset="../assets/media/thumbs/11_480.webp 480w, ../assets/media/11.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="11" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/12_480.webp" srcset="../assets/media/thumbs/12_480.webp 480w, ../assets/media/12.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="11" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/13_480.webp" srcset="../assets/media/thumbs/13_480.webp 480w, ../assets/media/13.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="11" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/14_480.webp" srcset="../assets/media/thumbs/14_480.webp 480w, ../assets/media/14.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="11" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/15_480.webp" srcset="../assets/media/thumbs/15_480.webp 480w, ../assets/media/15.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="11" data-image-index="4" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/16_480.webp" srcset="../assets/media/thumbs/16_480.webp 480w, ../assets/media/16.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="11" data-image-index="5" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/17_480.webp" srcset="../assets/media/thumbs/17_480.webp 480w, ../assets/media/17.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="11" data-image-index="6" /></div></div>
      <div class="actions">
        <span>4 205 просмотров · 57 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/11" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/11.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="10" data-search="редактирование знаний внутри llm в постоянно изменяющихся условиях важно поддерживать актуальность информации, которой обладают llm. остро встаёт вопрос редактирования знаний модели — его и рассматривают авторы сегодняшней статьи. в ней представлено три типа методов редактирования знаний. первый — обращение к внешним знаниям. в этом случае модель получает новую информацию в виде контекстов, а затем пользуется ими, когда нужно ответить на вопрос. вначале собирают набор демонстраций: + копирования — показываем модели, что надо уметь повторять факты; + обновления — не только повтор факта, но и перефразирование; + сохранения — показываем llm, что при изменении одного факта другие не должны меняться. далее эти демонстрации добавляются в текущий промт и отправляются на вход. второй тип — внедрение знаний. это добавление новых параметров, которые позволяют выстроить связи между старыми и новыми знаниями. суть заключается в создании нового feed forward-слоя, который будет меньше первого. веса на втором определяются файнтюнингом по текстам с изменёнными знаниями. третий вид — собственно, редактирование. новые параметры при этом не добавляются, но меняются данные, которыми модель уже обладает. существует два способа сделать это: + meta learning предполагает обучение гиперсетки, предсказывающей изменения параметров, которые необходимо сделать. исходя из этого мы обновляем веса и минимизируем loss на новых данных. + location-then-edit предполагает определение конкретного места в модели, где сосредоточены знания о каком-либо факте. для этого мы наблюдаем за тем, как меняются градиенты. эффективность изменения знаний авторы предлагают измерять с помощью специального бенчмарка. он состоит из шести датасетов, проверяющих разные аспекты: вставку, добавление, изменение и удаление знаний. датасет доступен по этой ссылке. спасибо за внимание! делитесь мыслями по теме в комментариях :) разбор подготовил ❣ артём сафин душный nlp редактирование знаний внутри llm в постоянно изменяющихся условиях важно поддерживать актуальность информации, которой обладают llm. остро встаёт вопрос редактирования знаний модели — его и рассматривают авторы сегодняшней статьи . в ней представлено три типа методов редактирования знаний. первый — обращение к внешним знаниям . в этом случае модель получает новую информацию в виде контекстов, а затем пользуется ими, когда нужно ответить на вопрос. вначале собирают набор демонстраций: + копирования — показываем модели, что надо уметь повторять факты; + обновления — не только повтор факта, но и перефразирование; + сохранения — показываем llm, что при изменении одного факта другие не должны меняться. далее эти демонстрации добавляются в текущий промт и отправляются на вход. второй тип — внедрение знаний . это добавление новых параметров, которые позволяют выстроить связи между старыми и новыми знаниями. суть заключается в создании нового feed forward-слоя, который будет меньше первого. веса на втором определяются файнтюнингом по текстам с изменёнными знаниями. третий вид — собственно, редактирование . новые параметры при этом не добавляются, но меняются данные, которыми модель уже обладает. существует два способа сделать это: + meta learning предполагает обучение гиперсетки, предсказывающей изменения параметров, которые необходимо сделать. исходя из этого мы обновляем веса и минимизируем loss на новых данных. + location-then-edit предполагает определение конкретного места в модели, где сосредоточены знания о каком-либо факте. для этого мы наблюдаем за тем, как меняются градиенты. эффективность изменения знаний авторы предлагают измерять с помощью специального бенчмарка. он состоит из шести датасетов, проверяющих разные аспекты: вставку, добавление, изменение и удаление знаний. датасет доступен по этой ссылке . спасибо за внимание! делитесь мыслями по теме в комментариях :) разбор подготовил ❣ артём сафин душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-05-22T09:16:45+00:00" href="./posts/10.html">2024-05-22 09:16 UTC</a></div>
      </div>
      <div class="post-body"><strong>Редактирование знаний внутри LLM<br><br></strong>В постоянно изменяющихся условиях важно поддерживать актуальность информации, которой обладают LLM. Остро встаёт вопрос редактирования знаний модели — его и рассматривают авторы сегодняшней <a href="https://arxiv.org/abs/2401.01286" rel="nofollow noopener noreferrer">статьи</a>. <br><br>В ней представлено три типа методов редактирования знаний. Первый — <strong>обращение к внешним знаниям</strong>. В этом случае модель получает новую информацию в виде контекстов, а затем пользуется ими, когда нужно ответить на вопрос. <br><br>Вначале собирают набор демонстраций: <br>  + копирования — показываем модели, что надо уметь повторять факты; <br>  + обновления — не только повтор факта, но и перефразирование; <br>  + сохранения — показываем LLM, что при изменении одного факта другие не должны меняться. <br>Далее эти демонстрации добавляются в текущий промт и отправляются на вход. <br><br>Второй тип — <strong>внедрение знаний</strong>. Это добавление новых параметров, которые позволяют выстроить связи между старыми и новыми знаниями. Суть заключается в создании нового feed forward-слоя, который будет меньше первого. Веса на втором определяются файнтюнингом по текстам с изменёнными знаниями. <br><br>Третий вид — собственно, <strong>редактирование</strong>. Новые параметры при этом не добавляются, но меняются данные, которыми модель уже обладает. Существует два способа сделать это: <br>  + Meta Learning предполагает обучение гиперсетки, предсказывающей изменения параметров, которые необходимо сделать. Исходя из этого мы обновляем веса и минимизируем loss на новых данных.<br>  + Location-then-Edit предполагает определение конкретного места в модели, где сосредоточены знания о каком-либо факте. Для этого мы наблюдаем за тем, как меняются градиенты. <br><br>Эффективность изменения знаний авторы предлагают измерять с помощью специального бенчмарка. Он состоит из шести датасетов, проверяющих разные аспекты: вставку, добавление, изменение и удаление знаний. Датасет доступен по <a href="https://huggingface.co/datasets/zjunlp/KnowEdit" rel="nofollow noopener noreferrer">этой ссылке</a>. <br><br>Спасибо за внимание! Делитесь мыслями по теме в комментариях :)<br><br><em>Разбор подготовил </em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> <em>Артём Сафин<br><br></em><a href="https://t.me/stuffyNLP" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/10_480.webp" srcset="../assets/media/thumbs/10_480.webp 480w, ../assets/media/10.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="10" data-image-index="0" /></div></div>
      <div class="actions">
        <span>4 736 просмотров · 37 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/10" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/10.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="8" data-search="слияние моделей — часть ii в прошлый раз мы рассмотрели несколько классических методов мёржинга: усреднение весов, slerp и ties. в этом посте завершим обзор ещё двумя не менее интересным способами. итак, метод dare схож с ties, но избавляются здесь не от слишком маленьких, а от случайных изменений. можно выкинуть до 90% значений без ущерба для качества. дальше нужно действовать как в методе slerp, о котором мы рассказывали в прошлом посте, а затем необходимо скалирование весов, чтобы сохранить их среднюю магнитуду. к классическим (но извращённым) методам слияния также относится passthrough. модели, созданные с его помощью, ещё называют «франкенштейнами» — и это неудивительно. passthrough предполагает простую замену слоёв одной модели слоями другой. самый популярный метод сейчас — это moe (mixture of experts). суть его заключается в замене ffn-слоев на «смесь экспертов». в таком случае каждый токен отправляется «эксперту», которому больше всего подходит. распределением занимается роутер. его можно инициализировать случайно и дообучить. схему можно увидеть выше. но есть вариант и без дообучения. скажем, у нас есть несколько моделей. для каждой из них выберем промты, на которых она лучше всего работает. для промтов можно предпосчитать среднее скрытое представление, а затем сравнить со скрытым представлением токенов, которые подаются в модель. если они схожи — эксперты получают соответствующие токены. а чтобы определить похожесть достаточно прибегнуть к простому скалярному произведению. мы перечислили далеко не все методы мёржа — эта сфера развивается и эволюционирует как живой организм. к тому же, мы не углублялись в подробности каждого способа, а лишь описали их в общих чертах. рассказывайте в комментариях, какими вы пользовались сами и какие вам кажутся самыми эффективными? душный nlp слияние моделей — часть ii в прошлый раз мы рассмотрели несколько классических методов мёржинга: усреднение весов, slerp и ties. в этом посте завершим обзор ещё двумя не менее интересным способами. итак, метод dare схож с ties, но избавляются здесь не от слишком маленьких, а от случайных изменений. можно выкинуть до 90% значений без ущерба для качества. дальше нужно действовать как в методе slerp, о котором мы рассказывали в прошлом посте, а затем необходимо скалирование весов, чтобы сохранить их среднюю магнитуду. к классическим (но извращённым) методам слияния также относится passthrough . модели, созданные с его помощью, ещё называют «франкенштейнами» — и это неудивительно. passthrough предполагает простую замену слоёв одной модели слоями другой. самый популярный метод сейчас — это moe (mixture of experts) . суть его заключается в замене ffn-слоев на «смесь экспертов». в таком случае каждый токен отправляется «эксперту», которому больше всего подходит. распределением занимается роутер. его можно инициализировать случайно и дообучить. схему можно увидеть выше. но есть вариант и без дообучения. скажем, у нас есть несколько моделей. для каждой из них выберем промты, на которых она лучше всего работает. для промтов можно предпосчитать среднее скрытое представление, а затем сравнить со скрытым представлением токенов, которые подаются в модель. если они схожи — эксперты получают соответствующие токены. а чтобы определить похожесть достаточно прибегнуть к простому скалярному произведению. мы перечислили далеко не все методы мёржа — эта сфера развивается и эволюционирует как живой организм. к тому же, мы не углублялись в подробности каждого способа, а лишь описали их в общих чертах. рассказывайте в комментариях, какими вы пользовались сами и какие вам кажутся самыми эффективными? душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-05-17T09:33:21+00:00" href="./posts/8.html">2024-05-17 09:33 UTC</a></div>
      </div>
      <div class="post-body"><strong>Слияние моделей — часть II<br><br></strong>В <a href="https://t.me/stuffyNLP/7" rel="nofollow noopener noreferrer">прошлый раз</a> мы рассмотрели несколько классических методов мёржинга: усреднение весов, SLERP и TIES. В этом посте завершим обзор ещё двумя не менее интересным способами. <br><br>Итак, метод <strong>DARE</strong> схож с TIES, но избавляются здесь не от слишком маленьких, а от случайных изменений. Можно выкинуть до 90% значений без ущерба для качества. Дальше нужно действовать как в методе SLERP, о котором мы рассказывали в прошлом посте, а затем необходимо скалирование весов, чтобы сохранить их среднюю магнитуду. <br><br>К классическим (но извращённым) методам слияния также относится <strong>Passthrough</strong>. Модели, созданные с его помощью, ещё называют «Франкенштейнами» — и это неудивительно. Passthrough предполагает простую замену слоёв одной модели слоями другой. <br><br>Самый популярный метод сейчас — это <strong>MoE (Mixture of Experts)</strong>. Суть его заключается в замене FFN-слоев на «смесь экспертов». В таком случае каждый токен отправляется «эксперту», которому больше всего подходит. Распределением занимается роутер. Его можно инициализировать случайно и дообучить. Схему можно увидеть выше.<br><br>Но есть вариант и без дообучения. Скажем, у нас есть несколько моделей. Для каждой из них выберем промты, на которых она лучше всего работает. Для промтов можно предпосчитать среднее скрытое представление, а затем сравнить со скрытым представлением токенов, которые подаются в модель. Если они схожи — эксперты получают соответствующие токены. А чтобы определить похожесть достаточно прибегнуть к простому скалярному произведению.  <br><br>Мы перечислили далеко не все методы мёржа — эта сфера развивается и эволюционирует как живой организм. К тому же, мы не углублялись в подробности каждого способа, а лишь описали их в общих чертах. Рассказывайте в комментариях, какими вы пользовались сами и какие вам кажутся самыми эффективными? <br><br><a href="https://t.me/stuffyNLP" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/8_480.webp" srcset="../assets/media/thumbs/8_480.webp 480w, ../assets/media/8.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="8" data-image-index="0" /></div></div>
      <div class="actions">
        <span>3 401 просмотров · 18 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/8" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/8.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="7" data-search="слияние моделей — часть i как можно догадаться из названия, слияние моделей — или мёржинг — это процесс совмещения двух или более llm в одну. самый базовый тип методов — усреднение весов. например, exponential moving average, предполагающий смешивание веса одной модели на нескольких последних итерациях. но есть и другие способы — о них и расскажем в этом посте. slerp (spherical linear interpolation) предполагает, что мы рассматриваем веса модели не как набор чисел, а как вектора. схему этого метода вы можете увидеть на картинке выше. усреднение происходит за счёт изменения угла между векторами в модели — то есть поворотом весов одной модели в сторону другой. метод схож с усреднением весов потому что это тоже линейное комбинирование. более того, если вектора весов сонаправлены, то slerp сводится к их усреднению. магнитуда векторов в slerp сохраняется. минус метода в том, что его можно использовать только для слияния двух моделей единовременно. здесь стоит сказать о таск-векторах — то есть разнице весов предобученной и файнтюненной модели. при мерджинге файнтюнов одного предобучения можно сливать вместе не веса, а соответствующие таск-вектора. следующий метод — ties. здесь сперва нужно посчитать таск-вектора моделей и в тех параметрах, где вектора меньше некоего порога, занулить их. далее следует «разрешить конфликты» между таск-векторами двух llm: то есть избавиться от противоположных векторов и создать единый. если изменение направлено в одну сторону, то делать ничего не надо, а если в разные — занулить наименьшее. наконец, происходит сам мёрж любым способом. ties так же подходит только для слияния двух моделей. на этом на сегодня всё. а в следующей части мы расскажем ещё о паре классических методов и о самом популярном сейчас способе мержинга. оставайтесь на связи и делитесь своим опытом в комментариях! душный nlp слияние моделей — часть i как можно догадаться из названия, слияние моделей — или мёржинг — это процесс совмещения двух или более llm в одну. самый базовый тип методов — усреднение весов. например, exponential moving average , предполагающий смешивание веса одной модели на нескольких последних итерациях. но есть и другие способы — о них и расскажем в этом посте. slerp (spherical linear interpolation) предполагает, что мы рассматриваем веса модели не как набор чисел, а как вектора. схему этого метода вы можете увидеть на картинке выше. усреднение происходит за счёт изменения угла между векторами в модели — то есть поворотом весов одной модели в сторону другой. метод схож с усреднением весов потому что это тоже линейное комбинирование. более того, если вектора весов сонаправлены, то slerp сводится к их усреднению. магнитуда векторов в slerp сохраняется. минус метода в том, что его можно использовать только для слияния двух моделей единовременно. здесь стоит сказать о таск-векторах — то есть разнице весов предобученной и файнтюненной модели. при мерджинге файнтюнов одного предобучения можно сливать вместе не веса, а соответствующие таск-вектора. следующий метод — ties . здесь сперва нужно посчитать таск-вектора моделей и в тех параметрах, где вектора меньше некоего порога, занулить их. далее следует «разрешить конфликты» между таск-векторами двух llm: то есть избавиться от противоположных векторов и создать единый. если изменение направлено в одну сторону, то делать ничего не надо, а если в разные — занулить наименьшее. наконец, происходит сам мёрж любым способом. ties так же подходит только для слияния двух моделей. на этом на сегодня всё. а в следующей части мы расскажем ещё о паре классических методов и о самом популярном сейчас способе мержинга. оставайтесь на связи и делитесь своим опытом в комментариях! душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-05-03T14:51:13+00:00" href="./posts/7.html">2024-05-03 14:51 UTC</a></div>
      </div>
      <div class="post-body"><strong>Слияние моделей — часть I<br><br></strong>Как можно догадаться из названия, слияние моделей — или мёржинг —  это процесс совмещения двух или более LLM в одну. Самый базовый тип методов — усреднение весов. Например, <strong>exponential moving average</strong>, предполагающий смешивание веса одной модели на нескольких последних итерациях. Но есть и <a href="https://slgero.medium.com/merge-large-language-models-29897aeb1d1a" rel="nofollow noopener noreferrer">другие способы</a> — о них и расскажем в этом посте. <br><br><strong>SLERP (Spherical Linear Interpolation)</strong> предполагает, что мы рассматриваем веса модели не как набор чисел, а как вектора. Схему этого метода вы можете увидеть на картинке выше. <br><br>Усреднение происходит за счёт изменения угла между векторами в модели — то есть поворотом весов одной модели в сторону другой. Метод схож с усреднением весов потому что это тоже линейное комбинирование. <br><br>Более того, если вектора весов сонаправлены, то SLERP сводится к их усреднению. Магнитуда векторов в SLERP сохраняется. Минус метода в том, что его можно использовать только для слияния двух моделей единовременно. <br><br>Здесь стоит сказать о таск-векторах — то есть разнице весов предобученной и файнтюненной модели. При мерджинге файнтюнов одного предобучения можно сливать вместе не веса, а соответствующие таск-вектора. <br><br>Следующий метод — <strong>TIES</strong>. Здесь сперва нужно посчитать таск-вектора моделей и в тех параметрах, где вектора меньше некоего порога, занулить их. Далее следует «разрешить конфликты» между таск-векторами двух LLM: то есть избавиться от противоположных векторов и создать единый. Если изменение направлено в одну сторону, то делать ничего не надо, а если в разные — занулить наименьшее. Наконец, происходит сам мёрж любым способом. TIES так же подходит только для слияния двух моделей.<br><br>На этом на сегодня всё. А в следующей части мы расскажем ещё о паре классических методов и о самом популярном сейчас способе мержинга. Оставайтесь на связи и делитесь своим опытом в комментариях!<br><br><a href="https://t.me/stuffyNLP" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/7_480.webp" srcset="../assets/media/thumbs/7_480.webp 480w, ../assets/media/7.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="7" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 715 просмотров · 21 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/7" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/7.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="4" data-search="lima почти все знания и способности модель получает из претрейна, а алаймент лишь определяет, какое подраспределение форматов использовать для общения с пользователем. эта гипотеза лежит в основе статьи о lima — 65b llama-модели, файнтюненной на 1350 примерах. авторы заверяют: модель показывает отличные результаты благодаря разнообразным и качественным данным на алайменте. разберемся, как инженеры к этому пришли. тексты в датасете должны быть единообразными (как для ии-ассистента), но из разных источников. за данными обращались к stack exchange, wikihow и reddit, а какую-то часть написали сами. всё отобрали вручную — с reddit взяли только ответы с сабреддитов askreddit и writingprompts. на этапе обучения добавили токен end of turn для распознавания спикеров, а также residual dropout для роста дропаута от слоя к слою. в оценке использовали pairwise-замеры: брали ответы на один промт от двух моделей, предлагая оценщику выбрать лучший по определённым критериям. оценщиков заменили на gpt-4 без сильной потери качества — решения llm и людей совпадали на 78-79%. по результатам pairwise вышло, что lima побеждала alpaca и davinci-003 (рис. 1). при проверке gpt-4 показатели lima были выше, чем при проверке человеком (рис. 2). авторы статьи обучили llama 7b таким же способом. но вместо 2 принимали по 5 ответов на промт, а датасет для алаймента расширили до 2000 примеров. тестирование снова доверили chatgpt — она оценивала ответы двух 7b-моделей, обученных на «грязных» и «отфильтрованных» данных со stack exchange. в первом случае результат оказался 3,3 из 6, во втором — 3,83. для сравнения, обучение на премодерируемом контенте с wikihow дало оценку 3,49 (рис. 3). эти результаты кажутся не совсем валидным — слишком уж мало примеров. авторы статьи пришли к выводу, что самое главное — качество и разнообразие датасета, а скейлинг по данным почти не влияет на ответы языковой модели. спасибо, что прочитали! расскажите, что думаете о lima, в комментариях. разбор подготовил ❣ роман горб @stuffynlp lima почти все знания и способности модель получает из претрейна, а алаймент лишь определяет, какое подраспределение форматов использовать для общения с пользователем. эта гипотеза лежит в основе статьи о lima — 65b llama-модели, файнтюненной на 1350 примерах. авторы заверяют: модель показывает отличные результаты благодаря разнообразным и качественным данным на алайменте. разберемся, как инженеры к этому пришли. тексты в датасете должны быть единообразными (как для ии-ассистента), но из разных источников. за данными обращались к stack exchange, wikihow и reddit, а какую-то часть написали сами. всё отобрали вручную — с reddit взяли только ответы с сабреддитов askreddit и writingprompts. на этапе обучения добавили токен end of turn для распознавания спикеров, а также residual dropout для роста дропаута от слоя к слою. в оценке использовали pairwise-замеры: брали ответы на один промт от двух моделей, предлагая оценщику выбрать лучший по определённым критериям. оценщиков заменили на gpt-4 без сильной потери качества — решения llm и людей совпадали на 78-79%. по результатам pairwise вышло, что lima побеждала alpaca и davinci-003 ( рис. 1 ). при проверке gpt-4 показатели lima были выше, чем при проверке человеком ( рис. 2 ). авторы статьи обучили llama 7b таким же способом. но вместо 2 принимали по 5 ответов на промт, а датасет для алаймента расширили до 2000 примеров. тестирование снова доверили chatgpt — она оценивала ответы двух 7b-моделей, обученных на «грязных» и «отфильтрованных» данных со stack exchange. в первом случае результат оказался 3,3 из 6, во втором — 3,83. для сравнения, обучение на премодерируемом контенте с wikihow дало оценку 3,49 ( рис. 3 ). эти результаты кажутся не совсем валидным — слишком уж мало примеров. авторы статьи пришли к выводу, что самое главное — качество и разнообразие датасета, а скейлинг по данным почти не влияет на ответы языковой модели. спасибо, что прочитали! расскажите, что думаете о lima, в комментариях. разбор подготовил ❣ роман горб @stuffynlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-04-19T13:36:19+00:00" href="./posts/4.html">2024-04-19 13:36 UTC</a></div>
      </div>
      <div class="post-body"><strong>LIMA<br><br></strong>Почти все знания и способности модель получает из претрейна, а алаймент лишь определяет, какое подраспределение форматов использовать для общения с пользователем. Эта гипотеза лежит в основе <a href="https://arxiv.org/abs/2305.11206.pdf" rel="nofollow noopener noreferrer">статьи о LIMA</a> — 65B LLaMa-модели, файнтюненной на 1350 примерах. Авторы заверяют: модель показывает отличные результаты благодаря разнообразным и качественным данным на алайменте. Разберемся, как инженеры к этому пришли. <br><br>Тексты в датасете должны быть единообразными (как для ИИ-ассистента), но из разных источников. За данными обращались к Stack Exchange, wikiHow и Reddit, а какую-то часть написали сами. Всё отобрали вручную — с Reddit взяли только ответы с сабреддитов AskReddit и WritingPrompts. <br><br>На этапе обучения добавили токен End of Turn для распознавания спикеров, а также residual dropout для роста дропаута от слоя к слою. В оценке использовали pairwise-замеры: брали ответы на один промт от двух моделей, предлагая оценщику выбрать лучший по определённым критериям. Оценщиков заменили на GPT-4 без сильной потери качества — решения LLM и людей совпадали на 78-79%. <br><br>По результатам pairwise вышло, что LIMA побеждала Alpaca и DaVinci-003 (<em>рис. 1</em>). При проверке GPT-4 показатели LIMA были выше, чем при проверке человеком (<em>рис. 2</em>). <br><br>Авторы статьи обучили LLaMa 7B таким же способом. Но вместо 2 принимали по 5 ответов на промт, а датасет для алаймента расширили до 2000 примеров. Тестирование снова доверили ChatGPT — она оценивала ответы двух 7B-моделей, обученных на «грязных» и «отфильтрованных» данных со Stack Exchange. В первом случае результат оказался 3,3 из 6, во втором — 3,83. Для сравнения, обучение на премодерируемом контенте с wikiHow дало оценку 3,49 (<em>рис. 3</em>). Эти результаты кажутся не совсем валидным — слишком уж мало примеров. <br><br>Авторы статьи пришли к выводу, что самое главное — качество и разнообразие датасета, а скейлинг по данным почти не влияет на ответы языковой модели.<br><br>Спасибо, что прочитали! Расскажите, что думаете о LIMA, в комментариях. <br><br><em>Разбор подготовил </em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> <em>Роман Горб<br><br></em><a href="https://t.me/stuffyNLP" rel="nofollow noopener noreferrer">@stuffyNLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/4_480.webp" srcset="../assets/media/thumbs/4_480.webp 480w, ../assets/media/4.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="4" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/5_480.webp" srcset="../assets/media/thumbs/5_480.webp 480w, ../assets/media/5.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="4" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/6_480.webp" srcset="../assets/media/thumbs/6_480.webp 480w, ../assets/media/6.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="4" data-image-index="2" /></div></div>
      <div class="actions">
        <span>1 994 просмотров · 17 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/4" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/4.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="3" data-search="⚗️ что такое дистилляция и как она применяется в llm чем больше модель, тем сложнее ее инферить и дороже обучать. решить проблему призвана, в том числе, дистилляция — передача знаний от тяжёлой модели («учителя») более лёгкой («ученика»). расскажем, какие типы дистилляции существуют и как их используют. классический способ предложил джеффри хинтон в статье 2015 года. учёный выдвигает гипотезу, что распределение классов, которые модель предлагает в качестве ответа, само по себе содержит немало знаний. поэтому имеет смысл тренировать «ученика» не на ответах, а на распределении классов «учителя», используя softmax с температурой. в качестве лосса использовали кросс-энтропию между двумя распределениями — ответами учителя и ученика. одна из первых моделей, которую дистиллировали на претрейне, — distilbert. результат получился впечатляющим: language understanding удалось сохранить на 97%, а скорость по заявлению авторов выросла на 60%. интересно, что дистиллировали веса, а в архитектуре модели изначально было вдвое меньше энкодер-блоков, чем у базовой bert — 6 против 12. в основе обучения — перекрестная энтропия ответов «учителя» и «ученика», mlm и l cos — косинусная близость между эмбеддингами на скрытых слоях. идеи distilbert позднее применяли, например, в distilgpt. самый простой из современных методов — имитация модели. его суть — добиться, чтобы небольшая модель копировала поведение крупной. для этого «учителя» просят генерировать ответы на разные запросы, а потом на них обучают «ученика». маленькие модели отлично подражают большим, но не развивают собственные навыки. поэтому «ученики» не получают новые знания, зато неплохо справляются с тем, чтобы извлекать имеющиеся. этот метод подходит, когда нужно натренировать модель под конкретные задачи, например, для суммаризации или разметки данных. для дистилляции знаний в «младшую» модель можно использовать метод chain-of-thought prompting. суть: просить llm давать не только ответ, но и описывать цепочку рассуждений, которые к нему привели. как показывают исследования, такой подход существенно увеличивает качество ответов на некоторых датасетах. к примеру, авторы статьи distilling step-by-step! попросили «ученика» предсказывать не только ответы «учителя», но и обоснования, чередуя запросы. так маленькая модель тренируется думать как большая llm, а не просто копирует ответы и поведение — на некоторых датасетах этот подход даёт отличный результат. кроме того, можно использовать датасет, составленный по reward-модели. в этом случае «ученик» будет тренироваться не на всех ответах «учителя», а только на тех, которые reward-модель считает хорошими, что тоже может улучшить результаты. наконец, можно расширить датасет, на котором учится младшая модель, с помощью генерации с разными параметрами вроде температуры или seed. набор данных по одному промту получится более разнообразным, а поведение «ученика» в теории должно больше походить на поведение «учителя». на этом всё. спасибо, что прочитали! делитесь опытом и впечатлениями от поста в комментариях! а во второй части текста мы разберём другие методы дистилляции и, конечно, затронем minillm. оставайтесь на связи! разбор помог подготовить ❣ сергей воробьев @stuffynlp ⚗️ что такое дистилляция и как она применяется в llm чем больше модель, тем сложнее ее инферить и дороже обучать. решить проблему призвана, в том числе, дистилляция — передача знаний от тяжёлой модели («учителя») более лёгкой («ученика»). расскажем, какие типы дистилляции существуют и как их используют. классический способ предложил джеффри хинтон в статье 2015 года . учёный выдвигает гипотезу, что распределение классов, которые модель предлагает в качестве ответа, само по себе содержит немало знаний. поэтому имеет смысл тренировать «ученика» не на ответах, а на распределении классов «учителя», используя softmax с температурой. в качестве лосса использовали кросс-энтропию между двумя распределениями — ответами учителя и ученика. одна из первых моделей, которую дистиллировали на претрейне, — distilbert . результат получился впечатляющим: language understanding удалось сохранить на 97%, а скорость по заявлению авторов выросла на 60%. интересно, что дистиллировали веса, а в архитектуре модели изначально было вдвое меньше энкодер-блоков, чем у базовой bert — 6 против 12. в основе обучения — перекрестная энтропия ответов «учителя» и «ученика», mlm и l cos — косинусная близость между эмбеддингами на скрытых слоях. идеи distilbert позднее применяли, например, в distilgpt . самый простой из современных методов — имитация модели. его суть — добиться, чтобы небольшая модель копировала поведение крупной. для этого «учителя» просят генерировать ответы на разные запросы, а потом на них обучают «ученика». маленькие модели отлично подражают большим, но не развивают собственные навыки. поэтому «ученики» не получают новые знания, зато неплохо справляются с тем, чтобы извлекать имеющиеся. этот метод подходит, когда нужно натренировать модель под конкретные задачи, например, для суммаризации или разметки данных. для дистилляции знаний в «младшую» модель можно использовать метод chain-of-thought prompting . суть: просить llm давать не только ответ, но и описывать цепочку рассуждений, которые к нему привели. как показывают исследования, такой подход существенно увеличивает качество ответов на некоторых датасетах. к примеру, авторы статьи distilling step-by-step! попросили «ученика» предсказывать не только ответы «учителя», но и обоснования, чередуя запросы. так маленькая модель тренируется думать как большая llm, а не просто копирует ответы и поведение — на некоторых датасетах этот подход даёт отличный результат. кроме того, можно использовать датасет, составленный по reward-модели. в этом случае «ученик» будет тренироваться не на всех ответах «учителя», а только на тех, которые reward-модель считает хорошими, что тоже может улучшить результаты. наконец, можно расширить датасет, на котором учится младшая модель, с помощью генерации с разными параметрами вроде температуры или seed. набор данных по одному промту получится более разнообразным, а поведение «ученика» в теории должно больше походить на поведение «учителя». на этом всё. спасибо, что прочитали! делитесь опытом и впечатлениями от поста в комментариях! а во второй части текста мы разберём другие методы дистилляции и, конечно, затронем minillm. оставайтесь на связи! разбор помог подготовить ❣ сергей воробьев @stuffynlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-04-12T16:36:31+00:00" href="./posts/3.html">2024-04-12 16:36 UTC</a></div>
      </div>
      <div class="post-body">⚗️ <strong>Что такое дистилляция и как она применяется в LLM<br><br></strong>Чем больше модель, тем сложнее ее инферить и дороже обучать. Решить проблему призвана, в том числе, дистилляция — передача знаний от тяжёлой модели («учителя») более лёгкой («ученика»). Расскажем, какие типы дистилляции существуют и как их используют.<br><br>Классический способ предложил Джеффри Хинтон в <a href="https://arxiv.org/abs/1503.02531" rel="nofollow noopener noreferrer">статье 2015 года</a>. Учёный выдвигает гипотезу, что распределение классов, которые модель предлагает в качестве ответа, само по себе содержит немало знаний. Поэтому имеет смысл тренировать «ученика» не на ответах, а на распределении классов «учителя», используя Softmax с температурой. В качестве лосса использовали кросс-энтропию между двумя распределениями — ответами учителя и ученика.<br><br>Одна из первых моделей, которую дистиллировали на претрейне, — <a href="https://arxiv.org/abs/1910.01108" rel="nofollow noopener noreferrer">DistilBERT</a>. Результат получился впечатляющим: language understanding удалось сохранить на 97%, а скорость по заявлению авторов выросла на 60%. Интересно, что дистиллировали веса, а в архитектуре модели изначально было вдвое меньше энкодер-блоков, чем у базовой BERT — 6 против 12. В основе обучения — перекрестная энтропия ответов «учителя» и «ученика», MLM и L cos — косинусная близость между эмбеддингами на скрытых слоях. Идеи DistilBERT позднее применяли, например, в <a href="https://arxiv.org/pdf/2110.08460.pdf" rel="nofollow noopener noreferrer">DistilGPT</a>. <br><br>Самый простой из современных методов — имитация модели. Его суть — добиться, чтобы небольшая модель копировала поведение крупной. Для этого «учителя» просят генерировать ответы на разные запросы, а потом на них обучают «ученика».<br><br>Маленькие модели отлично подражают большим, но не развивают собственные навыки. Поэтому «ученики» не получают новые знания, зато неплохо справляются с тем, чтобы извлекать имеющиеся. Этот метод подходит, когда нужно натренировать модель под конкретные задачи, например, для суммаризации или разметки данных.<br><br>Для дистилляции знаний в «младшую» модель можно использовать метод <a href="https://arxiv.org/abs/2201.11903" rel="nofollow noopener noreferrer">Chain-of-Thought Prompting</a>. Суть: просить LLM давать не только ответ, но и описывать цепочку рассуждений, которые к нему привели. Как показывают исследования, такой подход существенно увеличивает качество ответов на некоторых датасетах.<br><br>К примеру, авторы статьи <a href="https://arxiv.org/pdf/2305.02301.pdf" rel="nofollow noopener noreferrer">Distilling Step-by-Step!</a> попросили «ученика» предсказывать не только ответы «учителя», но и обоснования, чередуя запросы. Так маленькая модель тренируется думать как большая LLM, а не просто копирует ответы и поведение — на некоторых датасетах этот подход даёт отличный результат.<br><br>Кроме того, можно использовать датасет, составленный по reward-модели. В этом случае «ученик» будет тренироваться не на всех ответах «учителя», а только на тех, которые reward-модель считает хорошими, что тоже может улучшить результаты.<br><br>Наконец, можно расширить датасет, на котором учится младшая модель, с помощью генерации с разными параметрами вроде температуры или seed. Набор данных по одному промту получится более разнообразным, а поведение «ученика» в теории должно больше походить на поведение «учителя».<br><br>На этом всё. Спасибо, что прочитали! Делитесь опытом и впечатлениями от поста в комментариях! А во второй части текста мы разберём другие методы дистилляции и, конечно, затронем MiniLLM. Оставайтесь на связи! <br><br><em>Разбор помог подготовить </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Сергей Воробьев<br><br></em>@stuffyNLP</div>
      <div class="actions">
        <span>9 759 просмотров · 29 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/3" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/3.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="2" data-search="привет! на связи nlp-команда яндекса. коллективный разум создал этот канал, чтобы делиться разборами научных статей со всеми, кто жаждет знаний. не стесняйтесь общаться в комментариях! мы за обмен опытом и увлекательные дискуссии. кстати, форточку открывать не собираемся — здесь душно, потому что так нужно. привет! на связи nlp-команда яндекса. коллективный разум создал этот канал, чтобы делиться разборами научных статей со всеми, кто жаждет знаний. не стесняйтесь общаться в комментариях! мы за обмен опытом и увлекательные дискуссии. кстати, форточку открывать не собираемся — здесь душно, потому что так нужно.">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-04-04T16:40:27+00:00" href="./posts/2.html">2024-04-04 16:40 UTC</a></div>
      </div>
      <div class="post-body">Привет! На связи NLP-команда Яндекса. Коллективный разум создал этот канал, чтобы делиться разборами научных статей со всеми, кто жаждет знаний. Не стесняйтесь общаться в комментариях! Мы за обмен опытом и увлекательные дискуссии. Кстати, форточку открывать не собираемся — здесь душно, потому что так нужно.</div>
      <div class="actions">
        <span>1 649 просмотров · 17 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/2" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/2.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    </div>
    
    <div class="pager static-pager" style="justify-content:center">
      <div class="page-links">
        <a class="nav-link" href="page-3.html">←</a>
        <a class="page-link" href="index.html">1</a> <a class="page-link" href="page-2.html">2</a> <a class="page-link" href="page-3.html">3</a> <a class="page-link current" href="page-4.html">4</a>
        <a class="nav-link disabled" href="#">→</a>
      </div>
    </div>
    
  </main>

  <footer class="footer">
    <div class="container">
      <div class="footer-inner">
        <span>based on <a href="https://github.com/ml-brand/tg-to-gh-pages" target="_blank" rel="noopener">tg-to-gh-pages</a> (created by <a href="https://github.com/ml-brand" target="_blank" rel="noopener">ML Brand</a>)</span>
        <a id="repoLink" href="https://github.com/ml-brand/tg-to-gh-pages" target="_blank" rel="noopener">Do the same with your channel.</a>
        <span class="footer-links">
          static copy ·
          <a href="../feed.xml" target="_blank" rel="noopener">RSS</a> ·
          <a href="../atom.xml" target="_blank" rel="noopener">Atom</a>
        </span>
      </div>
    </div>
  </footer>

  <script>
    window.__STATIC_POSTS = [{"id": 38, "media": [{"kind": "photo", "path": "../assets/media/38.jpg", "thumb": "../assets/media/thumbs/38_480.webp", "size": 79323, "mime": "image/jpeg", "name": null}]}, {"id": 37, "media": [{"kind": "photo", "path": "../assets/media/37.jpg", "thumb": "../assets/media/thumbs/37_480.webp", "size": 33633, "mime": "image/jpeg", "name": null}]}, {"id": 36, "media": [{"kind": "photo", "path": "../assets/media/36.jpg", "thumb": "../assets/media/thumbs/36_480.webp", "size": 66201, "mime": "image/jpeg", "name": null}]}, {"id": 35, "media": [{"kind": "photo", "path": "../assets/media/35.jpg", "thumb": "../assets/media/thumbs/35_480.webp", "size": 46772, "mime": "image/jpeg", "name": null}]}, {"id": 33, "media": [{"kind": "photo", "path": "../assets/media/33.jpg", "thumb": "../assets/media/thumbs/33_480.webp", "size": 84341, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/34.jpg", "thumb": "../assets/media/thumbs/34_480.webp", "size": 204016, "mime": "image/jpeg", "name": null}]}, {"id": 32, "media": [{"kind": "photo", "path": "../assets/media/32.jpg", "thumb": "../assets/media/thumbs/32_480.webp", "size": 103779, "mime": "image/jpeg", "name": null}]}, {"id": 30, "media": [{"kind": "photo", "path": "../assets/media/30.jpg", "thumb": "../assets/media/thumbs/30_480.webp", "size": 125361, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/31.jpg", "thumb": "../assets/media/thumbs/31_480.webp", "size": 75821, "mime": "image/jpeg", "name": null}]}, {"id": 29, "media": []}, {"id": 28, "media": []}, {"id": 27, "media": [{"kind": "photo", "path": "../assets/media/27.jpg", "thumb": "../assets/media/thumbs/27_480.webp", "size": 72495, "mime": "image/jpeg", "name": null}]}, {"id": 26, "media": []}, {"id": 25, "media": [{"kind": "photo", "path": "../assets/media/25.jpg", "thumb": "../assets/media/thumbs/25_480.webp", "size": 65052, "mime": "image/jpeg", "name": null}]}, {"id": 24, "media": [{"kind": "photo", "path": "../assets/media/24.jpg", "thumb": "../assets/media/thumbs/24_480.webp", "size": 38409, "mime": "image/jpeg", "name": null}]}, {"id": 23, "media": []}, {"id": 22, "media": []}, {"id": 21, "media": [{"kind": "photo", "path": "../assets/media/21.jpg", "thumb": "../assets/media/thumbs/21_480.webp", "size": 45628, "mime": "image/jpeg", "name": null}]}, {"id": 19, "media": [{"kind": "photo", "path": "../assets/media/19.jpg", "thumb": "../assets/media/thumbs/19_480.webp", "size": 70502, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/20.jpg", "thumb": "../assets/media/thumbs/20_480.webp", "size": 62665, "mime": "image/jpeg", "name": null}]}, {"id": 18, "media": [{"kind": "photo", "path": "../assets/media/18.jpg", "thumb": "../assets/media/thumbs/18_480.webp", "size": 229784, "mime": "image/jpeg", "name": null}]}, {"id": 11, "media": [{"kind": "photo", "path": "../assets/media/11.jpg", "thumb": "../assets/media/thumbs/11_480.webp", "size": 82804, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/12.jpg", "thumb": "../assets/media/thumbs/12_480.webp", "size": 132896, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/13.jpg", "thumb": "../assets/media/thumbs/13_480.webp", "size": 123509, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/14.jpg", "thumb": "../assets/media/thumbs/14_480.webp", "size": 166926, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/15.jpg", "thumb": "../assets/media/thumbs/15_480.webp", "size": 152476, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/16.jpg", "thumb": "../assets/media/thumbs/16_480.webp", "size": 151057, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/17.jpg", "thumb": "../assets/media/thumbs/17_480.webp", "size": 150215, "mime": "image/jpeg", "name": null}]}, {"id": 10, "media": [{"kind": "photo", "path": "../assets/media/10.jpg", "thumb": "../assets/media/thumbs/10_480.webp", "size": 86530, "mime": "image/jpeg", "name": null}]}, {"id": 8, "media": [{"kind": "photo", "path": "../assets/media/8.jpg", "thumb": "../assets/media/thumbs/8_480.webp", "size": 66300, "mime": "image/jpeg", "name": null}]}, {"id": 7, "media": [{"kind": "photo", "path": "../assets/media/7.jpg", "thumb": "../assets/media/thumbs/7_480.webp", "size": 65099, "mime": "image/jpeg", "name": null}]}, {"id": 4, "media": [{"kind": "photo", "path": "../assets/media/4.jpg", "thumb": "../assets/media/thumbs/4_480.webp", "size": 69657, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/5.jpg", "thumb": "../assets/media/thumbs/5_480.webp", "size": 70596, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/6.jpg", "thumb": "../assets/media/thumbs/6_480.webp", "size": 44128, "mime": "image/jpeg", "name": null}]}, {"id": 3, "media": []}, {"id": 2, "media": []}];
    window.__STATIC_META = {"title": "Душный NLP", "username": "stuffyNLP", "channel": "stuffyNLP", "last_sync_utc": "2026-02-20T17:24:27Z", "posts_count": 115, "last_seen_message_id": 228, "stats": {"new": 106, "updated": 8, "media_downloaded": 106}, "avatar": "assets/channel_avatar.jpg", "meta_schema_version": "1.0.0", "posts_schema_version": "1.0.0"};
  </script>
  <script src="../common.js"></script>
  <script src="../static.js"></script>
</body>
</html>
