<!doctype html>
<html lang="ru">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Душный NLP — статическая версия (стр. 2/4)</title>
  <meta name="description" content="Статическая версия зеркала Telegram-канала" />
  <link rel="icon" href="../favicon.ico?v=2026-02-11T20%3A21%3A55Z" sizes="any" />
  <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32.png?v=2026-02-11T20%3A21%3A55Z" />
  <link rel="apple-touch-icon" href="../apple-touch-icon.png?v=2026-02-11T20%3A21%3A55Z" />

  <link rel="stylesheet" href="../style.css" />
  <script src="../metrika.js"></script>
</head>
<body>
  <header class="header">
    <div class="container">
      <div class="title-grid">
        <a class="grid-avatar" href="#" target="_blank" rel="noopener">
          <img id="channelAvatar" class="channel-avatar" src="../assets/channel_avatar.jpg" alt="Аватар канала"  />
        </a>
        <div class="grid-main">
          <div class="title-head">
            <div class="title-left">
              <a class="badge-chip" id="siteTitleWrap" href="#" target="_blank" rel="noopener"><h1 id="siteTitle">Душный NLP</h1></a>
            </div>
            <div class="hero-actions">
              <a id="subscribeBtn" class="subscribe-btn" href="https://t.me/+1Z41UptsLwszZDE6" target="_blank" rel="noopener" >Подписаться</a>
              <a class="icon-btn" href="../" aria-label="Перейти к динамической версии">↺</a>
              <button id="themeToggle" class="icon-btn" type="button" aria-label="Переключить тему"></button>
            </div>
          </div>
        </div>
        <div class="controls"></div>
      </div>
    </div>
  </header>

  
  <div id="promoBanner" class="promo-banner" hidden>
    <div class="container promo-inner">
      <span class="promo-text"><a href="https://t.me/addlist/5NH3RoVejEI1MGEy">Подпишись на все наши ML каналы. Они классные, отвечаем!</a></span>
      <button id="promoClose" class="promo-close" type="button" aria-label="Скрыть плашку">×</button>
    </div>
  </div>
  

  <main class="container">
    
    <div class="pager static-pager" style="justify-content:center">
      <div class="page-links">
        <a class="nav-link" href="index.html">←</a>
        <a class="page-link" href="index.html">1</a> <a class="page-link current" href="page-2.html">2</a> <a class="page-link" href="page-3.html">3</a> <a class="page-link" href="page-4.html">4</a>
        <a class="nav-link" href="page-3.html">→</a>
      </div>
    </div>
    
    <div id="posts" class="posts">
      
    <article class="post" data-post-id="155" data-search="и ещё постеры с icml 2025 rstar-math: small llms can master math reasoning with self-evolved deep thinking статья показывает, как с помощью маленькой модели (1,5-7в) добиться качества на уровне больших вроде openai o1. для этого использует реворд-модель, которая умеет оценивать каждый шаг в рассуждении (process reward model, prm) и генерируют рассуждения с помощью monte carlo tree search. как получить качественную prm: шаг за шагом бустим политику и prm. сначала обучаем начальную политику на синтетике от deepseek-coder v2 (236b). далее, используя её, получаем данные для обучения prm. следующим шагом с помощью и политики, и prm генерируем новые более качественные данные. обновляем все модели. а в конце полируем их, генерируя и обучаясь на траекториях для более сложных задач. на каждом шаге для валидации правильности шагов используется код (то есть просят модель сгенерировать python-код для проверки шага). результат — улучшение скоров на десятки процентов на математических бенчмарках. versaprm: multi-domain process reward model via synthetic reasoning data ещё немного о prm — versaprm. авторы заметили, что такие модели работают только для математики, но не на остальных доменах (биология, философия, юридический домен). проблема в данных — нет качественных размеченных пошаговых рассуждений для этих доменов. взяли вопросы из нужных доменов, сгенерировали рассуждения небольшой моделью (llama-3.1 8b), оценили каждый шаг большой моделью (llama-3.1 80b), и обучили на этом prm. далее при генерации ответов — взвешенный majority vote, где в качестве весов используют усреднённое по шагам предсказания prm. получают хорошие приросты по всем доменам. правда, тут есть вопросы, так как для обучения и теста применяют данные из одного и того же бенчмарка mmlu-pro. collabllm: from passive responders to active collaborators на сессии alignment and agents был доклад, отмеченный как outstanding paper. он посвящён тому, что диалоговая модель иногда должна отвечать не сразу, а сначала уточнить запрос пользователя — но без лишней «болтовни». доклад довольно простой, и при этом получил признание. интересное увидели ❣ алексей поспелов и ермек капушев #yaicml25 душный nlp и ещё постеры с icml 2025 rstar-math: small llms can master math reasoning with self-evolved deep thinking статья показывает, как с помощью маленькой модели (1,5-7в) добиться качества на уровне больших вроде openai o1. для этого использует реворд-модель, которая умеет оценивать каждый шаг в рассуждении (process reward model, prm) и генерируют рассуждения с помощью monte carlo tree search. как получить качественную prm: шаг за шагом бустим политику и prm. сначала обучаем начальную политику на синтетике от deepseek-coder v2 (236b). далее, используя её, получаем данные для обучения prm. следующим шагом с помощью и политики, и prm генерируем новые более качественные данные. обновляем все модели. а в конце полируем их, генерируя и обучаясь на траекториях для более сложных задач. на каждом шаге для валидации правильности шагов используется код (то есть просят модель сгенерировать python-код для проверки шага). результат — улучшение скоров на десятки процентов на математических бенчмарках. versaprm: multi-domain process reward model via synthetic reasoning data ещё немного о prm — versaprm. авторы заметили, что такие модели работают только для математики, но не на остальных доменах (биология, философия, юридический домен). проблема в данных — нет качественных размеченных пошаговых рассуждений для этих доменов. взяли вопросы из нужных доменов, сгенерировали рассуждения небольшой моделью (llama-3.1 8b), оценили каждый шаг большой моделью (llama-3.1 80b), и обучили на этом prm. далее при генерации ответов — взвешенный majority vote, где в качестве весов используют усреднённое по шагам предсказания prm. получают хорошие приросты по всем доменам. правда, тут есть вопросы, так как для обучения и теста применяют данные из одного и того же бенчмарка mmlu-pro. collabllm: from passive responders to active collaborators на сессии alignment and agents был доклад, отмеченный как outstanding paper. он посвящён тому, что диалоговая модель иногда должна отвечать не сразу, а сначала уточнить запрос пользователя — но без лишней «болтовни». доклад довольно простой, и при этом получил признание. интересное увидели ❣ алексей поспелов и ермек капушев #yaicml25 душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-07-18T14:59:36+00:00" href="./posts/155.html">2025-07-18 14:59 UTC</a></div>
      </div>
      <div class="post-body"><strong>И ещё постеры с ICML 2025</strong><br><br><a href="https://arxiv.org/abs/2501.04519" rel="nofollow noopener noreferrer"><strong>rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking</strong></a><br><br>Статья показывает, как с помощью маленькой модели (1,5-7В) добиться качества на уровне больших вроде OpenAI o1. Для этого использует реворд-модель, которая умеет оценивать каждый шаг в рассуждении (process reward model, PRM) и генерируют рассуждения с помощью Monte Carlo Tree Search. <br><br>Как получить качественную PRM: шаг за шагом бустим политику и PRM. Сначала обучаем начальную политику на синтетике от DeepSeek-coder v2 (236B). Далее, используя её, получаем данные для обучения PRM. Следующим шагом с помощью и политики, и PRM генерируем новые более качественные данные. Обновляем все модели. А в конце полируем их, генерируя и обучаясь на траекториях для более сложных задач. <br><br>На каждом шаге для валидации правильности шагов используется код (то есть просят модель сгенерировать python-код для проверки шага). Результат — улучшение скоров на десятки процентов на математических бенчмарках.<br><br><a href="https://arxiv.org/abs/2502.06737" rel="nofollow noopener noreferrer"><strong>VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data</strong></a><br><br>Ещё немного о PRM — VersaPRM. Авторы заметили, что такие модели работают только для математики, но не на остальных доменах (биология, философия, юридический домен). Проблема в данных — нет качественных размеченных пошаговых рассуждений для этих доменов. <br><br>Взяли вопросы из нужных доменов, сгенерировали рассуждения небольшой моделью (Llama-3.1 8B), оценили каждый шаг большой моделью (Llama-3.1 80B), и обучили на этом PRM. Далее при генерации ответов — взвешенный majority vote, где в качестве весов используют усреднённое по шагам предсказания PRM. <br><br>Получают хорошие приросты по всем доменам. Правда, тут есть вопросы, так как для обучения и теста применяют данные из одного и того же бенчмарка MMLU-Pro.<br><br><a href="https://icml.cc/virtual/2025/poster/45988" rel="nofollow noopener noreferrer"><strong>CollabLLM: From Passive Responders to Active Collaborators</strong></a><br><br>На сессии Alignment and Agents был доклад, отмеченный как Outstanding Paper. Он посвящён тому, что диалоговая модель иногда должна отвечать не сразу, а сначала уточнить запрос пользователя — но без лишней «болтовни». Доклад довольно простой, и при этом получил признание.<br><br><em>Интересное увидели </em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> <em>Алексей Поспелов и Ермек Капушев</em><br><br>#YaICML25<br><br><a href="https://t.me/+Jg-f8FI9jUw2Nzky" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/155_480.webp" srcset="../assets/media/thumbs/155_480.webp 480w, ../assets/media/155.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="155" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/156_480.webp" srcset="../assets/media/thumbs/156_480.webp 480w, ../assets/media/156.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="155" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/157_480.webp" srcset="../assets/media/thumbs/157_480.webp 480w, ../assets/media/157.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="155" data-image-index="2" /></div></div>
      <div class="actions">
        <span>2 142 просмотров · 18 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/155" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/155.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="151" data-search="несём новую порцию статей с icml 2025 продолжаем рассказывать о том, что увидели на конференции. outlier gradient analysis: efficiently identifying detrimental training samples for deep learning models для нахождения плохих или, наоборот, хороших примеров в датасете часто используют influence function — это некоторый скор, который показывает, насколько сильно изменится лосс, если пример убрать из обучения. проблема в том, что для вычисления функции надо обращать гессиан по параметрам модели, что вычислительно очень сложно. в этой статье заметили, что на самом деле можно смотреть только на градиенты модели по примерам, которые мы проверяем. если они сонаправлены с градиентами по данным из обучения — примеры хорошие, и наоборот. далее, на основе этого можно применять методы детекции аномалий для нахождения примеров, которые портят обучение, и отфильтровывать их (но можно делать и наоборот — искать хорошие примеры и добавлять их в обучающую выборку). основное преимущество метода — вычислительная простота; не нужны супердорогие обращения гессиана: только forward и backward pass модели для заданных примеров. towards memorization estimation: fast, formal and free как померить меморизацию посэмплово, запоминала модель пример или нет? для этого надо обучить модель один раз на данных с этим примером, а потом ещё несколько моделей на данных без него, и померить лосс на примере. это очень дорого вычислительно. но можно сделать проще — вычислять лосс на примере несколько раз в течение обучения и посчитать сумму. если она выше некоторого порога, значит модель не смогла запомнить пример. где это можно применять? для фильтрации данных. если вдруг модель никак не может выучить какой-то пример, то, вероятно, в нём есть шум (например, неправильное решение математической задачи или неполное условие). такие примеры можно выкидывать и улучшать точность модели или уменьшать компьют на обучение. интересная и простая идея, надо проверять, действительно ли она будет работать для llm (в статье проверяли только на задаче компьютерного зрения, в которой одни и те же данные проходят несколько эпох). nice data selection for instruction tuning in llms with non-differentiable evaluation metric в этой статье снова задаются вопросом, как выбирать такие примеры для обучения, чтобы на валидации получать хорошее качество. отличие в том, что качество на валидации измеряется не лоссом, а произвольной необязательно дифференцируемой функцией (например, accuracy). в качестве её градиента используют policy gradient. jailbreaking llms and agentic systems: attacks, defenses, and evaluations на туториале рассказали о защите языковых моделей от нарушения политик элайнмента — например, чтобы модель не выдавала инструкции по созданию опасных веществ или не генерировала дискриминационный контент. оказалось, что white-box-модели с доступом к весам (например, llama) до сих пор уязвимы к так называемым token-based-атакам — вставке «мусорных» токенов в промпт. с этим неплохо работают методы поиска инжекта, близкого к кластеру безопасных промптов. промпт-инжекты по-прежнему похожи на попытки обмануть не очень внимательного человека, но сейчас работают лучше. для большинства моделей удаётся подобрать рабочий инжект за 256 попыток («shots»). дальше рассказывали о методах защиты. понятный способ — кластеризовать опасные состояния, добавить состояния отклонения ответа и дообучить модель переходить в них. однако такой подход снижает качество ответов даже на безобидные вопросы (например, «how to kill python script» — из-за слова kill). другой способ — «пошатать» промпты и с помощью majority vote ответов решить, отказаться отвечать или выдать ответ на исходный промпт. при этом иногда ответить может быть приемлемо: например, если инструкция по сборке бомбы нерабочая. в заключительной части рассказали о взломе агентов. выяснилось, что там уязвимостей ещё больше, потому что появляется дополнительная возможность дать на вход вредоносный контент, причем его достаточно совсем мало. интересное увидели ❣ алексей поспелов и ермек капушев #yaicml25 душный nlp несём новую порцию статей с icml 2025 продолжаем рассказывать о том, что увидели на конференции. outlier gradient analysis: efficiently identifying detrimental training samples for deep learning models для нахождения плохих или, наоборот, хороших примеров в датасете часто используют influence function — это некоторый скор, который показывает, насколько сильно изменится лосс, если пример убрать из обучения. проблема в том, что для вычисления функции надо обращать гессиан по параметрам модели, что вычислительно очень сложно. в этой статье заметили, что на самом деле можно смотреть только на градиенты модели по примерам, которые мы проверяем. если они сонаправлены с градиентами по данным из обучения — примеры хорошие, и наоборот. далее, на основе этого можно применять методы детекции аномалий для нахождения примеров, которые портят обучение, и отфильтровывать их (но можно делать и наоборот — искать хорошие примеры и добавлять их в обучающую выборку). основное преимущество метода — вычислительная простота; не нужны супердорогие обращения гессиана: только forward и backward pass модели для заданных примеров. towards memorization estimation: fast, formal and free как померить меморизацию посэмплово, запоминала модель пример или нет? для этого надо обучить модель один раз на данных с этим примером, а потом ещё несколько моделей на данных без него, и померить лосс на примере. это очень дорого вычислительно. но можно сделать проще — вычислять лосс на примере несколько раз в течение обучения и посчитать сумму. если она выше некоторого порога, значит модель не смогла запомнить пример. где это можно применять? для фильтрации данных. если вдруг модель никак не может выучить какой-то пример, то, вероятно, в нём есть шум (например, неправильное решение математической задачи или неполное условие). такие примеры можно выкидывать и улучшать точность модели или уменьшать компьют на обучение. интересная и простая идея, надо проверять, действительно ли она будет работать для llm (в статье проверяли только на задаче компьютерного зрения, в которой одни и те же данные проходят несколько эпох). nice data selection for instruction tuning in llms with non-differentiable evaluation metric в этой статье снова задаются вопросом, как выбирать такие примеры для обучения, чтобы на валидации получать хорошее качество. отличие в том, что качество на валидации измеряется не лоссом, а произвольной необязательно дифференцируемой функцией (например, accuracy). в качестве её градиента используют policy gradient. jailbreaking llms and agentic systems: attacks, defenses, and evaluations на туториале рассказали о защите языковых моделей от нарушения политик элайнмента — например, чтобы модель не выдавала инструкции по созданию опасных веществ или не генерировала дискриминационный контент. оказалось, что white-box-модели с доступом к весам (например, llama) до сих пор уязвимы к так называемым token-based-атакам — вставке «мусорных» токенов в промпт. с этим неплохо работают методы поиска инжекта, близкого к кластеру безопасных промптов. промпт-инжекты по-прежнему похожи на попытки обмануть не очень внимательного человека, но сейчас работают лучше. для большинства моделей удаётся подобрать рабочий инжект за 256 попыток («shots»). дальше рассказывали о методах защиты. понятный способ — кластеризовать опасные состояния, добавить состояния отклонения ответа и дообучить модель переходить в них. однако такой подход снижает качество ответов даже на безобидные вопросы (например, «how to kill python script» — из-за слова kill). другой способ — «пошатать» промпты и с помощью majority vote ответов решить, отказаться отвечать или выдать ответ на исходный промпт. при этом иногда ответить может быть приемлемо: например, если инструкция по сборке бомбы нерабочая. в заключительной части рассказали о взломе агентов. выяснилось, что там уязвимостей ещё больше, потому что появляется дополнительная возможность дать на вход вредоносный контент, причем его достаточно совсем мало. интересное увидели ❣ алексей поспелов и ермек капушев #yaicml25 душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-07-17T13:34:30+00:00" href="./posts/151.html">2025-07-17 13:34 UTC</a></div>
      </div>
      <div class="post-body"><strong>Несём новую порцию статей с ICML 2025<br></strong><br>Продолжаем рассказывать о том, что увидели на конференции. <br><br><a href="https://arxiv.org/abs/2405.03869" rel="nofollow noopener noreferrer"><strong>Outlier Gradient Analysis: Efficiently Identifying Detrimental Training Samples for Deep Learning Models</strong></a><br><br>Для нахождения плохих или, наоборот, хороших примеров в датасете часто используют influence function — это некоторый скор, который показывает, насколько сильно изменится лосс, если пример убрать из обучения. Проблема в том, что для вычисления функции надо обращать гессиан по параметрам модели, что вычислительно очень сложно. <br><br>В этой статье заметили, что на самом деле можно смотреть только на градиенты модели по примерам, которые мы проверяем. Если они сонаправлены с градиентами по данным из обучения — примеры хорошие, и наоборот. Далее, на основе этого можно применять методы детекции аномалий для нахождения примеров, которые портят обучение, и отфильтровывать их (но можно делать и наоборот — искать хорошие примеры и добавлять их в обучающую выборку). Основное преимущество метода — вычислительная простота; не нужны супердорогие обращения гессиана: только forward и backward pass модели для заданных примеров.<br><br><a href="https://icml.cc/virtual/2025/poster/45633" rel="nofollow noopener noreferrer"><strong>Towards Memorization Estimation: Fast, Formal and Free</strong></a><br><br>Как померить меморизацию посэмплово, запоминала модель пример или нет? Для этого надо обучить модель один раз на данных с этим примером, а потом ещё несколько моделей на данных без него, и померить лосс на примере. Это очень дорого вычислительно. Но можно сделать проще — вычислять лосс на примере несколько раз в течение обучения и посчитать сумму. Если она выше некоторого порога, значит модель не смогла запомнить пример. <br><br>Где это можно применять? Для фильтрации данных. Если вдруг модель никак не может выучить какой-то пример, то, вероятно, в нём есть шум (например, неправильное решение математической задачи или неполное условие). Такие примеры можно выкидывать и улучшать точность модели или уменьшать компьют на обучение. Интересная и простая идея, надо проверять, действительно ли она будет работать для LLM (в статье проверяли только на задаче компьютерного зрения, в которой одни и те же данные проходят несколько эпох).<br><br><a href="https://icml.cc/virtual/2025/poster/46560" rel="nofollow noopener noreferrer"><strong>NICE Data Selection for Instruction Tuning in LLMs with Non-differentiable Evaluation Metric</strong></a><strong><br></strong><br>В этой статье снова задаются вопросом, как выбирать такие примеры для обучения, чтобы на валидации получать хорошее качество. Отличие в том, что качество на валидации измеряется не лоссом, а произвольной необязательно дифференцируемой функцией (например, accuracy). В качестве её градиента используют policy gradient.<br><br><a href="https://icml.cc/virtual/2025/40010" rel="nofollow noopener noreferrer"><strong>Jailbreaking LLMs and Agentic Systems: Attacks, Defenses, and Evaluations</strong></a><br><br>На туториале рассказали о защите языковых моделей от нарушения политик элайнмента — например, чтобы модель не выдавала инструкции по созданию опасных веществ или не генерировала дискриминационный контент. Оказалось, что white-box-модели с доступом к весам (например, Llama) до сих пор уязвимы к так называемым token-based-атакам — вставке «мусорных» токенов в промпт. С этим неплохо работают методы поиска инжекта, близкого к кластеру безопасных промптов.<br><br>Промпт-инжекты по-прежнему похожи на попытки обмануть не очень внимательного человека, но сейчас работают лучше. Для большинства моделей удаётся подобрать рабочий инжект за 256 попыток («shots»).<br><br>Дальше рассказывали о методах защиты. Понятный способ — кластеризовать опасные состояния, добавить состояния отклонения ответа и дообучить модель переходить в них. Однако такой подход снижает качество ответов даже на безобидные вопросы (например, «how to kill python script» — из-за слова kill).<br><br>Другой способ — «пошатать» промпты и с помощью majority vote ответов решить, отказаться отвечать или выдать ответ на исходный промпт. При этом иногда ответить может быть приемлемо: например, если инструкция по сборке бомбы нерабочая.<br><br>В заключительной части рассказали о взломе агентов. Выяснилось, что там уязвимостей ещё больше, потому что появляется дополнительная возможность дать на вход вредоносный контент, причем его достаточно совсем мало.<br><br><em>Интересное увидели </em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> <em>Алексей Поспелов и Ермек Капушев</em><br><br>#YaICML25<br><br><a href="https://t.me/+nIOpI3hRKbk1NGEy" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/151_480.webp" srcset="../assets/media/thumbs/151_480.webp 480w, ../assets/media/151.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="151" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/152_480.webp" srcset="../assets/media/thumbs/152_480.webp 480w, ../assets/media/152.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="151" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/153_480.webp" srcset="../assets/media/thumbs/153_480.webp 480w, ../assets/media/153.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="151" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/154_480.webp" srcset="../assets/media/thumbs/154_480.webp 480w, ../assets/media/154.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="151" data-image-index="3" /></div></div>
      <div class="actions">
        <span>2 365 просмотров · 15 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/151" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/151.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="147" data-search="соскучились по конференциям? тогда icml 2025 спешит на помощь! в ванкувере стартовала конференция icml, а это значит, что мы — уже по традиции — будем делиться самым интересным с мероприятия. и вот первая подборка постеров, с пылу с жару. scion: training deep learning models with norm-constrained lmos самый популярный оптимизатор — adamw — не делает никаких предположений о геометрии весов модели. из-за этого во время обучения надо накапливать и хранить статистики градиента. в scion сразу вводят предположение о норме весов и используют linear minimization oracle для вычисления их апдейта на каждой итерации. для разных типов слоёв можно (и нужно) использовать разные нормы. получаем менее требовательный к памяти алгоритм — не надо хранить первый и второй моменты градиента. кроме того, оптимальные гиперпараметры переносятся между моделями разных размеров. а главное — scion находит лучший лосс по сравнению с adamw и позволяет сократить общее время обучения на 25-40% . это происходит благодаря большому батчу. learning dynamics in continual pre-training for large language models было много постеров о scaling laws. на этом — исследуют динамику дообучения (continual pre-training), зависимость от lr schedule и от данных. заметили, что на дообучении лосс сходится к тому же значению, что и при обучении на этом же датасете с нуля. кроме того, лосс повторяет форму lr scheduler с некоторой задержкой. опираясь на это, выводят scaling law. ну а дальше подбирают некоторые оптимальные гиперпараметры обучения. scaling collapse reveals universal dynamics in compute-optimally trained neural networks ещё один интересный постер о scaling law. здесь показали, что если построить график нормированного лосса (нормируем на финальное значение) от нормированного компьюта (переводим в [0; 1]), то кривые для моделей разных размеров накладываются друг на друга. причём этот феномен зависит от lr и lr scheduler. для переобученных моделей кривые будут накладываться с некоторым шумом, а для неоптимальных lr — могут и вовсе расходиться. также выводят scaling law, который зависит от lr scheduler. как это можно использовать на практике — пока вопрос открытый. layer by layer: uncovering hidden representations in language models интересный постер об эмбеддингах промежуточных слоёв трансформера. всегда считалось, что если нужны эмбеддинги для какой-нибудь задачи (например, классификации), то надо просто снять их с последнего слоя, и будет хорошо. а здесь авторы исследовали, насколько хороши эмбеддинги промежуточных слоёв (проверяют на mteb), и оказалось, что всегда лучше брать какой-то промежуточный. чтобы узнать, какой именно — считаем метрику prompt entropy для каждого слоя по некоторому набору входных данных. чем она меньше — тем лучше будут работать эмбеддинги с этого слоя. интересным поделился ❣ ермек капушев #yaicml25 душный nlp соскучились по конференциям? тогда icml 2025 спешит на помощь! в ванкувере стартовала конференция icml, а это значит, что мы — уже по традиции — будем делиться самым интересным с мероприятия. и вот первая подборка постеров, с пылу с жару. scion: training deep learning models with norm-constrained lmos самый популярный оптимизатор — adamw — не делает никаких предположений о геометрии весов модели. из-за этого во время обучения надо накапливать и хранить статистики градиента. в scion сразу вводят предположение о норме весов и используют linear minimization oracle для вычисления их апдейта на каждой итерации. для разных типов слоёв можно (и нужно) использовать разные нормы. получаем менее требовательный к памяти алгоритм — не надо хранить первый и второй моменты градиента. кроме того, оптимальные гиперпараметры переносятся между моделями разных размеров. а главное — scion находит лучший лосс по сравнению с adamw и позволяет сократить общее время обучения на 25-40% . это происходит благодаря большому батчу. learning dynamics in continual pre-training for large language models было много постеров о scaling laws. на этом — исследуют динамику дообучения (continual pre-training), зависимость от lr schedule и от данных. заметили, что на дообучении лосс сходится к тому же значению, что и при обучении на этом же датасете с нуля. кроме того, лосс повторяет форму lr scheduler с некоторой задержкой. опираясь на это, выводят scaling law. ну а дальше подбирают некоторые оптимальные гиперпараметры обучения. scaling collapse reveals universal dynamics in compute-optimally trained neural networks ещё один интересный постер о scaling law. здесь показали, что если построить график нормированного лосса (нормируем на финальное значение) от нормированного компьюта (переводим в [0; 1]), то кривые для моделей разных размеров накладываются друг на друга. причём этот феномен зависит от lr и lr scheduler. для переобученных моделей кривые будут накладываться с некоторым шумом, а для неоптимальных lr — могут и вовсе расходиться. также выводят scaling law, который зависит от lr scheduler. как это можно использовать на практике — пока вопрос открытый. layer by layer: uncovering hidden representations in language models интересный постер об эмбеддингах промежуточных слоёв трансформера. всегда считалось, что если нужны эмбеддинги для какой-нибудь задачи (например, классификации), то надо просто снять их с последнего слоя, и будет хорошо. а здесь авторы исследовали, насколько хороши эмбеддинги промежуточных слоёв (проверяют на mteb), и оказалось, что всегда лучше брать какой-то промежуточный. чтобы узнать, какой именно — считаем метрику prompt entropy для каждого слоя по некоторому набору входных данных. чем она меньше — тем лучше будут работать эмбеддинги с этого слоя. интересным поделился ❣ ермек капушев #yaicml25 душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-07-16T11:43:25+00:00" href="./posts/147.html">2025-07-16 11:43 UTC</a></div>
      </div>
      <div class="post-body"><strong>Соскучились по конференциям? Тогда ICML 2025</strong> <strong>спешит на помощь!</strong><br><br>В Ванкувере стартовала конференция ICML, а это значит, что мы — уже по традиции — будем делиться самым интересным с мероприятия. И вот первая подборка постеров, с пылу с жару. <br><br><a href="https://arxiv.org/abs/2502.07529" rel="nofollow noopener noreferrer"><strong>Scion: Training Deep Learning Models with Norm-Constrained LMOs</strong></a><br><br>Самый популярный оптимизатор — AdamW — не делает никаких предположений о геометрии весов модели. Из-за этого во время обучения надо накапливать и хранить статистики градиента. В Scion сразу вводят предположение о норме весов и используют linear minimization oracle для вычисления их апдейта на каждой итерации. Для разных типов слоёв можно (и нужно) использовать разные нормы.<br><br>Получаем менее требовательный к памяти алгоритм — не надо хранить первый и второй моменты градиента. Кроме того, оптимальные гиперпараметры переносятся между моделями разных размеров. А главное — Scion находит лучший лосс по сравнению с AdamW и позволяет сократить общее время обучения на 25-40% . Это происходит благодаря большому батчу.<br><br><a href="https://arxiv.org/abs/2505.07796" rel="nofollow noopener noreferrer"><strong>Learning Dynamics in Continual Pre-Training for Large Language Models</strong></a><br><br>Было много постеров о scaling laws. На этом — исследуют динамику дообучения (continual Pre-training), зависимость от lr schedule и от данных. Заметили, что на дообучении лосс сходится к тому же значению, что и при обучении на этом же датасете с нуля. Кроме того, лосс повторяет форму lr scheduler с некоторой задержкой. Опираясь на это, выводят scaling law. Ну а дальше подбирают некоторые оптимальные гиперпараметры обучения.<br><br><a href="https://arxiv.org/abs/2507.02119" rel="nofollow noopener noreferrer"><strong>Scaling Collapse Reveals Universal Dynamics in Compute-Optimally Trained Neural Networks</strong></a><br><br>Ещё один интересный постер о scaling law. Здесь показали, что если построить график нормированного лосса (нормируем на финальное значение) от нормированного компьюта (переводим в [0; 1]), то кривые для моделей разных размеров накладываются друг на друга. Причём этот феномен зависит от lr и lr scheduler. Для переобученных моделей кривые будут накладываться с некоторым шумом, а для неоптимальных lr — могут и вовсе расходиться. Также выводят scaling law, который зависит от lr scheduler. Как это можно использовать на практике — пока вопрос открытый.<br><br><a href="https://arxiv.org/abs/2502.02013" rel="nofollow noopener noreferrer"><strong>Layer by Layer: Uncovering Hidden Representations in Language Models</strong></a><br><br>Интересный постер об эмбеддингах промежуточных слоёв трансформера. Всегда считалось, что если нужны эмбеддинги для какой-нибудь задачи (например, классификации), то надо просто снять их с последнего слоя, и будет хорошо. А здесь авторы исследовали, насколько хороши эмбеддинги промежуточных слоёв (проверяют на MTEB), и оказалось, что всегда лучше брать какой-то промежуточный. Чтобы узнать, какой именно — считаем метрику prompt entropy для каждого слоя по некоторому набору входных данных. Чем она меньше — тем лучше будут работать эмбеддинги с этого слоя.<br><br><em>Интересным поделился </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Ермек Капушев</em><br><br>#YaICML25<br><br><a href="https://t.me/+3YH7D0nopAVjYThi" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/147_480.webp" srcset="../assets/media/thumbs/147_480.webp 480w, ../assets/media/147.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="147" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/148_480.webp" srcset="../assets/media/thumbs/148_480.webp 480w, ../assets/media/148.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="147" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/149_480.webp" srcset="../assets/media/thumbs/149_480.webp 480w, ../assets/media/149.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="147" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/150_480.webp" srcset="../assets/media/thumbs/150_480.webp 480w, ../assets/media/150.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="147" data-image-index="3" /></div></div>
      <div class="actions">
        <span>2 408 просмотров · 32 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/147" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/147.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="137" data-search="впечатления от конференции iclr 2025 минувшая iclr была насыщенной и полезной. мы попросили инженеров яндекса, посетивших конференцию, поделиться впечатлениями и рассказать о том, что им запомнилось. материалы, которые упоминаются в карточках: — asynchronous rlhf. faster and more efficient off-policy rl for llms — learning dynamics of llm finetuning — cheating automatic llm benchmarks: null models achieve high win rates — strong model collapse — maximizing the potential of synthetic data: insights from random matrix theory — ist-daslab/moe-quant: code for data-aware compression of deepseek models *компания meta признана экстремистской организацией в россии. душный nlp впечатления от конференции iclr 2025 минувшая iclr была насыщенной и полезной. мы попросили инженеров яндекса, посетивших конференцию, поделиться впечатлениями и рассказать о том, что им запомнилось. материалы, которые упоминаются в карточках: — asynchronous rlhf. faster and more efficient off-policy rl for llms — learning dynamics of llm finetuning — cheating automatic llm benchmarks: null models achieve high win rates — strong model collapse — maximizing the potential of synthetic data: insights from random matrix theory — ist-daslab/moe-quant: code for data-aware compression of deepseek models *компания meta признана экстремистской организацией в россии. душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-07-11T09:27:04+00:00" href="./posts/137.html">2025-07-11 09:27 UTC</a></div>
      </div>
      <div class="post-body"><strong>Впечатления от конференции ICLR 2025</strong><br><br>Минувшая ICLR была насыщенной и полезной. Мы попросили инженеров Яндекса, посетивших конференцию, поделиться впечатлениями и рассказать о том, что им запомнилось. <br><br>Материалы, которые упоминаются в карточках:<br><br>— <a href="https://arxiv.org/abs/2410.18252" rel="nofollow noopener noreferrer">Asynchronous RLHF. Faster And More Efficient Off-Policy RL For LLMs</a> <br>— <a href="https://arxiv.org/abs/2407.10490" rel="nofollow noopener noreferrer">Learning Dynamics of LLM Finetuning</a> <br>— <a href="https://arxiv.org/abs/2410.07137" rel="nofollow noopener noreferrer">Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates</a> <br>— <a href="https://arxiv.org/abs/2410.04840" rel="nofollow noopener noreferrer">Strong Model Collapse</a><br>— <a href="https://arxiv.org/abs/2410.08942" rel="nofollow noopener noreferrer">Maximizing the Potential of Synthetic Data: Insights from Random Matrix Theory</a><br>— <a href="https://github.com/IST-DASLab/MoE-Quant/" rel="nofollow noopener noreferrer">IST-DASLab/MoE-Quant: Code for data-aware compression of DeepSeek models</a><br><br><em>*Компания Meta признана экстремистской организацией в России.</em><br><br><a href="https://t.me/+3YH7D0nopAVjYThi" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/137_480.webp" srcset="../assets/media/thumbs/137_480.webp 480w, ../assets/media/137.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="137" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/138_480.webp" srcset="../assets/media/thumbs/138_480.webp 480w, ../assets/media/138.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="137" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/139_480.webp" srcset="../assets/media/thumbs/139_480.webp 480w, ../assets/media/139.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="137" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/140_480.webp" srcset="../assets/media/thumbs/140_480.webp 480w, ../assets/media/140.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="137" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/141_480.webp" srcset="../assets/media/thumbs/141_480.webp 480w, ../assets/media/141.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="137" data-image-index="4" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/142_480.webp" srcset="../assets/media/thumbs/142_480.webp 480w, ../assets/media/142.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="137" data-image-index="5" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/143_480.webp" srcset="../assets/media/thumbs/143_480.webp 480w, ../assets/media/143.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="137" data-image-index="6" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/144_480.webp" srcset="../assets/media/thumbs/144_480.webp 480w, ../assets/media/144.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="137" data-image-index="7" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/145_480.webp" srcset="../assets/media/thumbs/145_480.webp 480w, ../assets/media/145.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="137" data-image-index="8" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/146_480.webp" srcset="../assets/media/thumbs/146_480.webp 480w, ../assets/media/146.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="137" data-image-index="9" /></div></div>
      <div class="actions">
        <span>3 078 просмотров · 21 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/137" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/137.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="136" data-search="dapo: an open-source llm reinforcement learning system at scale сегодня разберём короткую, но ёмкую статью из китая. авторы предлагают опенсорсный метод работы с большими llm rl: алгоритмы, инфраструктуру кода и датасеты. забавно, что на момент подготовки обзора у ребят почти пустой github — большая его часть заполнена картинками. dapo — dynamic sampling policy optimization — не представляет из себя чего-то кардинально нового. использованные авторами подходы либо витали в воздухе, либо публиковались в других статьях. этот метод — модификация grpo, который в свою очередь получился после улучшения ppo. все эти алгоритмы объединяет возможность переиспользовать генерации. в обычных on-policy rl-алгоритмах каждый шаг оптимизации требует генерации свежей модели. а в ppo-подобных можно заранее создать большой батч ответов и сделать для него не один, а сразу несколько шагов оптимизации. зачем? большой батч эффективнее генерировать! новое классное свойство появляется за счёт использования importance sampling и трюка с обрезкой градиентов там, где свежая политика и так уже слишком сильно отличается от той, что сгенерировала данные. конкретно dapo отличается от grpo четырьмя вещами. здесь есть: — модификация процедуры обрезки градиентов — clip-higher. верхний порог обрезки выше, чем у grpo, что улучшает итоговое качество. — динамическое сэмплирование: авторы предлагают с запасом генерировать ответы и выкидывать те, которые набрали одинаковую награду. — усреднение функционала ошибки по токенам, а не по запросам. это придаёт больший вес длинным генерациям в общем функционале. — фильтрация слишком длинных ответов. ответы, превысившие рекомендуемую длину получают небольшой штраф, а ответы вышедшие за максимальную длину — вообще не участвуют в оптимизации. кроме прочего, авторы модифицируют обучающий датасет: используют llm, которая модифицирует запросы так, чтобы правильные ответы на них были целыми числами. это упрощает парсинг ответов модели и их валидацию. самый классный, на мой взгляд, результат, — авторам dapo удалось обойти sota deepseek-r1-zero-qwen-32b в решении задач олимпиадной математики. при этом они потратили 50% от мощностей, которые использовали для аналогичного обучения qwen. разбор подготовил ❣ павел темирчев душный nlp dapo: an open-source llm reinforcement learning system at scale сегодня разберём короткую, но ёмкую статью из китая . авторы предлагают опенсорсный метод работы с большими llm rl: алгоритмы, инфраструктуру кода и датасеты. забавно, что на момент подготовки обзора у ребят почти пустой github — большая его часть заполнена картинками. dapo — d ynamic s a mpling p olicy o ptimization — не представляет из себя чего-то кардинально нового. использованные авторами подходы либо витали в воздухе, либо публиковались в других статьях. этот метод — модификация grpo, который в свою очередь получился после улучшения ppo. все эти алгоритмы объединяет возможность переиспользовать генерации. в обычных on-policy rl-алгоритмах каждый шаг оптимизации требует генерации свежей модели. а в ppo-подобных можно заранее создать большой батч ответов и сделать для него не один, а сразу несколько шагов оптимизации. зачем? большой батч эффективнее генерировать! новое классное свойство появляется за счёт использования importance sampling и трюка с обрезкой градиентов там, где свежая политика и так уже слишком сильно отличается от той, что сгенерировала данные. конкретно dapo отличается от grpo четырьмя вещами. здесь есть: — модификация процедуры обрезки градиентов — clip-higher. верхний порог обрезки выше, чем у grpo, что улучшает итоговое качество. — динамическое сэмплирование: авторы предлагают с запасом генерировать ответы и выкидывать те, которые набрали одинаковую награду. — усреднение функционала ошибки по токенам, а не по запросам. это придаёт больший вес длинным генерациям в общем функционале. — фильтрация слишком длинных ответов. ответы, превысившие рекомендуемую длину получают небольшой штраф, а ответы вышедшие за максимальную длину — вообще не участвуют в оптимизации. кроме прочего, авторы модифицируют обучающий датасет: используют llm, которая модифицирует запросы так, чтобы правильные ответы на них были целыми числами. это упрощает парсинг ответов модели и их валидацию. самый классный, на мой взгляд, результат, — авторам dapo удалось обойти sota deepseek-r1-zero-qwen-32b в решении задач олимпиадной математики. при этом они потратили 50% от мощностей, которые использовали для аналогичного обучения qwen. разбор подготовил ❣ павел темирчев душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-07-09T10:15:50+00:00" href="./posts/136.html">2025-07-09 10:15 UTC</a></div>
      </div>
      <div class="post-body"><strong>DAPO: An Open-Source LLM Reinforcement Learning System at Scale</strong><br><br>Сегодня разберём короткую, но ёмкую <a href="https://arxiv.org/abs/2503.14476" rel="nofollow noopener noreferrer">статью из Китая</a>. Авторы предлагают опенсорсный метод работы с большими LLM RL: алгоритмы, инфраструктуру кода и датасеты. Забавно, что на момент подготовки обзора у ребят почти пустой GitHub — большая его часть заполнена картинками.<br><br>DAPO — <strong>D</strong>ynamic s<strong>A</strong>mpling <strong>P</strong>olicy <strong>O</strong>ptimization — не представляет из себя чего-то кардинально нового. Использованные авторами подходы либо витали в воздухе, либо публиковались в других статьях. <br><br>Этот метод — модификация GRPO, который в свою очередь получился после улучшения PPO. Все эти алгоритмы объединяет возможность переиспользовать генерации. В обычных on-policy RL-алгоритмах каждый шаг оптимизации требует генерации свежей модели. А в PPO-подобных можно заранее создать большой батч ответов и сделать для него не один, а сразу несколько шагов оптимизации. Зачем? Большой батч эффективнее генерировать!<br><br>Новое классное свойство появляется за счёт использования importance sampling и трюка с обрезкой градиентов там, где свежая политика и так уже слишком сильно отличается от той, что сгенерировала данные.<br><br>Конкретно DAPO отличается от GRPO четырьмя вещами. Здесь есть:<br><br>— Модификация процедуры обрезки градиентов — Clip-Higher. Верхний порог обрезки выше, чем у GRPO, что улучшает итоговое качество.<br>— Динамическое сэмплирование: авторы предлагают с запасом генерировать ответы и выкидывать те, которые набрали одинаковую награду. <br>— Усреднение функционала ошибки по токенам, а не по запросам. Это придаёт больший вес длинным генерациям в общем функционале.<br>— Фильтрация слишком длинных ответов. Ответы, превысившие рекомендуемую длину получают небольшой штраф, а ответы вышедшие за максимальную длину — вообще не участвуют в оптимизации. <br><br>Кроме прочего, авторы модифицируют обучающий датасет: используют LLM, которая модифицирует запросы так, чтобы правильные ответы на них были целыми числами. Это упрощает парсинг ответов модели и их валидацию.<br><br>Самый классный, на мой взгляд, результат, — авторам DAPO удалось обойти SoTA DeepSeek-R1-Zero-Qwen-32B в решении задач олимпиадной математики. При этом они потратили 50% от мощностей, которые использовали для аналогичного обучения Qwen. <br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Павел Темирчев</em><br><br><a href="https://t.me/+n23yMGRYcKs2ZTgy" rel="nofollow noopener noreferrer">Душный NLP</a></div>
      <div class="actions">
        <span>3 099 просмотров · 14 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/136" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/136.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="135" data-search="iclr 2025: что нового в мультимодальном ранжировании на хабре вышла статья алексея спасёнова и кирилла никорова из поиска яндекса по картинкам и видео. алексей и кирилл побывали на конференции iclr, которая прошла в апреле в сингапуре, и привезли с собой не только впечатления, но и (возможно) загар, и (совершенно точно) подборку интересных статей. полностью ознакомиться с ней вы можете на хабре, а здесь расскажем о нескольких работах. multi-field adaptive retrieval работа от авторов из northeastern university, augment code и microsoft посвящена улучшению поиска по структурированным данным с произвольным числом блоков с помощью подхода под названием multi-field adaptive retrieval (mfar). авторы комбинируют близость лексикографическую (bm25) и семантическую — на основе векторных представлений. для вычисления близости между запросом и документом используется скалярное произведение (dot product), а энкодеры дообучаются в контрастивном режиме. также применяется механизм внимания: модель учится определять значимость каждого блока документа относительно запроса. на этапе генерации кандидатов сначала выбираются топ-k документов стандартными методами ретривала, после чего проводится уточнение результатов с помощью mfar. multimodal unsupervised domain generalization by retrieving across the modality gap авторы из boston university предлагают подход к задаче domain generalization — улучшение обобщающей способности моделей без доступа к целевому домену. они улучшают качество поиска с использованием approximate nearest neighbor (ann) за счёт уточнённых эмбеддингов объектов. для этого используется аугментация текстовых описаний классов: к каждому классу генерируется набор вариантов запросов, после чего вычисляются эмбеддинги этих текстов. центроиды изображений смещаются в сторону усреднённых позиций, рассчитанных относительно эмбеддингов аугментированных текстов. полученные представления используются для дообучения clip — таким образом модель становится более устойчивой к вариативности запросов и доменных сдвигов. tempme: video temporal token merging for efficient text-video retrieval в этой статье авторы предлагают новую архитектуру для ранжирования видео по текстовому запросу. temporal token merging (tempme) — эффективная в вычислительном плане архитектура с небольшим количеством параметров. основа архитектуры — text-video-clip-модель. выигрыш в вычислительном плане достигается благодаря так называемым блокам intra- и cross-clip merging. в них происходят агрегации эмбеддингов похожих кадров и патчей. тем самым от слоя к слою уменьшается не только пространственная размерность, но и временная. авторы получают ускорение в 1,8 раза и улучшение качества ранжирования видео на 4,4% (в терминах mar@10), по сравнению с предыдущими вычислительно эффективными методами text-video retrieval. в данных использовались как очень короткие видео по 4–5 секунд (датасет lsmdc), так и довольно продолжительные — вплоть до 20 минут (датасет activitynet). однако домен всех датасетов, конечно же, сильно смещён относительно стандартного поискового потока. #yaiclr душный nlp iclr 2025: что нового в мультимодальном ранжировании на хабре вышла статья алексея спасёнова и кирилла никорова из поиска яндекса по картинкам и видео. алексей и кирилл побывали на конференции iclr, которая прошла в апреле в сингапуре, и привезли с собой не только впечатления, но и (возможно) загар, и (совершенно точно) подборку интересных статей. полностью ознакомиться с ней вы можете на хабре, а здесь расскажем о нескольких работах. multi-field adaptive retrieval работа от авторов из northeastern university, augment code и microsoft посвящена улучшению поиска по структурированным данным с произвольным числом блоков с помощью подхода под названием multi-field adaptive retrieval (mfar). авторы комбинируют близость лексикографическую (bm25) и семантическую — на основе векторных представлений. для вычисления близости между запросом и документом используется скалярное произведение (dot product), а энкодеры дообучаются в контрастивном режиме. также применяется механизм внимания: модель учится определять значимость каждого блока документа относительно запроса. на этапе генерации кандидатов сначала выбираются топ-k документов стандартными методами ретривала, после чего проводится уточнение результатов с помощью mfar. multimodal unsupervised domain generalization by retrieving across the modality gap авторы из boston university предлагают подход к задаче domain generalization — улучшение обобщающей способности моделей без доступа к целевому домену. они улучшают качество поиска с использованием approximate nearest neighbor (ann) за счёт уточнённых эмбеддингов объектов. для этого используется аугментация текстовых описаний классов: к каждому классу генерируется набор вариантов запросов, после чего вычисляются эмбеддинги этих текстов. центроиды изображений смещаются в сторону усреднённых позиций, рассчитанных относительно эмбеддингов аугментированных текстов. полученные представления используются для дообучения clip — таким образом модель становится более устойчивой к вариативности запросов и доменных сдвигов. tempme: video temporal token merging for efficient text-video retrieval в этой статье авторы предлагают новую архитектуру для ранжирования видео по текстовому запросу. temporal token merging ( tempme ) — эффективная в вычислительном плане архитектура с небольшим количеством параметров. основа архитектуры — text-video-clip-модель. выигрыш в вычислительном плане достигается благодаря так называемым блокам intra- и cross-clip merging. в них происходят агрегации эмбеддингов похожих кадров и патчей. тем самым от слоя к слою уменьшается не только пространственная размерность, но и временная. авторы получают ускорение в 1,8 раза и улучшение качества ранжирования видео на 4,4% (в терминах mar@10), по сравнению с предыдущими вычислительно эффективными методами text-video retrieval. в данных использовались как очень короткие видео по 4–5 секунд (датасет lsmdc), так и довольно продолжительные — вплоть до 20 минут (датасет activitynet). однако домен всех датасетов, конечно же, сильно смещён относительно стандартного поискового потока. #yaiclr душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-07-03T08:11:14+00:00" href="./posts/135.html">2025-07-03 08:11 UTC</a></div>
      </div>
      <div class="post-body"><strong>ICLR 2025: что нового в мультимодальном ранжировании</strong><br><br>На <a href="https://habr.com/ru/companies/yandex/articles/923488/" rel="nofollow noopener noreferrer">Хабре вышла статья</a> Алексея Спасёнова и Кирилла Никорова из Поиска Яндекса по картинкам и видео. Алексей и Кирилл побывали на конференции ICLR, которая прошла в апреле в Сингапуре, и привезли с собой не только впечатления, но и (возможно) загар, и (совершенно точно) подборку интересных статей. Полностью ознакомиться с ней вы можете на Хабре, а здесь расскажем о нескольких работах. <br><br><a href="https://arxiv.org/pdf/2410.20056" rel="nofollow noopener noreferrer"><strong>Multi-Field Adaptive Retrieval</strong></a><br><br>Работа от авторов из Northeastern University, Augment Code и Microsoft посвящена улучшению поиска по структурированным данным с произвольным числом блоков с помощью подхода под названием Multi-Field Adaptive Retrieval (MFAR).<br><br>Авторы комбинируют близость лексикографическую (BM25) и семантическую — на основе векторных представлений. Для вычисления близости между запросом и документом используется скалярное произведение (dot product), а энкодеры дообучаются в контрастивном режиме.<br><br>Также применяется механизм внимания: модель учится определять значимость каждого блока документа относительно запроса. На этапе генерации кандидатов сначала выбираются топ-k документов стандартными методами ретривала, после чего проводится уточнение результатов с помощью MFAR.<br><br><a href="https://arxiv.org/pdf/2402.04416" rel="nofollow noopener noreferrer"><strong>Multimodal Unsupervised Domain Generalization by Retrieving Across the Modality Gap</strong></a><br><br>Авторы из Boston University предлагают подход к задаче Domain Generalization — улучшение обобщающей способности моделей без доступа к целевому домену.<br><br>Они улучшают качество поиска с использованием Approximate Nearest Neighbor (ANN) за счёт уточнённых эмбеддингов объектов. Для этого используется аугментация текстовых описаний классов: к каждому классу генерируется набор вариантов запросов, после чего вычисляются эмбеддинги этих текстов.<br><br>Центроиды изображений смещаются в сторону усреднённых позиций, рассчитанных относительно эмбеддингов аугментированных текстов. Полученные представления используются для дообучения CLIP — таким образом модель становится более устойчивой к вариативности запросов и доменных сдвигов.<br><br><a href="https://arxiv.org/pdf/2409.01156" rel="nofollow noopener noreferrer"><strong>TempMe: Video Temporal Token Merging for Efficient Text-Video Retrieval</strong></a><strong> </strong><br><br>В этой статье авторы предлагают новую архитектуру для ранжирования видео по текстовому запросу. Temporal Token Merging (<a href="https://github.com/LunarShen/TempMe" rel="nofollow noopener noreferrer">TempMe</a>) — эффективная в вычислительном плане архитектура с небольшим количеством параметров. Основа архитектуры — text-video-CLIP-модель. <br>Выигрыш в вычислительном плане достигается благодаря так называемым блокам Intra- и Cross-clip Merging. В них происходят агрегации эмбеддингов похожих кадров и патчей. Тем самым от слоя к слою уменьшается не только пространственная размерность, но и временная.<br><br>Авторы получают ускорение в 1,8 раза и улучшение качества ранжирования видео на 4,4% (в терминах mAR@10), по сравнению с предыдущими вычислительно эффективными методами text-video retrieval. В данных использовались как очень короткие видео по 4–5 секунд (датасет LSMDC), так и довольно продолжительные — вплоть до 20 минут (датасет ActivityNet). Однако домен всех датасетов, конечно же, сильно смещён относительно стандартного поискового потока.<br><br>#YaICLR<br><br><a href="https://t.me/+bA9RnvyroYRmNjUy" rel="nofollow noopener noreferrer">Душный NLP</a></div>
      <div class="actions">
        <span>3 698 просмотров · 13 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/135" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/135.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="134" data-search="srpo — альтернатива dpo сегодняшняя статья о self-improving robust preference optimization (srpo). это алгоритм оффлайн-rlhf, подобный dpo, но более подходящий для off-policy датасета ранжирования. кроме того, srpo лучше переносится на ood-задачи. основная идея метода заключается в состязательном обучении двух политик: генерирующей и улучшающей. задача улучшающей политики — на основании запроса и имеющегося ответа создать улучшенную версию этого ответа; задача генерирующей — научиться создавать ответы, которые нельзя значительно улучшить. обе политики обучаются на парах предпочтений, полученных от людей. решение состязательной задачи сводится к минимизации линейной комбинации из двух сонаправленных функций потерь. в работе показано, что оптимальное решение этой задачи не зависит от политики, из которой был собран датасет предпочтений. благодаря этому srpo оказывается более устойчивым к изменению в распределении данных. метод можно реализовать с помощью одной llm, которая выступает и в качестве генератора, и в качестве «улучшатора». обученную модель можно применять итеративно, каждый раз корректируя ответ, полученный на предыдущем шаге, чего не предполагают методы вроде dpo или ipo. даже без итераций, srpo выигрывает у dpo и ipo: на сложных arena-hard-промптах метод показывает 56% win-rate. на задаче суммаризации reddit tl;dr srpo на 4-й итерации srpo достигает максимального качества. разбор подготовил ❣ алексей зотов душный nlp srpo — альтернатива dpo сегодняшняя статья о self-improving robust preference optimization (srpo). это алгоритм оффлайн-rlhf, подобный dpo, но более подходящий для off-policy датасета ранжирования. кроме того, srpo лучше переносится на ood-задачи. основная идея метода заключается в состязательном обучении двух политик: генерирующей и улучшающей. задача улучшающей политики — на основании запроса и имеющегося ответа создать улучшенную версию этого ответа; задача генерирующей — научиться создавать ответы, которые нельзя значительно улучшить. обе политики обучаются на парах предпочтений, полученных от людей. решение состязательной задачи сводится к минимизации линейной комбинации из двух сонаправленных функций потерь. в работе показано, что оптимальное решение этой задачи не зависит от политики, из которой был собран датасет предпочтений. благодаря этому srpo оказывается более устойчивым к изменению в распределении данных. метод можно реализовать с помощью одной llm, которая выступает и в качестве генератора, и в качестве «улучшатора». обученную модель можно применять итеративно, каждый раз корректируя ответ, полученный на предыдущем шаге, чего не предполагают методы вроде dpo или ipo. даже без итераций, srpo выигрывает у dpo и ipo: на сложных arena-hard-промптах метод показывает 56% win-rate. на задаче суммаризации reddit tl;dr srpo на 4-й итерации srpo достигает максимального качества. разбор подготовил ❣ алексей зотов душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-06-30T08:15:51+00:00" href="./posts/134.html">2025-06-30 08:15 UTC</a></div>
      </div>
      <div class="post-body"><strong>SRPO — альтернатива DPO</strong><br><br>Сегодняшняя <a href="https://arxiv.org/abs/2406.01660" rel="nofollow noopener noreferrer">статья</a> о Self-Improving Robust Preference Optimization (SRPO). Это алгоритм оффлайн-RLHF, подобный DPO, но более подходящий для off-policy датасета ранжирования. Кроме того, SRPO лучше переносится на OOD-задачи. <br><br>Основная идея метода заключается в состязательном обучении двух политик: генерирующей и улучшающей. Задача улучшающей политики — на основании запроса и имеющегося ответа создать улучшенную версию этого ответа; задача генерирующей — научиться создавать ответы, которые нельзя значительно улучшить. <br><br>Обе политики обучаются на парах предпочтений, полученных от людей. Решение состязательной задачи сводится к минимизации линейной комбинации из двух сонаправленных функций потерь. В работе показано, что оптимальное решение этой задачи не зависит от политики, из которой был собран датасет предпочтений. Благодаря этому SRPO оказывается более устойчивым к изменению в распределении данных.<br><br>Метод можно реализовать с помощью одной LLM, которая выступает и в качестве генератора, и в качестве «улучшатора». Обученную модель можно применять итеративно, каждый раз корректируя ответ, полученный на предыдущем шаге, чего не предполагают методы вроде DPO или IPO.<br><br>Даже без итераций, SRPO выигрывает у DPO и IPO: на сложных Arena-Hard-промптах метод показывает 56% win-rate. На задаче суммаризации Reddit TL;DR SRPO на 4-й итерации SRPO достигает максимального качества. <br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Алексей Зотов</em><br><br><a href="https://t.me/+PKgx81sjtfA2Y2E6" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/134_480.webp" srcset="../assets/media/thumbs/134_480.webp 480w, ../assets/media/134.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="134" data-image-index="0" /></div></div>
      <div class="actions">
        <span>3 301 просмотров · 26 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/134" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/134.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="133" data-search="scaling laws for precision scaling laws успешно применяются при проектировании llm, позволяя определить оптимальное число параметров модели n и объём обучающих данных d (в токенах) для минимизации лосса l при фиксированных вычислительном бюджете c. эта методология, например, использовалась при создании флагманской модели llama 3. сегодня разберём публикацию о чувствительном к точности scaling law. авторы статьи подчёркивают, что традиционные scaling laws предполагают фиксированную точность представления параметров модели p (например, fp16) как на этапе обучения, так и на инференсе. в свете развития аппаратной поддержки вычислений с пониженной точностью (например, fp4 в архитектуре nvidia blackwell), исследование оптимального компромисса между p, n и d становится важной задачей. поэтому авторы публикации решили проанализировать влияние квантизации после обучения (post-training quantization) модели на качество и модификации scaling laws с учётом точности параметров. авторы провели 465 экспериментов с моделями размером от 30m до 1,7b (n), обученными на 1,5–26b токенов (d), с использованием точности от 3 до 16 бит (p). в основе исследований — архитектура olmo и датасет dolma v1.7, а в качестве алгоритма квантизации — gptq. основные выводы: — деградация качества после квантизации усиливается при росте соотношения d/(n⋅p) и сильном сжатии весов. перетренированные (overtrainned) модели — с высоким d/(n⋅p) — демонстрируют наибольшую чувствительность к квантизации. в крайних случаях увеличение d приводит к ухудшению итогового качества после квантизации, то есть дополнительное обучение начинает вредить инференсу. тут можно заметить противоречие, если мы захотим обучить модель с фиксированным числом параметров n: с одной стороны, уменьшение точности весов модели при обучении делает её менее чувствительной к пост-квантизации, а с другой — это увеличивает отношение d/(n⋅p), из-за чего качество будет деградировать. однако эксперименты показали, что первый эффект перевешивает второй. — в статье предложили модифицированную формулу для scaling laws с учётом post-train-квантизации, а также точности p для весов, активаций и kv-кэша. — авторы пришли к выводам, что оптимальная битность при совместной оптимизации n, d, p в их пайплайне составила 7–8 — независимо от бюджета c=n⋅d⋅p. это говорит о том, что на практике обучение в fp16 может быть избыточным, в то время как погоня за слишком низкой битностью (ниже 4 бит) потребует непропорционального увеличения n (более чем в четыре раза) и сделает такие подходы неэффективными. — обнаруженная авторами зависимость показывает, что при фиксированном c уменьшение p приоритезирует рост n над увеличением d. например, при переходе от fp16 к fp8 освободившиеся ресурсы в первую очередь стоит потратить на увеличение размера модели. — в случае, если мы обучаем модель с фиксированным числом параметров n (например, когда обучаем семейство моделей на общем претрейн-датасете), оптимальная точность весов p для перетренерованной модели без post-train-квантизации растёт, при увеличении числа токенов в претрейне d. несмотря на интересные результаты, авторы отмечают, что у их работы есть ограничения, которые ещё необходимо исследовать. так, они использовали единую архитектуру для моделей с различной точностью p; в расчётах полагали, что скорость вычислений линейно зависит от p, а это не всегда верно на практике. также для оценки качества модели использовали только лосс без метрик в downstream-задачах. разбор подготовил ❣ дмитрий ульянов душный nlp scaling laws for precision scaling laws успешно применяются при проектировании llm, позволяя определить оптимальное число параметров модели n и объём обучающих данных d (в токенах) для минимизации лосса l при фиксированных вычислительном бюджете c . эта методология, например, использовалась при создании флагманской модели llama 3. сегодня разберём публикацию о чувствительном к точности scaling law. авторы статьи подчёркивают, что традиционные scaling laws предполагают фиксированную точность представления параметров модели p (например, fp16) как на этапе обучения, так и на инференсе. в свете развития аппаратной поддержки вычислений с пониженной точностью (например, fp4 в архитектуре nvidia blackwell), исследование оптимального компромисса между p , n и d становится важной задачей. поэтому авторы публикации решили проанализировать влияние квантизации после обучения (post-training quantization) модели на качество и модификации scaling laws с учётом точности параметров. авторы провели 465 экспериментов с моделями размером от 30m до 1,7b ( n ), обученными на 1,5–26b токенов ( d ), с использованием точности от 3 до 16 бит ( p ). в основе исследований — архитектура olmo и датасет dolma v1.7, а в качестве алгоритма квантизации — gptq. основные выводы: — деградация качества после квантизации усиливается при росте соотношения d/(n⋅p) и сильном сжатии весов. перетренированные (overtrainned) модели — с высоким d/(n⋅p) — демонстрируют наибольшую чувствительность к квантизации. в крайних случаях увеличение d приводит к ухудшению итогового качества после квантизации, то есть дополнительное обучение начинает вредить инференсу. тут можно заметить противоречие, если мы захотим обучить модель с фиксированным числом параметров n : с одной стороны, уменьшение точности весов модели при обучении делает её менее чувствительной к пост-квантизации, а с другой — это увеличивает отношение d/(n⋅p) , из-за чего качество будет деградировать. однако эксперименты показали, что первый эффект перевешивает второй. — в статье предложили модифицированную формулу для scaling laws с учётом post-train-квантизации, а также точности p для весов, активаций и kv-кэша. — авторы пришли к выводам, что оптимальная битность при совместной оптимизации n , d , p в их пайплайне составила 7–8 — независимо от бюджета c=n⋅d⋅p . это говорит о том, что на практике обучение в fp16 может быть избыточным, в то время как погоня за слишком низкой битностью (ниже 4 бит) потребует непропорционального увеличения n (более чем в четыре раза) и сделает такие подходы неэффективными. — обнаруженная авторами зависимость показывает, что при фиксированном c уменьшение p приоритезирует рост n над увеличением d . например, при переходе от fp16 к fp8 освободившиеся ресурсы в первую очередь стоит потратить на увеличение размера модели. — в случае, если мы обучаем модель с фиксированным числом параметров n (например, когда обучаем семейство моделей на общем претрейн-датасете), оптимальная точность весов p для перетренерованной модели без post-train-квантизации растёт, при увеличении числа токенов в претрейне d . несмотря на интересные результаты, авторы отмечают, что у их работы есть ограничения, которые ещё необходимо исследовать. так, они использовали единую архитектуру для моделей с различной точностью p ; в расчётах полагали, что скорость вычислений линейно зависит от p , а это не всегда верно на практике. также для оценки качества модели использовали только лосс без метрик в downstream-задачах. разбор подготовил ❣ дмитрий ульянов душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-06-19T11:00:01+00:00" href="./posts/133.html">2025-06-19 11:00 UTC</a></div>
      </div>
      <div class="post-body"><strong>Scaling Laws for Precision</strong><br><br>Scaling laws успешно применяются при проектировании LLM, позволяя определить оптимальное число параметров модели <em>N</em> и объём обучающих данных <em>D</em> (в токенах) для минимизации лосса <em>L</em> при фиксированных вычислительном бюджете <em>C</em>. Эта методология, например, использовалась при создании флагманской модели LLaMA 3. Сегодня разберём <a href="https://arxiv.org/abs/2411.04330" rel="nofollow noopener noreferrer">публикацию</a> о чувствительном к точности scaling law.<br><br>Авторы статьи подчёркивают, что традиционные scaling laws предполагают фиксированную точность представления параметров модели <em>P</em> (например, FP16) как на этапе обучения, так и на инференсе. В свете развития аппаратной поддержки вычислений с пониженной точностью (например, FP4 в архитектуре NVIDIA Blackwell), исследование оптимального компромисса между <em>P</em>, <em>N</em> и <em>D</em> становится важной задачей. Поэтому авторы публикации решили проанализировать влияние квантизации после обучения (post-training quantization) модели на качество и модификации scaling laws с учётом точности параметров. <br><br>Авторы провели 465 экспериментов с моделями размером от 30M до 1,7B (<em>N</em>), обученными на 1,5–26B токенов (<em>D</em>), с использованием точности от 3 до 16 бит (<em>P</em>). В основе исследований — архитектура OLMo и датасет Dolma v1.7, а в качестве алгоритма квантизации — GPTQ. Основные выводы:<br><br>— Деградация качества после квантизации усиливается при росте соотношения <em>D/(N⋅P)</em> и сильном сжатии весов. Перетренированные (overtrainned) модели — с высоким <em>D/(N⋅P)</em> — демонстрируют наибольшую чувствительность к квантизации. В крайних случаях увеличение <em>D</em> приводит к ухудшению итогового качества после квантизации, то есть дополнительное обучение начинает вредить инференсу. Тут можно заметить противоречие, если мы захотим обучить модель с фиксированным числом параметров <em>N</em>: с одной стороны, уменьшение точности весов модели при обучении делает её менее чувствительной к пост-квантизации, а с другой — это увеличивает отношение <em>D/(N⋅P)</em>, из-за чего качество будет деградировать. Однако эксперименты показали, что первый эффект перевешивает второй.<br><br>— В статье предложили модифицированную формулу для scaling laws с учётом post-train-квантизации, а также точности <em>P</em> для весов, активаций и KV-кэша.<br><br>— Авторы пришли к выводам, что оптимальная битность при совместной оптимизации <em>N</em>,<em> D</em>,<em> P</em> в их пайплайне составила 7–8 — независимо от бюджета <em>C=N⋅D⋅P</em>. Это говорит о том, что на практике обучение в FP16 может быть избыточным, в то время как погоня за слишком низкой битностью (ниже 4 бит) потребует непропорционального увеличения <em>N</em> (более чем в четыре раза) и сделает такие подходы неэффективными.<br><br>— Обнаруженная авторами зависимость показывает, что при фиксированном <em>C</em> уменьшение <em>P</em> приоритезирует рост <em>N</em> над увеличением <em>D</em>. Например, при переходе от FP16 к FP8 освободившиеся ресурсы в первую очередь стоит потратить на увеличение размера модели.<br><br>— В случае, если мы обучаем модель с фиксированным числом параметров <em>N</em> (например, когда обучаем семейство моделей на общем претрейн-датасете), оптимальная точность весов <em>P</em> для перетренерованной модели без post-train-квантизации растёт, при увеличении числа токенов в претрейне <em>D</em>.<br><br>Несмотря на интересные результаты, авторы отмечают, что у их работы есть ограничения, которые ещё необходимо исследовать. Так, они использовали единую архитектуру для моделей с различной точностью <em>P</em>; в расчётах полагали, что скорость вычислений линейно зависит от <em>P</em>, а это не всегда верно на практике. Также для оценки качества модели использовали только лосс без метрик в downstream-задачах.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Дмитрий Ульянов</em><br><br><a href="https://t.me/+zB8fenlihy0zZDEy" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/133_480.webp" srcset="../assets/media/thumbs/133_480.webp 480w, ../assets/media/133.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="133" data-image-index="0" /></div></div>
      <div class="actions">
        <span>3 856 просмотров · 26 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/133" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/133.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="132" data-search="параллельная генерация с hogwild! inference сегодня — статья инженеров yandex research, hse и ist austria. речь в публикации идёт о hogwild! inference — движке параллельного инференса для llm. авторы задались целью ускорить выполнение задачи одной моделью за счёт параллельной генерации. при этом инференс должен был оставаться интуитивно простым, а фреймворк — достаточно гибким, чтобы сделать эффективной коммуникацию между параллельными ветками генерации. наконец, авторы стремились к тому, чтобы характер взаимодействия инстансов зависел в первую очередь от самой модели, а не от фреймворка параллельной генерации, то есть оставить принцип параллельной работы на откуп самим моделям. метод hogwild! inference предполагает использование нескольких экземпляров llm — они называются «рабочими» (workers), — которые выполняют одну задачу параллельно, синхронизируясь через общий kv-кэш. это позволяет им видеть и учитывать генерации друг друга в реальном времени. идея в том, чтобы дать моделям возможность самим организовывать координацию без заранее заданных правил взаимодействия. в этот общий kv-кэш каждый рабочий добавляет свои токены, которые затем дополняют общий контекст. кэш организован как чат: завершённые абзацы reasoning каждого рабочего перемещаются в «историю», а текущие абзацы остаются в отдельном сегменте. при этом каждый рабочий видит текущую работу других — всё благодаря разделённым kv-блокам. чтобы избежать повторной обработки представлений на каждом шаге, авторы предлагают использовать свойства rope: для генерации нового токена каждым из рабочих блоки kv-кэша упорядочиваются по-разному для каждого рабочего (см. изображение). при этом сдвиг осуществляется не над всем блоком, а над query-токенами, что резко снижает вычислительные издержки. таким образом, каждый рабочий может видеть новые токены других рабочих сразу после их генерации. система использует zero-shot prompting: рабочим предлагается обсуждать решение задачи, разделять работу между собой, не дублировать друг друга. также авторы используют специальные интервенции в процесс генерации, чтобы сократить случаи, когда несколько рабочих совершают одну и ту же работу. каждую n токенов одному из агентов подсовывается промпт вида «делаю ли я лишнюю работу?» и предлагается ответить «да» или «нет». эксперименты показывают, что такая вставка часто позволяет рабочему понять, что его работа уже сделана другим и можно двигаться дальше, либо изменить свою стратегию решения задачи. авторы оценивают hogwild! inference на задачах, требующих длительных рассуждений и предполагающих тривиального разбиения на независимые подзадачи: limo, livecodebench, olympiadbench, aime. эксперименты на разных моделях (qwen3, qwq, deepseek r1, phi4-r) показывают, что метод позволяет решать задачи за меньшее число последовательных шагов, чем обычная генерация. например, qwq-32b в limo (817 задач на математику) c использованием hogwild! даёт прирост точности до 0,6 при 4000 токенах, в то время как бейзлайн — на уровне 0,4. эксперименты также подтверждают масштабируемость: при двух рабочих генерация ускоряется в 1,8 раза, при четырёх — в 3,4. разбор подготовил ❣ глеб родионов душный nlp параллельная генерация с hogwild! inference сегодня — статья инженеров yandex research, hse и ist austria. речь в публикации идёт о hogwild! inference — движке параллельного инференса для llm. авторы задались целью ускорить выполнение задачи одной моделью за счёт параллельной генерации. при этом инференс должен был оставаться интуитивно простым, а фреймворк — достаточно гибким, чтобы сделать эффективной коммуникацию между параллельными ветками генерации. наконец, авторы стремились к тому, чтобы характер взаимодействия инстансов зависел в первую очередь от самой модели, а не от фреймворка параллельной генерации, то есть оставить принцип параллельной работы на откуп самим моделям. метод hogwild! inference предполагает использование нескольких экземпляров llm — они называются «рабочими» (workers), — которые выполняют одну задачу параллельно, синхронизируясь через общий kv-кэш. это позволяет им видеть и учитывать генерации друг друга в реальном времени. идея в том, чтобы дать моделям возможность самим организовывать координацию без заранее заданных правил взаимодействия. в этот общий kv-кэш каждый рабочий добавляет свои токены, которые затем дополняют общий контекст. кэш организован как чат: завершённые абзацы reasoning каждого рабочего перемещаются в «историю», а текущие абзацы остаются в отдельном сегменте. при этом каждый рабочий видит текущую работу других — всё благодаря разделённым kv-блокам. чтобы избежать повторной обработки представлений на каждом шаге, авторы предлагают использовать свойства rope: для генерации нового токена каждым из рабочих блоки kv-кэша упорядочиваются по-разному для каждого рабочего (см. изображение). при этом сдвиг осуществляется не над всем блоком, а над query-токенами, что резко снижает вычислительные издержки. таким образом, каждый рабочий может видеть новые токены других рабочих сразу после их генерации. система использует zero-shot prompting: рабочим предлагается обсуждать решение задачи, разделять работу между собой, не дублировать друг друга. также авторы используют специальные интервенции в процесс генерации, чтобы сократить случаи, когда несколько рабочих совершают одну и ту же работу. каждую n токенов одному из агентов подсовывается промпт вида «делаю ли я лишнюю работу?» и предлагается ответить «да» или «нет». эксперименты показывают, что такая вставка часто позволяет рабочему понять, что его работа уже сделана другим и можно двигаться дальше, либо изменить свою стратегию решения задачи. авторы оценивают hogwild! inference на задачах, требующих длительных рассуждений и предполагающих тривиального разбиения на независимые подзадачи: limo, livecodebench, olympiadbench, aime. эксперименты на разных моделях (qwen3, qwq, deepseek r1, phi4-r) показывают, что метод позволяет решать задачи за меньшее число последовательных шагов, чем обычная генерация. например, qwq-32b в limo (817 задач на математику) c использованием hogwild! даёт прирост точности до 0,6 при 4000 токенах, в то время как бейзлайн — на уровне 0,4. эксперименты также подтверждают масштабируемость: при двух рабочих генерация ускоряется в 1,8 раза, при четырёх — в 3,4. разбор подготовил ❣ глеб родионов душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-06-17T12:16:48+00:00" href="./posts/132.html">2025-06-17 12:16 UTC</a></div>
      </div>
      <div class="post-body"><strong>Параллельная генерация с Hogwild! Inference</strong><br><br>Сегодня — <a href="https://arxiv.org/abs/2504.06261" rel="nofollow noopener noreferrer">статья</a> инженеров Yandex Research, HSE и IST Austria. Речь в публикации идёт о Hogwild! Inference — движке параллельного инференса для LLM.<br><br>Авторы задались целью ускорить выполнение задачи одной моделью за счёт параллельной генерации. При этом инференс должен был оставаться интуитивно простым, а фреймворк — достаточно гибким, чтобы сделать эффективной коммуникацию между параллельными ветками генерации. Наконец, авторы стремились к тому, чтобы характер взаимодействия инстансов зависел в первую очередь от самой модели, а не от фреймворка параллельной генерации, то есть оставить принцип параллельной работы на откуп самим моделям.<br><br>Метод Hogwild! Inference предполагает использование нескольких экземпляров LLM — они называются «рабочими» (workers), — которые выполняют одну задачу параллельно, синхронизируясь через общий KV-кэш. Это позволяет им видеть и учитывать генерации друг друга в реальном времени. Идея в том, чтобы дать моделям возможность самим организовывать координацию без заранее заданных правил взаимодействия.<br><br>В этот общий KV-кэш каждый рабочий добавляет свои токены, которые затем дополняют общий контекст. Кэш организован как чат: завершённые абзацы reasoning каждого рабочего перемещаются в «историю», а текущие абзацы остаются в отдельном сегменте. При этом каждый рабочий видит текущую работу других — всё благодаря разделённым KV-блокам. <br><br>Чтобы избежать повторной обработки представлений на каждом шаге, авторы предлагают использовать свойства RoPE: для генерации нового токена каждым из рабочих блоки KV-кэша упорядочиваются по-разному для каждого рабочего (см. изображение). При этом сдвиг осуществляется не над всем блоком, а над query-токенами, что резко снижает вычислительные издержки. Таким образом, каждый рабочий может видеть новые токены других рабочих сразу после их генерации.<br><br>Система использует zero-shot prompting: рабочим предлагается обсуждать решение задачи, разделять работу между собой, не дублировать друг друга. Также авторы используют специальные интервенции в процесс генерации, чтобы сократить случаи, когда несколько рабочих совершают одну и ту же работу. Каждую N токенов одному из агентов подсовывается промпт вида «Делаю ли я лишнюю работу?» и предлагается ответить «да» или «нет». Эксперименты показывают, что такая вставка часто позволяет рабочему понять, что его работа уже сделана другим и можно двигаться дальше, либо изменить свою стратегию решения задачи. <br><br>Авторы оценивают Hogwild! Inference на задачах, требующих длительных рассуждений и предполагающих тривиального разбиения на независимые подзадачи: LIMO, LiveCodeBench, OlympiadBench, AIME. Эксперименты на разных моделях (Qwen3, QwQ, Deepseek R1, Phi4-R) показывают, что метод позволяет решать задачи за меньшее число последовательных шагов, чем обычная генерация. Например, QwQ-32B в LIMO (817 задач на математику) c использованием Hogwild! даёт прирост точности до 0,6 при 4000 токенах, в то время как бейзлайн — на уровне 0,4. Эксперименты также подтверждают масштабируемость: при двух рабочих генерация ускоряется в 1,8 раза, при четырёх — в 3,4.<br><br><em>Разбор подготовил </em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> <em>Глеб Родионов</em><br><br><a href="https://t.me/+_UgXDlenCHw0OWRi" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/132_480.webp" srcset="../assets/media/thumbs/132_480.webp 480w, ../assets/media/132.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="132" data-image-index="0" /></div></div>
      <div class="actions">
        <span>3 240 просмотров · 43 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/132" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/132.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="131" data-search="cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars сегодня разберём статью от группы исследователей из стэнфордского университета про когнитивное поведение. авторы выясняют, при каких условиях модель становится self-improving reasoner: то есть, может учиться находить правильное решение без специальной разметки. test-time compute scaling — довольно мощная парадигма для задач, которые требуют рассуждения. для deepseek-r1-zero было показано: обучаясь решать задачи по математике и программированию, модель самостоятельно учится генерировать цепочки рассуждений. в этой статье авторы применяют тот же принцип к моделям qwen-2.5-3b и llama-3.2-3b с одинаковым сетапом обучения (rl+grpo) для задачи countdown. countdown — это когда из нескольких чисел с помощью стандартных арифметических операций (сложение, вычитание, умножение и деление) нужно получить целевое число. как видно из графиков, модель qwen довольно быстро достигла неплохого качества, в то время как llama сходится медленнее и работает хуже. проанализировав результаты, авторы обнаружили четыре главных когнитивных паттерна для решения логических задач: — verification. проверка, верно ли логически выдвинутое рассуждение. — backtracking. отказ от бесперспективных подходов. — subgoal setting. разделение сложных задач на более простые подзадачи. — backward chaining. подход от конца к началу: попытка понять по ответу, какие действия подходят к нему. эти паттерны вполне соответствуют человеческой логике. авторы предположили, что обучающая выборка qwen содержит в том или ином виде четыре ключевых паттерна, а llama — нет. чтобы научить llama вышеописанным паттернам, авторы сгенерировали мощной проприетарной моделью (claude 3.5 sonnet) небольшой датасет с этими паттернами. оказалось, что дообучение на небольшом количестве таких примеров приводит к существенному приросту качества работы llama для задачи countdown: оно сравнялось с qwen. примечательно, что итоговое качество не снижают даже примеры с неправильными ответами в обучающей выборке. это говорит о том, что демонстрация когнитивного поведения важнее правильных ответов. разбор подготовил ❣ денис кузнеделев душный nlp cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars сегодня разберём статью от группы исследователей из стэнфордского университета про когнитивное поведение. авторы выясняют, при каких условиях модель становится self-improving reasoner: то есть, может учиться находить правильное решение без специальной разметки. test-time compute scaling — довольно мощная парадигма для задач, которые требуют рассуждения. для deepseek-r1-zero было показано: обучаясь решать задачи по математике и программированию, модель самостоятельно учится генерировать цепочки рассуждений. в этой статье авторы применяют тот же принцип к моделям qwen-2.5-3b и llama-3.2-3b с одинаковым сетапом обучения (rl+grpo) для задачи countdown. countdown — это когда из нескольких чисел с помощью стандартных арифметических операций (сложение, вычитание, умножение и деление) нужно получить целевое число. как видно из графиков, модель qwen довольно быстро достигла неплохого качества, в то время как llama сходится медленнее и работает хуже. проанализировав результаты, авторы обнаружили четыре главных когнитивных паттерна для решения логических задач: — verification. проверка, верно ли логически выдвинутое рассуждение. — backtracking. отказ от бесперспективных подходов. — subgoal setting. разделение сложных задач на более простые подзадачи. — backward chaining. подход от конца к началу: попытка понять по ответу, какие действия подходят к нему. эти паттерны вполне соответствуют человеческой логике. авторы предположили, что обучающая выборка qwen содержит в том или ином виде четыре ключевых паттерна, а llama — нет. чтобы научить llama вышеописанным паттернам, авторы сгенерировали мощной проприетарной моделью (claude 3.5 sonnet) небольшой датасет с этими паттернами. оказалось, что дообучение на небольшом количестве таких примеров приводит к существенному приросту качества работы llama для задачи countdown: оно сравнялось с qwen. примечательно, что итоговое качество не снижают даже примеры с неправильными ответами в обучающей выборке. это говорит о том, что демонстрация когнитивного поведения важнее правильных ответов. разбор подготовил ❣ денис кузнеделев душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-06-06T13:46:28+00:00" href="./posts/131.html">2025-06-06 13:46 UTC</a></div>
      </div>
      <div class="post-body"><strong>Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs</strong><br><br>Сегодня разберём <a href="https://arxiv.org/abs/2503.01307" rel="nofollow noopener noreferrer">статью</a> от группы исследователей из Стэнфордского университета про когнитивное поведение. Авторы выясняют, при каких условиях модель становится self-improving reasoner: то есть, может учиться находить правильное решение без специальной разметки.<br><br>Test-time compute scaling — довольно мощная парадигма для задач, которые требуют рассуждения. Для <a href="https://arxiv.org/abs/2501.12948" rel="nofollow noopener noreferrer">DeepSeek-R1-Zero</a> было показано: обучаясь решать задачи по математике и программированию, модель самостоятельно учится генерировать цепочки рассуждений. В этой статье авторы применяют тот же принцип к моделям Qwen-2.5-3B и Llama-3.2-3B с одинаковым сетапом обучения (RL+GRPO) для задачи Countdown.<br><br>Countdown — это когда из нескольких чисел с помощью стандартных арифметических операций (сложение, вычитание, умножение и деление) нужно получить целевое число. Как видно из графиков, модель Qwen довольно быстро достигла неплохого качества, в то время как Llama сходится медленнее и работает хуже. Проанализировав результаты, авторы обнаружили четыре главных когнитивных паттерна для решения логических задач:<br><br>— <strong>Verification. </strong>Проверка, верно ли логически выдвинутое рассуждение. <br>— <strong>Backtracking.</strong> Отказ от бесперспективных подходов.<br>— <strong>Subgoal setting.</strong> Разделение сложных задач на более простые подзадачи.<br>— <strong>Backward chaining.</strong> Подход от конца к началу: попытка понять по ответу, какие действия подходят к нему.<br><br>Эти паттерны вполне соответствуют человеческой логике. Авторы предположили, что обучающая выборка Qwen содержит в том или ином виде четыре ключевых паттерна, а Llama — нет. Чтобы научить Llama вышеописанным паттернам, авторы сгенерировали мощной проприетарной моделью (Claude 3.5 Sonnet) небольшой датасет с этими паттернами.  <br><br>Оказалось, что дообучение на небольшом количестве таких примеров приводит к существенному приросту качества работы Llama для задачи Countdown: оно сравнялось с Qwen. <br><br>Примечательно, что итоговое качество не снижают даже примеры с неправильными ответами в обучающей выборке. Это говорит о том, что демонстрация когнитивного поведения важнее правильных ответов. <br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Денис Кузнеделев</em><br><br><a href="https://t.me/+o-VVOVhZdiI0N2Zi" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/131_480.webp" srcset="../assets/media/thumbs/131_480.webp 480w, ../assets/media/131.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="131" data-image-index="0" /></div></div>
      <div class="actions">
        <span>3 648 просмотров · 27 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/131" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/131.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="127" data-search="dmpo — модификация dpo сегодняшняя статья — о direct multi-turn preference optimization. это модификация dpo, которая позволяет получить лучшие результаты. но не всё так просто. традиционный dpo заточен на работу с парами «префикс и суффикс» — то есть запрос пользователя и ответ. авторы статьи задались целью распространить его на длинные цепочки. однако в публикации работают не диалогами, а с окружением из трёх датасетов: alfworld, webshop и scienceworld. скажем, в scienceworld агенту даётся задание — например, выяснить, обладает ли металлическая вилка электропроводностью — и текстовое представление нескольких «комнат» с разными объектами. с ними можно выполнять некоторые действия, чтобы достигнуть поставленной цели. в scienceworld задачи чуть сложнее, чем, например, в alfworld, где может потребоваться, к примеру, просто убрать посуду в шкаф. авторы статьи отмечают, что при обучении на цепочках использование стандартной формулы для kl-дивергенции приводит к большой накопительной ошибке. поэтому они обращаются к state-action occupancy measure (saom). суть этого метода заключается в модификации обычного rl-лосса (изображение 1), введении дисконта так, чтобы у более ранних шагов был больший вес. формула saom добавляется в формулу dpo вместо kl-дивергенции, после чего авторы добавляют нормализацию на длины траекторий и получают ещё одну формулу (изображение 2). согласно ей, каждый следующий шаг в диалоге меньше влияет на лосс. такова основная идея статьи, но самое интересное — это эксперименты. авторы проводили sft-модели на датасетах, о которых сказано выше, а затем проводили тесты на других кусках этих же датасетов и определяли, успешно ли справилась модель. далее есть два сетапа. первый — noisy (rq1) — включает все безуспешные траектории в качестве негативов. как позитивные примеры используются не успехи модели, а экспертные траектории из датасетов. для clean-сетапа (rq2) отфильтровывают noisy-траектории (как именно, не сообщают) и выбирают высококачественные в качестве проигравших. dmpo в rq2 даёт весьма ощутимый прирост относительно sft (изображение 3), а в rq1 различия не столь ощутимы. хотя метод авторов статьи всё ещё побеждает. при этом dmpo, по их словам, «не убивает» длину цепочек. разбор подготовил ❣ сергей дуликов душный nlp dmpo — модификация dpo сегодняшняя статья — о direct multi-turn preference optimization. это модификация dpo, которая позволяет получить лучшие результаты. но не всё так просто. традиционный dpo заточен на работу с парами «префикс и суффикс» — то есть запрос пользователя и ответ. авторы статьи задались целью распространить его на длинные цепочки. однако в публикации работают не диалогами, а с окружением из трёх датасетов: alfworld, webshop и scienceworld. скажем, в scienceworld агенту даётся задание — например, выяснить, обладает ли металлическая вилка электропроводностью — и текстовое представление нескольких «комнат» с разными объектами. с ними можно выполнять некоторые действия, чтобы достигнуть поставленной цели. в scienceworld задачи чуть сложнее, чем, например, в alfworld, где может потребоваться, к примеру, просто убрать посуду в шкаф. авторы статьи отмечают, что при обучении на цепочках использование стандартной формулы для kl-дивергенции приводит к большой накопительной ошибке. поэтому они обращаются к state-action occupancy measure (saom). суть этого метода заключается в модификации обычного rl-лосса ( изображение 1 ), введении дисконта так, чтобы у более ранних шагов был больший вес. формула saom добавляется в формулу dpo вместо kl-дивергенции, после чего авторы добавляют нормализацию на длины траекторий и получают ещё одну формулу ( изображение 2 ). согласно ей, каждый следующий шаг в диалоге меньше влияет на лосс. такова основная идея статьи, но самое интересное — это эксперименты. авторы проводили sft-модели на датасетах, о которых сказано выше, а затем проводили тесты на других кусках этих же датасетов и определяли, успешно ли справилась модель. далее есть два сетапа. первый — noisy (rq1) — включает все безуспешные траектории в качестве негативов. как позитивные примеры используются не успехи модели, а экспертные траектории из датасетов. для clean-сетапа (rq2) отфильтровывают noisy-траектории (как именно, не сообщают) и выбирают высококачественные в качестве проигравших. dmpo в rq2 даёт весьма ощутимый прирост относительно sft ( изображение 3 ), а в rq1 различия не столь ощутимы. хотя метод авторов статьи всё ещё побеждает. при этом dmpo, по их словам, «не убивает» длину цепочек. разбор подготовил ❣ сергей дуликов душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-05-30T09:30:47+00:00" href="./posts/127.html">2025-05-30 09:30 UTC</a></div>
      </div>
      <div class="post-body"><strong>DMPO — модификация DPO</strong><br><br>Сегодняшняя <a href="https://arxiv.org/abs/2406.14868" rel="nofollow noopener noreferrer">статья</a> — о Direct Multi-Turn Preference Optimization. Это модификация DPO, которая позволяет получить лучшие результаты. Но не всё так просто.    <br><br>Традиционный DPO заточен на работу с парами «префикс и суффикс» — то есть запрос пользователя и ответ. Авторы статьи задались целью распространить его на длинные цепочки. Однако в публикации работают не диалогами, а с окружением из трёх датасетов: ALFWorld, WebShop и ScienceWorld. <br><br>Скажем, в ScienceWorld агенту даётся задание — например, выяснить, обладает ли металлическая вилка электропроводностью — и текстовое представление нескольких «комнат» с разными объектами. С ними можно выполнять некоторые действия, чтобы достигнуть поставленной цели. В ScienceWorld задачи чуть сложнее, чем, например, в ALFWorld, где может потребоваться, к примеру, просто убрать посуду в шкаф. <br><br>Авторы статьи отмечают, что при обучении на цепочках использование стандартной формулы для KL-дивергенции приводит к большой накопительной ошибке. Поэтому они обращаются к State-Action Occupancy Measure (SAOM). Суть этого метода заключается в модификации обычного RL-лосса (<em>изображение 1</em>), введении дисконта так, чтобы у более ранних шагов был больший вес. <br><br>Формула SAOM добавляется в формулу DPO вместо KL-дивергенции, после чего авторы добавляют нормализацию на длины траекторий и получают ещё одну формулу (<em>изображение 2</em>). Согласно ей, каждый следующий шаг в диалоге меньше влияет на лосс. <br><br>Такова основная идея статьи, но самое интересное — это эксперименты. Авторы проводили SFT-модели на датасетах, о которых сказано выше, а затем проводили тесты на других кусках этих же датасетов и определяли, успешно ли справилась модель. <br><br>Далее есть два сетапа. Первый — Noisy (RQ1) — включает все безуспешные траектории в качестве негативов. Как позитивные примеры используются не успехи модели, а экспертные траектории из датасетов. Для clean-сетапа (RQ2) отфильтровывают noisy-траектории (как именно, не сообщают) и выбирают высококачественные в качестве проигравших.  <br><br>DMPO в RQ2 даёт весьма ощутимый прирост относительно SFT (<em>изображение 3</em>), а в RQ1 различия не столь ощутимы. Хотя метод авторов статьи всё ещё побеждает. При этом DMPO, по их словам, «не убивает» длину цепочек. <br><em><br>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Сергей Дуликов</em><br><br><a href="https://t.me/+WUeVr0juqqIyN2Fi" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/127_480.webp" srcset="../assets/media/thumbs/127_480.webp 480w, ../assets/media/127.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="127" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/128_480.webp" srcset="../assets/media/thumbs/128_480.webp 480w, ../assets/media/128.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="127" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/129_480.webp" srcset="../assets/media/thumbs/129_480.webp 480w, ../assets/media/129.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="127" data-image-index="2" /></div></div>
      <div class="actions">
        <span>3 462 просмотров · 21 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/127" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/127.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="126" data-search="как улучшили eagle-3 сегодняшняя статья — о eagle-3. это новая версия популярного метода спекулятивного декодинга. расскажем, как её улучшили по сравнению с прошлыми итерациями. спекулятивный декодинг — это способ ускорения инференса, предполагающий использование черновой (draft) модели, которая предлагает варианты продолжения генераций. основная модель проверяет эти варианты, выбирая один с помощью процедуры верификации. качество генераций при этом не страдает, ведь окончательное решение о принятии тех или иных токенов лежит на основной модели. один из самых известных методов спекулятивного декодинга — extrapolation algorithm for greater language-model efficiency (eagle). в его рамках модель принимает не только прошлые токены, но и их feature-вектора. это позволяет увеличить точность угадывания токенов. обновлённая версия eagle — eagle-3 — призвана сделать угадывание ещё более точным. для этого можно просто налить больше данных в обучение eagle-модели. однако, как показала практика, такой подход работает не слишком хорошо. авторы метода посчитали, что здесь мешает feature loss, на который учится eagle. выход — избавиться от feature loss и учить только на kl-лосс между предсказаниями eagle-головы и основной модели. проверка этой гипотезы показала, что без feature loss точность угадывания первого токена действительно увеличивается при добавлении новых данных. однако она падает для следующих токенов. всё из-за того, что теряется способность предсказывать в глубину. решение: во время обучения делать не одну, а сразу несколько итераций eagle-головы, осуществляя предсказание в глубину. авторы сделали ещё одно улучшение. в прошлых версиях метода в eagle-модель подавали хиддены с последнего слоя таргет-модели, а также эмбеддинги токенов, отсэмплированных из них. исследователи предположили, что в хидденах недостаточно информации, чтобы эффективно предсказывать токены. вероятно, больше данных содержится в хидденах с промежуточных трансформерных слоёв. в eagle-3 авторы конкатенируют хиддены с трёх decoder-слоёв — с третего от начала, третьего от конца и слоя в середине между ними — и уже их передают на вход eagle-модели. суммируя, авторы eagle-3: — убрали feature loss; — добавили несколько шагов на обучении и увеличили объём данных; — решили отправлять в eagle-модель хиддены с нескольких слоёв. по сравнению с инференсом без использования eagle, всё это позволило получить прирост в скорости в 6,5 раза — и без потери качества. число токенов за одну итерацию увеличилось на 50% по сравнению с eagle-2: с 4,05 до 6,13. разбор подготовил ❣ алексей гликин душный nlp как улучшили eagle-3 сегодняшняя статья — о eagle-3. это новая версия популярного метода спекулятивного декодинга. расскажем, как её улучшили по сравнению с прошлыми итерациями. спекулятивный декодинг — это способ ускорения инференса, предполагающий использование черновой (draft) модели, которая предлагает варианты продолжения генераций. основная модель проверяет эти варианты, выбирая один с помощью процедуры верификации. качество генераций при этом не страдает, ведь окончательное решение о принятии тех или иных токенов лежит на основной модели. один из самых известных методов спекулятивного декодинга — extrapolation algorithm for greater language-model efficiency (eagle). в его рамках модель принимает не только прошлые токены, но и их feature-вектора. это позволяет увеличить точность угадывания токенов. обновлённая версия eagle — eagle-3 — призвана сделать угадывание ещё более точным. для этого можно просто налить больше данных в обучение eagle-модели. однако, как показала практика, такой подход работает не слишком хорошо. авторы метода посчитали, что здесь мешает feature loss, на который учится eagle. выход — избавиться от feature loss и учить только на kl-лосс между предсказаниями eagle-головы и основной модели. проверка этой гипотезы показала, что без feature loss точность угадывания первого токена действительно увеличивается при добавлении новых данных. однако она падает для следующих токенов. всё из-за того, что теряется способность предсказывать в глубину. решение: во время обучения делать не одну, а сразу несколько итераций eagle-головы, осуществляя предсказание в глубину. авторы сделали ещё одно улучшение. в прошлых версиях метода в eagle-модель подавали хиддены с последнего слоя таргет-модели, а также эмбеддинги токенов, отсэмплированных из них. исследователи предположили, что в хидденах недостаточно информации, чтобы эффективно предсказывать токены. вероятно, больше данных содержится в хидденах с промежуточных трансформерных слоёв. в eagle-3 авторы конкатенируют хиддены с трёх decoder-слоёв — с третего от начала, третьего от конца и слоя в середине между ними — и уже их передают на вход eagle-модели. суммируя, авторы eagle-3: — убрали feature loss; — добавили несколько шагов на обучении и увеличили объём данных; — решили отправлять в eagle-модель хиддены с нескольких слоёв. по сравнению с инференсом без использования eagle, всё это позволило получить прирост в скорости в 6,5 раза — и без потери качества. число токенов за одну итерацию увеличилось на 50% по сравнению с eagle-2: с 4,05 до 6,13. разбор подготовил ❣ алексей гликин душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-05-23T09:17:00+00:00" href="./posts/126.html">2025-05-23 09:17 UTC</a></div>
      </div>
      <div class="post-body"><strong>Как улучшили EAGLE-3</strong><br><br>Сегодняшняя <a href="https://arxiv.org/abs/2503.01840" rel="nofollow noopener noreferrer">статья</a> — о EAGLE-3. Это новая версия популярного метода спекулятивного декодинга. Расскажем, как её улучшили по сравнению с прошлыми итерациями. <br><br>Спекулятивный декодинг — это способ ускорения инференса, предполагающий использование черновой (draft) модели, которая предлагает варианты продолжения генераций. Основная модель проверяет эти варианты, выбирая один с помощью процедуры верификации. Качество генераций при этом не страдает, ведь окончательное решение о принятии тех или иных токенов лежит на основной модели.<br><br>Один из самых известных методов спекулятивного декодинга — Extrapolation Algorithm for Greater Language-model Efficiency (EAGLE). В его рамках модель принимает не только прошлые токены, но и их feature-вектора. Это позволяет увеличить точность угадывания токенов. <br><br>Обновлённая версия EAGLE — EAGLE-3 — призвана сделать угадывание ещё более точным. Для этого можно просто налить больше данных в обучение EAGLE-модели. Однако, как показала практика, такой подход работает не слишком хорошо. Авторы метода посчитали, что здесь мешает feature loss, на который учится EAGLE. Выход — избавиться от feature loss и учить только на KL-лосс между предсказаниями EAGLE-головы и основной модели. <br><br>Проверка этой гипотезы показала, что без feature loss точность угадывания первого токена действительно увеличивается при добавлении новых данных. Однако она падает для следующих токенов. Всё из-за того, что теряется способность предсказывать в глубину. Решение: во время обучения делать не одну, а сразу несколько итераций EAGLE-головы, осуществляя предсказание в глубину.<br><br>Авторы сделали ещё одно улучшение. В прошлых версиях метода в EAGLE-модель подавали хиддены с последнего слоя таргет-модели, а также эмбеддинги токенов, отсэмплированных из них. Исследователи предположили, что в хидденах недостаточно информации, чтобы эффективно предсказывать токены. Вероятно, больше данных содержится в хидденах с промежуточных трансформерных слоёв. В EAGLE-3 авторы конкатенируют хиддены с трёх decoder-слоёв — с третего от начала, третьего от конца и слоя в середине между ними — и уже их передают на вход EAGLE-модели. <br><br>Суммируя, авторы EAGLE-3:<br><br>— убрали feature loss;<br>— добавили несколько шагов на обучении и увеличили объём данных;<br>— решили отправлять в EAGLE-модель хиддены с нескольких слоёв.<br><br>По сравнению с инференсом без использования EAGLE, всё это позволило получить прирост в скорости в 6,5 раза — и без потери качества. Число токенов за одну итерацию увеличилось на 50% по сравнению с EAGLE-2: с 4,05 до 6,13. <br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Алексей Гликин</em><br><br><a href="https://t.me/+iHXjunqCCbIxYWYy" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/126_480.webp" srcset="../assets/media/thumbs/126_480.webp 480w, ../assets/media/126.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="126" data-image-index="0" /></div></div>
      <div class="actions">
        <span>3 842 просмотров · 21 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/126" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/126.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="124" data-search="технический отчёт command a компания cohere выпустила технический отчёт своей опенсорс-модели command a. рассказываем главное, что мы из него узнали. command a — это dense-модель на 111 миллиардов параметров. всего она поддерживает 23 языка, а на английском по качеству не уступает deepseek-v3. создатели заявляют, что скорость инференса — до 156 токенов в секунду на всего одной h100 (вероятно, с квантизацией модели в тип пониженной точности). это в 1,75 раза выше, чем у gpt-4o и в 2,4 раза выше, чем у deepseek-v3. в архитектуре модели, помимо gqa и swiglu, применяли чередующиеся аттеншен-слои (interleaved attention layers). чередовали три блока со скользящим окном и rope и один фулл-аттеншен-блок без позиционных эмбеддингов (nope). благодаря этому аттеншен становится быстрее и, вероятно, лучше работает с длинным контекстом. похожая архитектура применяется в llama 4. авторы использовали µp и µtransfer для настройки гиперпараметров — параметры подбирались на маленьких моделях, а затем без дополнительного обучения (zero-shot) переносились в большую. также прибегали к data parallel, fully sharded data parallel, sequence parallel и tensor parallel. веса модели перед вычислениями преобразовывали в fp8. чувствительные операции, такие как экспоненты, софтмакс, нормализация слоёв и выходные эмбеддинги, оставляли в fp32, а вычисления аттеншена выполняли в bf16. при обучении полностью в fp8 авторы не замечали нестабильности, но обнаруживали деградацию на бенчмарках. чтобы справиться с ней, первые шаги обучали в bf16, а уже после этого переходили на fp8. обучение происходило с постепенным расширением контекста: первые 30 тысяч шагов с контекстным окном 8 тысяч токенов, далее — 32 тысячи токенов, потом — 128 тысяч и 256 тысяч. многие данные для длинного контекста были синтетическими. посттрейн проходил в шесть стадий. на каждой обучали несколько независимых моделей, которые после этого сливались в одну с помощью техники мержинга. авторы отмечают, что такой подход позволил работать над разными доменами (код, математика, безопасность и так далее) отдельным командам и использовать наиболее подходящий задаче набор rl-алгоритмов, наград и данных. — instruct-модель. обучали начальную instruct-модель с помощью supervised learning на основе базовой модели. — sft-экспертные модели. обучали шесть sft-экспертов (для кода, математики, длинного контекста и так далее) поверх чекпойнта instruct-модели с использованием специализированных наборов данных, чтобы максимально повысить производительность в конкретных задачах. — sft soup-модель. объединяли шесть экспертных моделей в одну soup-модель, чтобы получить единую агрегированную sft-модель. — rl-экспертные модели. обучали шесть rl-экспертов поверх чекпойнта sft soup-модели. — rl soup-модель. объединяли шесть rl-экспертов в rl soup-модель, чтобы получить единую агрегированную rl-модель. — polished-модель. чередовали методы best-of-n, оффлайн- и онлайн-rl-алгоритмы. разбор подготовил ❣ владислав савинов душный nlp технический отчёт command a компания cohere выпустила технический отчёт своей опенсорс-модели command a. рассказываем главное, что мы из него узнали. command a — это dense-модель на 111 миллиардов параметров. всего она поддерживает 23 языка, а на английском по качеству не уступает deepseek-v3. создатели заявляют, что скорость инференса — до 156 токенов в секунду на всего одной h100 (вероятно, с квантизацией модели в тип пониженной точности). это в 1,75 раза выше, чем у gpt-4o и в 2,4 раза выше, чем у deepseek-v3. в архитектуре модели, помимо gqa и swiglu, применяли чередующиеся аттеншен-слои (interleaved attention layers). чередовали три блока со скользящим окном и rope и один фулл-аттеншен-блок без позиционных эмбеддингов ( nope ). благодаря этому аттеншен становится быстрее и, вероятно, лучше работает с длинным контекстом. похожая архитектура применяется в llama 4. авторы использовали µp и µtransfer для настройки гиперпараметров — параметры подбирались на маленьких моделях, а затем без дополнительного обучения (zero-shot) переносились в большую. также прибегали к data parallel, fully sharded data parallel, sequence parallel и tensor parallel. веса модели перед вычислениями преобразовывали в fp8. чувствительные операции, такие как экспоненты, софтмакс, нормализация слоёв и выходные эмбеддинги, оставляли в fp32, а вычисления аттеншена выполняли в bf16. при обучении полностью в fp8 авторы не замечали нестабильности, но обнаруживали деградацию на бенчмарках. чтобы справиться с ней, первые шаги обучали в bf16, а уже после этого переходили на fp8. обучение происходило с постепенным расширением контекста: первые 30 тысяч шагов с контекстным окном 8 тысяч токенов, далее — 32 тысячи токенов, потом — 128 тысяч и 256 тысяч. многие данные для длинного контекста были синтетическими. посттрейн проходил в шесть стадий. на каждой обучали несколько независимых моделей, которые после этого сливались в одну с помощью техники мержинга. авторы отмечают, что такой подход позволил работать над разными доменами (код, математика, безопасность и так далее) отдельным командам и использовать наиболее подходящий задаче набор rl-алгоритмов, наград и данных. — instruct-модель. обучали начальную instruct-модель с помощью supervised learning на основе базовой модели. — sft-экспертные модели. обучали шесть sft-экспертов (для кода, математики, длинного контекста и так далее) поверх чекпойнта instruct-модели с использованием специализированных наборов данных, чтобы максимально повысить производительность в конкретных задачах. — sft soup-модель. объединяли шесть экспертных моделей в одну soup-модель, чтобы получить единую агрегированную sft-модель. — rl-экспертные модели. обучали шесть rl-экспертов поверх чекпойнта sft soup-модели. — rl soup-модель. объединяли шесть rl-экспертов в rl soup-модель, чтобы получить единую агрегированную rl-модель. — polished-модель. чередовали методы best-of-n, оффлайн- и онлайн-rl-алгоритмы. разбор подготовил ❣ владислав савинов душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-05-05T08:30:25+00:00" href="./posts/124.html">2025-05-05 08:30 UTC</a></div>
      </div>
      <div class="post-body"><strong>Технический отчёт Command A</strong><br><br>Компания Cohere выпустила <a href="https://cohere.com/research/papers/command-a-an-enterprise-ready-family-of-large-language-models-2025-03-27" rel="nofollow noopener noreferrer">технический отчёт</a> своей опенсорс-модели Command A. Рассказываем главное, что мы из него узнали. <br><br>Command A — это dense-модель на 111 миллиардов параметров. Всего она поддерживает 23 языка, а на английском по качеству не уступает DeepSeek-V3. Создатели заявляют, что скорость инференса — до 156 токенов в секунду на всего одной H100 (вероятно, с квантизацией модели в тип пониженной точности). Это в 1,75 раза выше, чем у GPT-4o и в 2,4 раза выше, чем у DeepSeek-V3. <br><br>В архитектуре модели, помимо GQA и SwiGLU, применяли чередующиеся аттеншен-слои (Interleaved attention layers). Чередовали три блока со скользящим окном и RoPE и один фулл-аттеншен-блок без позиционных эмбеддингов (<a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/4e85362c02172c0c6567ce593122d31c-Paper-Conference.pdf" rel="nofollow noopener noreferrer">NoPE</a>). Благодаря этому аттеншен становится быстрее и, вероятно, лучше работает с длинным контекстом. Похожая архитектура применяется в Llama 4. <br><br>Авторы использовали µP и µTransfer для настройки гиперпараметров — параметры подбирались на маленьких моделях, а затем без дополнительного обучения (zero-shot) переносились в большую. Также прибегали к Data Parallel, Fully Sharded Data Parallel, Sequence Parallel и Tensor Parallel. <br><br>Веса модели перед вычислениями преобразовывали в FP8. Чувствительные операции, такие как экспоненты, софтмакс, нормализация слоёв и выходные эмбеддинги, оставляли в FP32, а вычисления аттеншена выполняли в BF16. При обучении полностью в FP8 авторы не замечали нестабильности, но обнаруживали деградацию на бенчмарках. Чтобы справиться с ней, первые шаги обучали в BF16, а уже после этого переходили на FP8. <br><br>Обучение происходило с постепенным расширением контекста: первые 30 тысяч шагов с контекстным окном 8 тысяч токенов, далее — 32 тысячи токенов, потом — 128 тысяч и 256 тысяч. Многие данные для длинного контекста были синтетическими. <br><br>Посттрейн проходил в шесть стадий. На каждой обучали несколько независимых моделей, которые после этого сливались в одну с помощью техники мержинга. Авторы отмечают, что такой подход позволил работать над разными доменами (код, математика, безопасность и так далее) отдельным командам и использовать наиболее подходящий задаче набор RL-алгоритмов, наград и данных.<br><br>—<strong> Instruct-модель.</strong> Обучали начальную Instruct-модель с помощью supervised learning на основе базовой модели.<br>— <strong>SFT-экспертные модели.</strong> Обучали шесть SFT-экспертов (для кода, математики, длинного контекста и так далее) поверх чекпойнта Instruct-модели с использованием специализированных наборов данных, чтобы максимально повысить производительность в конкретных задачах.<br>— <strong>SFT Soup-модель.</strong> Объединяли шесть экспертных моделей в одну soup-модель, чтобы получить единую агрегированную SFT-модель.<br>— <strong>RL-экспертные модели.</strong> Обучали шесть RL-экспертов поверх чекпойнта SFT soup-модели.<br>— <strong>RL Soup-модель. </strong>Объединяли шесть RL-экспертов в RL soup-модель, чтобы получить единую агрегированную RL-модель.<br>— <strong>Polished-модель.</strong> Чередовали методы best-of-N, оффлайн- и онлайн-RL-алгоритмы.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Владислав Савинов</em><br><br><a href="https://t.me/+gyXjn3gAZTBlOWNi" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/124_480.webp" srcset="../assets/media/thumbs/124_480.webp 480w, ../assets/media/124.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="124" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/125_480.webp" srcset="../assets/media/thumbs/125_480.webp 480w, ../assets/media/125.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="124" data-image-index="1" /></div></div>
      <div class="actions">
        <span>4 088 просмотров · 43 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/124" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/124.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="119" data-search="соскучились по постерам с iclr? их есть у нас! свежая подборка интересных статей, чтобы скрасить вечер понедельника. knowledge entropy decay during language model pretraining hinders new knowledge acquisition интересная статья о забывании фактов. известно, что факты хранятся в ff, но авторы посмотрели на динамику распределения весов в виде векторов, чтобы понять, почему плохо усваиваются знания после самого претрейна. оказывается, всё распределение лежит в маленьком проценте векторов, и они сильно портятся от дообучения. training language models to self-correct via reinforcement learning вместо промптинга, файнтюнинга и использования отдельной модели авторы пытаются встроить self-correction в модель. существующие решения либо требуют отдельных моделей для верификации, либо используют более крупные модели-учителя, либо страдают от проблем «смещения распределения» (модель исправляет чужие ошибки, но не свои) и «схлопывание поведения» (модель делает одинаковые попытки без реальной коррекции). авторы предлагают двустадийное решение задачи, где вторая попытка пытается исправлять ошибки первой. детали успеха: — на первом этапе создается хорошая инициализация для rl c принуждением первой попытки быть близкой к базовой модели (kl-регуляризация); — on-policy rl; — модифицированный реворд, сравнивающий прогресс между попытками. learning reward and policy jointly from demonstration and preference improves alignment статья о совместном обученим rl+sft+rm. всё в онлайне. реворд учится не только преференсам, но ещё и поощрять экспертные демонстрации из sft-датасета. лоссы довольно понятным образом можно вывести в цикле: — шаг обучения rm; — несколько шагов ppo. accelerating transformer inference and training with 2:4 activation sparsity авторы придумали как использовать sparse kernel в тренировке. заменяют swiglu на squared relu (утверждается, что без потери качества), и замечают, что после этого во втором матричном умножении появляется много нулей в активациях. используют 2:4 sparse kernels для того, чтобы ускорить это умножение (зануляя активации, которые ломают 2:4 картинку). на backward из-за того, что матрица транспонируется, приходится использовать пару трюков: — разбивать матрицу на две части — «очень плотные строки (5%)» и «разреженные строки (95%)» — и делать два отдельных гемма; — чтобы бороться с явлением «соседние токены часто либо одновременно нули, либо одновременно не нули» шафлят токены перед ffn, а потом шалят обратно; — используют row-wise-квантизацию; — получают x1.3 ускорение на ffn-блоках. в статье почему-то описывают только 1.5b-перплексию, но говорят, что на 7b и downstream-задачах вроде тоже работает неплохо. has my system prompt been used? large language model prompt membership inference инженеры amazon предлагают довольно простую процедуру расчёта стат.теста для проверки, использует ли llm новые вводные из системного промпта. тест основывается на средних значениях бертовых эмбеддингов того текста, который сгенерировала llm. по словам авторов, для статистической значимости даже на незначительных изменениях достаточно прогнать около 300 примеров для каждого промпта. *компания meta признана экстремистской организацией в россии. интересные постеры увидели ❣ екатерина редина, константин бабалян, павел темирчев, степан каргальцев, кирилл никоров #yaiclr душный nlp соскучились по постерам с iclr? их есть у нас! свежая подборка интересных статей, чтобы скрасить вечер понедельника. knowledge entropy decay during language model pretraining hinders new knowledge acquisition интересная статья о забывании фактов. известно, что факты хранятся в ff, но авторы посмотрели на динамику распределения весов в виде векторов, чтобы понять, почему плохо усваиваются знания после самого претрейна. оказывается, всё распределение лежит в маленьком проценте векторов, и они сильно портятся от дообучения. training language models to self-correct via reinforcement learning вместо промптинга, файнтюнинга и использования отдельной модели авторы пытаются встроить self-correction в модель. существующие решения либо требуют отдельных моделей для верификации, либо используют более крупные модели-учителя, либо страдают от проблем «смещения распределения» (модель исправляет чужие ошибки, но не свои) и «схлопывание поведения» (модель делает одинаковые попытки без реальной коррекции). авторы предлагают двустадийное решение задачи, где вторая попытка пытается исправлять ошибки первой. детали успеха: — на первом этапе создается хорошая инициализация для rl c принуждением первой попытки быть близкой к базовой модели (kl-регуляризация); — on-policy rl; — модифицированный реворд, сравнивающий прогресс между попытками. learning reward and policy jointly from demonstration and preference improves alignment статья о совместном обученим rl+sft+rm. всё в онлайне. реворд учится не только преференсам, но ещё и поощрять экспертные демонстрации из sft-датасета. лоссы довольно понятным образом можно вывести в цикле: — шаг обучения rm; — несколько шагов ppo. accelerating transformer inference and training with 2:4 activation sparsity авторы придумали как использовать sparse kernel в тренировке. заменяют swiglu на squared relu (утверждается, что без потери качества), и замечают, что после этого во втором матричном умножении появляется много нулей в активациях. используют 2:4 sparse kernels для того, чтобы ускорить это умножение (зануляя активации, которые ломают 2:4 картинку). на backward из-за того, что матрица транспонируется, приходится использовать пару трюков: — разбивать матрицу на две части — «очень плотные строки (5%)» и «разреженные строки (95%)» — и делать два отдельных гемма; — чтобы бороться с явлением «соседние токены часто либо одновременно нули, либо одновременно не нули» шафлят токены перед ffn, а потом шалят обратно; — используют row-wise-квантизацию; — получают x1.3 ускорение на ffn-блоках. в статье почему-то описывают только 1.5b-перплексию, но говорят, что на 7b и downstream-задачах вроде тоже работает неплохо. has my system prompt been used? large language model prompt membership inference инженеры amazon предлагают довольно простую процедуру расчёта стат.теста для проверки, использует ли llm новые вводные из системного промпта. тест основывается на средних значениях бертовых эмбеддингов того текста, который сгенерировала llm. по словам авторов, для статистической значимости даже на незначительных изменениях достаточно прогнать около 300 примеров для каждого промпта. *компания meta признана экстремистской организацией в россии. интересные постеры увидели ❣ екатерина редина, константин бабалян, павел темирчев, степан каргальцев, кирилл никоров #yaiclr душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-28T14:03:27+00:00" href="./posts/119.html">2025-04-28 14:03 UTC</a></div>
      </div>
      <div class="post-body"><strong>Соскучились по постерам с ICLR? Их есть у нас!</strong><br><br>Свежая подборка интересных статей, чтобы скрасить вечер понедельника.<br><br><a href="https://arxiv.org/abs/2410.01380" rel="nofollow noopener noreferrer"><strong>Knowledge Entropy Decay during Language Model Pretraining Hinders New Knowledge Acquisition</strong></a><br><br>Интересная статья о забывании фактов. Известно, что факты хранятся в FF, но авторы посмотрели на динамику распределения весов в виде векторов, чтобы понять, почему плохо усваиваются знания после самого претрейна. Оказывается, всё распределение лежит в маленьком проценте векторов, и они сильно портятся от дообучения.<br><br><a href="https://arxiv.org/abs/2409.12917" rel="nofollow noopener noreferrer"><strong>Training Language Models to Self-Correct via Reinforcement Learning</strong></a><br><br>Вместо промптинга, файнтюнинга и использования отдельной модели авторы пытаются встроить self-correction в модель. Существующие решения либо требуют отдельных моделей для верификации, либо используют более крупные модели-учителя, либо страдают от проблем «смещения распределения» (модель исправляет чужие ошибки, но не свои) и «схлопывание поведения» (модель делает одинаковые попытки без реальной коррекции).<br><br>Авторы предлагают двустадийное решение задачи, где вторая попытка пытается исправлять ошибки первой.<br><br>Детали успеха:<br><br> — на первом этапе создается хорошая инициализация для RL c принуждением первой попытки быть близкой к базовой модели (KL-регуляризация);<br><br>— on-policy RL;<br><br>— модифицированный реворд, сравнивающий прогресс между попытками.<br><br><a href="https://arxiv.org/abs/2406.06874" rel="nofollow noopener noreferrer"><strong>Learning Reward and Policy Jointly from Demonstration and Preference Improves Alignment</strong></a><br><br>Статья о совместном обученим RL+SFT+RM. Всё в онлайне. Реворд учится не только преференсам, но ещё и поощрять экспертные демонстрации из SFT-датасета.<br>Лоссы довольно понятным образом можно вывести <br><br>В цикле:<br><br>— шаг обучения RM;<br>— несколько шагов PPO.<br><br><a href="https://arxiv.org/abs/2503.16672" rel="nofollow noopener noreferrer"><strong>Accelerating Transformer Inference and Training with 2:4 Activation Sparsity</strong></a><br><br>Авторы придумали как использовать sparse kernel в тренировке. Заменяют SwiGLU на Squared ReLU (утверждается, что без потери качества), и замечают, что после этого во втором матричном умножении появляется много нулей в активациях. Используют 2:4 sparse kernels для того, чтобы ускорить это умножение (зануляя активации, которые ломают 2:4 картинку).<br><br>На backward из-за того, что матрица транспонируется, приходится использовать пару трюков:<br><br> — разбивать матрицу на две части — «очень плотные строки (5%)» и «разреженные строки (95%)» — и делать два отдельных гемма; <br>— чтобы бороться с явлением «соседние токены часто либо одновременно нули, либо одновременно не нули» шафлят токены перед FFN, а потом шалят обратно;<br>— используют row-wise-квантизацию;<br>— получают x1.3 ускорение на FFN-блоках.<br><br>В статье почему-то описывают только 1.5B-перплексию, но говорят, что на 7B и downstream-задачах вроде тоже работает неплохо.<br><br><a href="https://arxiv.org/abs/2502.09974" rel="nofollow noopener noreferrer"><strong>Has My System Prompt Been Used? Large Language Model Prompt Membership Inference</strong></a><br><br>Инженеры Amazon предлагают довольно простую процедуру расчёта стат.теста для проверки, использует ли LLM новые вводные из системного промпта. Тест основывается на средних значениях бертовых эмбеддингов того текста, который сгенерировала LLM. По словам авторов, для статистической значимости даже на незначительных изменениях достаточно прогнать около 300 примеров для каждого промпта.<br><br><em>*Компания Meta признана экстремистской организацией в России.<br><br>Интересные постеры увидели </em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji><em> Екатерина Редина, Константин Бабалян, Павел Темирчев, Степан Каргальцев, Кирилл Никоров</em><br><br>#YaICLR<br><br><a href="https://t.me/+koZSGGNHwAk5M2E6" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/119_480.webp" srcset="../assets/media/thumbs/119_480.webp 480w, ../assets/media/119.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="119" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/120_480.webp" srcset="../assets/media/thumbs/120_480.webp 480w, ../assets/media/120.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="119" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/121_480.webp" srcset="../assets/media/thumbs/121_480.webp 480w, ../assets/media/121.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="119" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/122_480.webp" srcset="../assets/media/thumbs/122_480.webp 480w, ../assets/media/122.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="119" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/123_480.webp" srcset="../assets/media/thumbs/123_480.webp 480w, ../assets/media/123.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="119" data-image-index="4" /></div></div>
      <div class="actions">
        <span>3 350 просмотров · 20 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/119" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/119.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="118" data-search="что мы делали в тени на iclr 2025 конференция завершается. почти все доклады прочитаны, почти все постеры отсмотрены, а участники и гости скоро вернутся домой. домой вернётся и команда ml-инженеров яндекса, которая последние дни рассказывала — и показывала, — что происходит на iclr. осталось совсем чуть-чуть, и самое время подвести итоги, собрав все наши материалы с мероприятия в одном посте. — первый день запомнился большой очередью на регистрацию, а также любопытными постерами о борьбе с эффектом «прайминга» и методе обучения реворд-модели. — второй день, в числе прочего, подарил нам статьи о системных промптах и подробный теоретический анализ sft и dpo. — третий день принёс статьи о q-learning и добавке к софтмаксу. и пусть конференция заканчивается, мы продолжаем работу. впереди — разборы самых ярких статей и впечатления из первых уст. оставайтесь с нами! а если вы хотите больше узнать о том, что происходило на iclr 2025, подписывайтесь на наши каналы-побратимы (все об ml): — ml underhood — speech info — рекомендательная — cv time #yaiclr душный nlp что мы делали в тени на iclr 2025 конференция завершается. почти все доклады прочитаны, почти все постеры отсмотрены, а участники и гости скоро вернутся домой. домой вернётся и команда ml-инженеров яндекса, которая последние дни рассказывала — и показывала, — что происходит на iclr. осталось совсем чуть-чуть, и самое время подвести итоги, собрав все наши материалы с мероприятия в одном посте. — первый день запомнился большой очередью на регистрацию, а также любопытными постерами о борьбе с эффектом «прайминга» и методе обучения реворд-модели. — второй день, в числе прочего, подарил нам статьи о системных промптах и подробный теоретический анализ sft и dpo. — третий день принёс статьи о q-learning и добавке к софтмаксу. и пусть конференция заканчивается, мы продолжаем работу. впереди — разборы самых ярких статей и впечатления из первых уст. оставайтесь с нами! а если вы хотите больше узнать о том, что происходило на iclr 2025, подписывайтесь на наши каналы-побратимы (все об ml): — ml underhood — speech info — рекомендательная — cv time #yaiclr душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-27T09:13:07+00:00" href="./posts/118.html">2025-04-27 09:13 UTC</a></div>
      </div>
      <div class="post-body"><strong>Что мы делали <del>в тени</del> на ICLR 2025 </strong><br><br>Конференция завершается. Почти все доклады прочитаны, почти все постеры отсмотрены, а участники и гости скоро вернутся домой. Домой вернётся и команда ML-инженеров Яндекса, которая последние дни рассказывала — и показывала, — что происходит на ICLR. Осталось совсем чуть-чуть, и самое время подвести итоги, собрав все наши материалы с мероприятия в одном посте. <br><br>— Первый день запомнился <a href="https://t.me/stuffyNLP/100" rel="nofollow noopener noreferrer">большой очередью</a> на регистрацию, а также <a href="https://t.me/stuffyNLP/99" rel="nofollow noopener noreferrer">любопытными постерами</a> о борьбе с эффектом «прайминга» и методе обучения реворд-модели.<br>— Второй день, в числе прочего, подарил нам <a href="https://t.me/stuffyNLP/101?single" rel="nofollow noopener noreferrer">статьи о системных промптах</a> и подробный <a href="https://t.me/stuffyNLP/106" rel="nofollow noopener noreferrer">теоретический анализ</a> SFT и DPO. <br>— Третий день принёс <a href="https://t.me/stuffyNLP/112" rel="nofollow noopener noreferrer">статьи о Q-Learning</a> и добавке к софтмаксу.<br><br>И пусть конференция заканчивается, мы продолжаем работу. Впереди — разборы самых ярких статей и впечатления из первых уст. Оставайтесь с нами! А если вы хотите больше узнать о том, что происходило на ICLR 2025, подписывайтесь на наши каналы-побратимы (все об ML): <br><br>— <a href="https://t.me/MLunderhood" rel="nofollow noopener noreferrer">ML Underhood</a><br>— <a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer">Speech Info</a><br>— <a href="https://t.me/RecSysChannel" rel="nofollow noopener noreferrer">Рекомендательная</a><br>— <a href="https://t.me/timeforcv" rel="nofollow noopener noreferrer">CV Time</a><br><br>#YaICLR<br><br><a href="https://t.me/+MsBd71oIIKJmZTY6" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/118_480.webp" srcset="../assets/media/thumbs/118_480.webp 480w, ../assets/media/118.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="118" data-image-index="0" /></div></div>
      <div class="actions">
        <span>3 346 просмотров · 29 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/118" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/118.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="112" data-search="свежая подборка постеров с iclr 2025 продолжаем рассказывать о самых ярких постерах конференции, которые сумели заметить. selective attention improves transformer инженеры из google придумали дешёвую добавку к софтмаксу в аттеншене, которая позволяет трансформеру легче забывать токены. это стабильно улучшает итоговое качество, как перплексию, так и downstream tasks. проверяли на размерах модели до 1в и контекстах до 2к. прирост в качестве как будто бы не снижается с увеличением размера модели и контекста. говорят, что, поскольку модель теперь нативно выучивает более sparse-аттеншн, то можно выкидывать токены из kv-кэша по некоторому трешхолду, уменьшая потребление памяти или ускоряя инференс. например, можно получить такую же перплексию, как у бейзлайна, но при kv-cache в восемь раз меньше. а если ещё и немного поменять лосс, чтобы заставить модель более активно выкидывать токены, то kv-cache можно сократить в 47 раз. scaling fp8 training to trillion-token llms тренируют llama 7b в fp8 (матричные умножения, и форвард, и бэквард). после 200b токенов видят расхождение, которого прежде нет, и утверждают, что это из-за того, что ветки swiglu становятся скоррелированными, и появляются outlier при их перемножении чтобы решить эту проблему, предлагают дополнительно скейлить одну из веток (а после третьего линейного слоя возвращать обратно). это стабилизирует обучение с минимальными потерями в скорости. из дополнительных трюков — квантизируют моменты адама в fp8 (e4m3 для первого и e5m2 для второго), чтобы сэкономить память. на маленьких моделях такого не наблюдали, но там использовали обычный gpt, без swiglu. сейчас авторы экспериментируют с nvfp4/mxfp4, говорят, что там нужен претрейн и посттрейн в bf16 с вормапами. regenesis: llms can grow into reasoning generalists via self-improvement интересная статья о том, как модель сама себе итеративно генерирует цепочку рассуждений — сначала общими словами, потом более конкретно под задачу. затем на эти финальные цепочки мы делаем sft. получается лучше star и с хорошей генерализуемостью. q-sft: q-learning for language models via supervised fine-tuning авторы решают одну проблему алгоритма q-learning для языковых моделей — не нужно обучать огромную голову (по q-значению на каждый токен) с нуля. они берут дебедер и дообучают его на q-значения с помощью кросс-энтропийного лосса. есть предположение, что в llm из-за детерминированных переходов среды это теоретически корректно. strong model collapse в статье утверждается, что синтетические данные ломают классические скейлинг лоу. причём ломает уже сильно, если доля синтетики просто фиксирована относительно обычных данных в претрейне. более качественная синтетика просто двигает вправо размер модели и количество данных, на котором произойдёт поломка. решение — итеративное обучение, с постепенным снижением доли синтетики в 0. ну или не использовать её вовсе. think: thinner key cache by query-driven pruning в отличие от других статей о сжатии kv-кэша, в этой авторы смотрят не на размерность seq_len, а делают в рантайме уменьшение размерности channel для q/k-матриц проекций с помощью поиска аутлаеров. в аттеншоне именно такие аутлаеры важны — остальные 40% можно убирать. из-за того, что делают динамически для каждого префикса, на prefill, то ftt увеличивается примерно на 10% (реализуется, кстати, относительно просто). но без потери качества ускоряется декодирование — как по занимаемой памяти, так и по латенси/фрупуту. более того, метод хорошо комбинируется с другими методами компрессии кэша по размерности seq_len и даёт ортогональное ускорение в 1,2 раза. интересные постеры увидели ❣ степан каргальцев, павел темирчев, андрей акшонов, николай скачков, роман горб #yaiclr душный nlp свежая подборка постеров с iclr 2025 продолжаем рассказывать о самых ярких постерах конференции, которые сумели заметить. selective attention improves transformer инженеры из google придумали дешёвую добавку к софтмаксу в аттеншене, которая позволяет трансформеру легче забывать токены. это стабильно улучшает итоговое качество, как перплексию, так и downstream tasks. проверяли на размерах модели до 1в и контекстах до 2к. прирост в качестве как будто бы не снижается с увеличением размера модели и контекста. говорят, что, поскольку модель теперь нативно выучивает более sparse-аттеншн, то можно выкидывать токены из kv-кэша по некоторому трешхолду, уменьшая потребление памяти или ускоряя инференс. например, можно получить такую же перплексию, как у бейзлайна, но при kv-cache в восемь раз меньше. а если ещё и немного поменять лосс, чтобы заставить модель более активно выкидывать токены, то kv-cache можно сократить в 47 раз. scaling fp8 training to trillion-token llms тренируют llama 7b в fp8 (матричные умножения, и форвард, и бэквард). после 200b токенов видят расхождение, которого прежде нет, и утверждают, что это из-за того, что ветки swiglu становятся скоррелированными, и появляются outlier при их перемножении чтобы решить эту проблему, предлагают дополнительно скейлить одну из веток (а после третьего линейного слоя возвращать обратно). это стабилизирует обучение с минимальными потерями в скорости. из дополнительных трюков — квантизируют моменты адама в fp8 (e4m3 для первого и e5m2 для второго), чтобы сэкономить память. на маленьких моделях такого не наблюдали, но там использовали обычный gpt, без swiglu. сейчас авторы экспериментируют с nvfp4/mxfp4, говорят, что там нужен претрейн и посттрейн в bf16 с вормапами. regenesis: llms can grow into reasoning generalists via self-improvement интересная статья о том, как модель сама себе итеративно генерирует цепочку рассуждений — сначала общими словами, потом более конкретно под задачу. затем на эти финальные цепочки мы делаем sft. получается лучше star и с хорошей генерализуемостью. q-sft: q-learning for language models via supervised fine-tuning авторы решают одну проблему алгоритма q-learning для языковых моделей — не нужно обучать огромную голову (по q-значению на каждый токен) с нуля. они берут дебедер и дообучают его на q-значения с помощью кросс-энтропийного лосса. есть предположение, что в llm из-за детерминированных переходов среды это теоретически корректно. strong model collapse в статье утверждается, что синтетические данные ломают классические скейлинг лоу. причём ломает уже сильно, если доля синтетики просто фиксирована относительно обычных данных в претрейне. более качественная синтетика просто двигает вправо размер модели и количество данных, на котором произойдёт поломка. решение — итеративное обучение, с постепенным снижением доли синтетики в 0. ну или не использовать её вовсе. think: thinner key cache by query-driven pruning в отличие от других статей о сжатии kv-кэша, в этой авторы смотрят не на размерность seq_len, а делают в рантайме уменьшение размерности channel для q/k-матриц проекций с помощью поиска аутлаеров. в аттеншоне именно такие аутлаеры важны — остальные 40% можно убирать. из-за того, что делают динамически для каждого префикса, на prefill, то ftt увеличивается примерно на 10% (реализуется, кстати, относительно просто). но без потери качества ускоряется декодирование — как по занимаемой памяти, так и по латенси/фрупуту. более того, метод хорошо комбинируется с другими методами компрессии кэша по размерности seq_len и даёт ортогональное ускорение в 1,2 раза. интересные постеры увидели ❣ степан каргальцев, павел темирчев, андрей акшонов, николай скачков, роман горб #yaiclr душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-26T15:02:38+00:00" href="./posts/112.html">2025-04-26 15:02 UTC</a></div>
      </div>
      <div class="post-body"><strong>Свежая подборка постеров с ICLR 2025</strong><br><br>Продолжаем рассказывать о самых ярких постерах конференции, которые сумели заметить.<br><br><a href="https://arxiv.org/abs/2410.02703" rel="nofollow noopener noreferrer"><strong>Selective Attention Improves Transformer</strong></a><br><br>Инженеры из Google придумали дешёвую добавку к софтмаксу в аттеншене, которая позволяет трансформеру легче забывать токены. Это стабильно улучшает итоговое качество, как перплексию, так и downstream tasks. Проверяли на размерах модели до 1В и контекстах до 2К. Прирост в качестве как будто бы не снижается с увеличением размера модели и контекста.<br><br>Говорят, что, поскольку модель теперь нативно выучивает более sparse-аттеншн, то можно выкидывать токены из kv-кэша по некоторому трешхолду, уменьшая потребление памяти или ускоряя инференс. Например, можно получить такую же перплексию, как у бейзлайна, но при kv-cache в восемь раз меньше. А если ещё и немного поменять лосс, чтобы заставить модель более активно выкидывать токены, то kv-cache можно сократить в 47 раз.<br><br><a href="https://arxiv.org/abs/2409.12517" rel="nofollow noopener noreferrer"><strong>Scaling FP8 training to trillion-token LLMs</strong></a><br><br>Тренируют Llama 7B в FP8 (матричные умножения, и форвард, и бэквард). После 200B токенов видят расхождение, которого прежде нет, и утверждают, что это из-за того, что ветки SwiGLU становятся скоррелированными, и появляются outlier при их перемножении<br><br>Чтобы решить эту проблему, предлагают дополнительно скейлить одну из веток (а после третьего линейного слоя возвращать обратно). Это стабилизирует обучение с минимальными потерями в скорости. Из дополнительных трюков — квантизируют моменты адама в FP8 (e4m3 для первого и e5m2 для второго), чтобы сэкономить память.<br><br>На маленьких моделях такого не наблюдали, но там использовали обычный GPT, без SwiGLU. Сейчас авторы экспериментируют с nvfp4/mxfp4, говорят, что там нужен претрейн и посттрейн в BF16 с вормапами.<br><br><a href="https://arxiv.org/abs/2410.02108" rel="nofollow noopener noreferrer"><strong>ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement</strong></a><br><br>Интересная статья о том, как модель сама себе итеративно генерирует цепочку рассуждений — сначала общими словами, потом более конкретно под задачу. Затем на эти финальные цепочки мы делаем SFT. Получается лучше star и с хорошей генерализуемостью.<br><br><a href="https://arxiv.org/abs/2411.05193" rel="nofollow noopener noreferrer"><strong>Q-SFT: Q-Learning for Language Models via Supervised Fine-Tuning</strong></a><br><br>Авторы решают одну проблему алгоритма Q-Learning для языковых моделей — не нужно обучать огромную голову (по q-значению на каждый токен) с нуля. Они берут дебедер и дообучают его на q-значения с помощью кросс-энтропийного лосса. Есть предположение, что в LLM из-за детерминированных переходов среды это теоретически корректно.<br><br><a href="https://arxiv.org/abs/2410.04840" rel="nofollow noopener noreferrer"><strong>Strong Model Collapse</strong></a><br><br>В статье утверждается, что синтетические данные ломают классические скейлинг лоу. Причём ломает уже сильно, если доля синтетики просто фиксирована относительно обычных данных в претрейне. Более качественная синтетика просто двигает вправо размер модели и количество данных, на котором произойдёт поломка.<br><br>Решение — итеративное обучение, с постепенным снижением доли синтетики в 0. Ну или не использовать её вовсе.<br><br><a href="https://arxiv.org/abs/2407.21018" rel="nofollow noopener noreferrer"><strong>ThinK: Thinner Key Cache by Query-Driven Pruning</strong></a><br><br>В отличие от других статей о сжатии kv-кэша, в этой авторы смотрят не на размерность seq_len, а делают в рантайме уменьшение размерности channel для Q/K-матриц проекций с помощью поиска аутлаеров. В аттеншоне именно такие аутлаеры важны — остальные 40% можно убирать.<br><br>Из-за того, что делают динамически для каждого префикса, на prefill, то FTT увеличивается примерно на 10% (реализуется, кстати, относительно просто). Но без потери качества ускоряется декодирование — как по занимаемой памяти, так и по латенси/фрупуту. <br><br>Более того, метод хорошо комбинируется с другими методами компрессии кэша по размерности seq_len и даёт ортогональное ускорение в 1,2 раза. <br><br><em>Интересные постеры увидели  </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Степан Каргальцев, Павел Темирчев, Андрей Акшонов, Николай Скачков, Роман Горб</em><br><br>#YaICLR<br><br><a href="https://t.me/+Re4xecmM_LxiMTNi" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/112_480.webp" srcset="../assets/media/thumbs/112_480.webp 480w, ../assets/media/112.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="112" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/113_480.webp" srcset="../assets/media/thumbs/113_480.webp 480w, ../assets/media/113.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="112" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/114_480.webp" srcset="../assets/media/thumbs/114_480.webp 480w, ../assets/media/114.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="112" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/115_480.webp" srcset="../assets/media/thumbs/115_480.webp 480w, ../assets/media/115.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="112" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/116_480.webp" srcset="../assets/media/thumbs/116_480.webp 480w, ../assets/media/116.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="112" data-image-index="4" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/117_480.webp" srcset="../assets/media/thumbs/117_480.webp 480w, ../assets/media/117.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="112" data-image-index="5" /></div></div>
      <div class="actions">
        <span>2 519 просмотров · 20 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/112" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/112.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="106" data-search="ещё крутые постеры с iclr 2025 продолжаем рассказывать о любопытных постерах проходящей прямо сейчас конференции. scalable influence and fact tracing for large language model pretraining deepmind предлагает новый метод дебага того, какие примеры влияют на ответы фактовых вопросов. говорят, что лучше всех градиентных методов определяют именно влияние документа из трейна на генерацию. ещё из интересного — показывают график, по которому видно, что модели небольшого размера очень часто опираются не на примеры, содержащие факт, а с ростом капасити разница падает. авторы отмечают, что их метод подходит даже для мультихопов и ризонингов, несмотря на один градиентный шаг. ещё сказали, что общались с группой, которая писала статьи о динамики обучения фактам, и они работают в эту сторону. и добавили, что метод полезен для файнтюнов. netmoe: accelerating moe training through dynamic sample placement авторы предлагают хитрую оптимизацию тренировки мixture-of-agents. во время dispatch + ffn они решают (с помощью аппроксимации integer linear programming), а не выгоднее ли оставить эксперта там, где он сейчас? возможно, в этом случае combine будет быстрее, из-за использования не дорогих inter-node-пересылок, а дешёвых intra-node или даже in-device. в результате получают ускорение в 1.67х для простого all-to-all, но ничего не мешает пробовать это же для более умных пересылок. zeroth-order policy gradient for reinforcement learning from human feedback without reward inference авторы делают оптимизацию нулевого порядка для rl. идейно похоже на evolution strategies, но «под капотом» тут другой оптимизатор. в статье также предлагают отказаться от rm и заменить её на людей или хотя бы на preference model. мотивация, зачем так делать, осталась непрозрачной. рискуем предположить, что это будет медленнее градиентных методов. learning from negative feedback, or positive feedback or both статья от deepmind, в которой предлагают обучать на a log(p(positive) - (1-a) log (negative) - b kl(p_ref(negative) || p(negative)) таким образом, становятся не нужны контрастные пары — достаточно положительных и отрицательных примеров. у авторов результаты получаются сравнимыми с dpo или лучше. self-improving robust preference optimization статья от сohere. авторы замешали nash preference learning в алгоритм вроде dpo/ipo. по формулам выглядит так, будто учат две конкурирующие модели: генератор и улучшатор ответов (на вход получает запрос и предыдущий ответ). но по факту это одна модель, просто улучшатору дают подводку вида «вот прошлый ответ, попробуй улучшить» online не пробовали, но рассказали, что можно вытащить реворд из их формул. learning dynamics of llm finetuning доклад с теоретическим анализом sft и dpo, который обосновывает галлюцинации в первом и падение победителя во втором. выводы: просадка победителя может быть связана с тем, что мы пытаемся уменьшать вероятности для проигравшего, когда они уже и так низкие. интересные постеры увидели ❣ екатерина редина, степан каргальцев, павел темирчев, дмитрий ульянов #yaiclr душный nlp ещё крутые постеры с iclr 2025 продолжаем рассказывать о любопытных постерах проходящей прямо сейчас конференции. scalable influence and fact tracing for large language model pretraining deepmind предлагает новый метод дебага того, какие примеры влияют на ответы фактовых вопросов. говорят, что лучше всех градиентных методов определяют именно влияние документа из трейна на генерацию. ещё из интересного — показывают график, по которому видно, что модели небольшого размера очень часто опираются не на примеры, содержащие факт, а с ростом капасити разница падает. авторы отмечают, что их метод подходит даже для мультихопов и ризонингов, несмотря на один градиентный шаг. ещё сказали, что общались с группой, которая писала статьи о динамики обучения фактам, и они работают в эту сторону. и добавили, что метод полезен для файнтюнов. netmoe: accelerating moe training through dynamic sample placement авторы предлагают хитрую оптимизацию тренировки мixture-of-agents. во время dispatch + ffn они решают (с помощью аппроксимации integer linear programming), а не выгоднее ли оставить эксперта там, где он сейчас? возможно, в этом случае combine будет быстрее, из-за использования не дорогих inter-node-пересылок, а дешёвых intra-node или даже in-device. в результате получают ускорение в 1.67х для простого all-to-all, но ничего не мешает пробовать это же для более умных пересылок. zeroth-order policy gradient for reinforcement learning from human feedback without reward inference авторы делают оптимизацию нулевого порядка для rl. идейно похоже на evolution strategies, но «под капотом» тут другой оптимизатор. в статье также предлагают отказаться от rm и заменить её на людей или хотя бы на preference model. мотивация, зачем так делать, осталась непрозрачной. рискуем предположить, что это будет медленнее градиентных методов. learning from negative feedback, or positive feedback or both статья от deepmind, в которой предлагают обучать на a log(p(positive) - (1-a) log (negative) - b kl(p_ref(negative) || p(negative)) таким образом, становятся не нужны контрастные пары — достаточно положительных и отрицательных примеров. у авторов результаты получаются сравнимыми с dpo или лучше. self-improving robust preference optimization статья от сohere. авторы замешали nash preference learning в алгоритм вроде dpo/ipo. по формулам выглядит так, будто учат две конкурирующие модели: генератор и улучшатор ответов (на вход получает запрос и предыдущий ответ). но по факту это одна модель, просто улучшатору дают подводку вида «вот прошлый ответ, попробуй улучшить» online не пробовали, но рассказали, что можно вытащить реворд из их формул. learning dynamics of llm finetuning доклад с теоретическим анализом sft и dpo, который обосновывает галлюцинации в первом и падение победителя во втором. выводы: просадка победителя может быть связана с тем, что мы пытаемся уменьшать вероятности для проигравшего, когда они уже и так низкие. интересные постеры увидели ❣ екатерина редина, степан каргальцев, павел темирчев, дмитрий ульянов #yaiclr душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-26T12:13:19+00:00" href="./posts/106.html">2025-04-26 12:13 UTC</a></div>
      </div>
      <div class="post-body"><strong>Ещё крутые постеры с ICLR 2025 </strong><br><br>Продолжаем рассказывать о любопытных постерах проходящей прямо сейчас конференции.<br><br><a href="https://arxiv.org/abs/2410.17413" rel="nofollow noopener noreferrer"><strong>Scalable Influence and Fact Tracing for Large Language Model Pretraining </strong></a><br><br>DeepMind предлагает новый метод дебага того, какие примеры влияют на ответы фактовых вопросов. Говорят, что лучше всех градиентных методов определяют именно влияние документа из трейна на генерацию. Ещё из интересного — показывают график, по которому видно, что модели небольшого размера очень часто опираются не на примеры, содержащие факт, а с ростом капасити разница падает.<br><br>Авторы отмечают, что их метод подходит даже для мультихопов и ризонингов, несмотря на один градиентный шаг. Ещё сказали, что общались с группой, которая писала статьи о динамики обучения фактам, и они работают в эту сторону. И добавили, что метод полезен для файнтюнов. <br><br><a href="https://openreview.net/forum?id=1qP3lsatCR" rel="nofollow noopener noreferrer"><strong>NetMoE: Accelerating MoE Training through Dynamic Sample Placement</strong></a><br><br>Авторы предлагают хитрую оптимизацию тренировки Мixture-of-Agents. Во время dispatch + ffn они решают (с помощью аппроксимации integer linear programming), а не выгоднее ли оставить эксперта там, где он сейчас? Возможно, в этом случае combine будет быстрее, из-за использования не дорогих inter-node-пересылок, а дешёвых intra-node или даже in-device. В результате Получают ускорение в 1.67х для простого all-to-all, но ничего не мешает пробовать это же для более умных пересылок.<br><br><a href="https://arxiv.org/abs/2409.17401" rel="nofollow noopener noreferrer"><strong>Zeroth-Order Policy Gradient for Reinforcement Learning from Human Feedback without Reward Inference</strong></a><br><br>Авторы делают оптимизацию нулевого порядка для RL. Идейно похоже на evolution strategies, но «под капотом» тут другой оптимизатор. В статье также предлагают отказаться от RM и заменить её на людей или хотя бы на preference model. Мотивация, зачем так делать, осталась непрозрачной. Рискуем предположить, что это будет медленнее градиентных методов.<br><br><a href="https://arxiv.org/abs/2410.04166" rel="nofollow noopener noreferrer"><strong>Learning from negative feedback, or positive feedback or both</strong></a><br><br>Статья от DeepMind, в которой предлагают обучать на <br><blockquote>a log(p(positive) - (1-a) log (negative) - b KL(p_ref(negative) || p(negative))</blockquote><br>Таким образом, становятся не нужны контрастные пары — достаточно положительных и отрицательных примеров. У авторов результаты получаются сравнимыми с DPO или лучше.<br><br><a href="https://arxiv.org/abs/2406.01660" rel="nofollow noopener noreferrer"><strong>Self-Improving Robust Preference Optimization</strong></a><strong><br></strong><br>Статья от Сohere. Авторы замешали Nash preference learning в алгоритм вроде DPO/IPO. По формулам выглядит так, будто учат две конкурирующие модели: генератор и улучшатор ответов (на вход получает запрос и предыдущий ответ).<br>Но по факту это одна модель, просто улучшатору дают подводку вида «вот прошлый ответ, попробуй улучшить» Online не пробовали, но рассказали, что можно вытащить реворд из их формул.<br><br><a href="https://arxiv.org/abs/2407.10490" rel="nofollow noopener noreferrer"><strong>Learning Dynamics of LLM Finetuning</strong></a><br><br>Доклад с теоретическим анализом SFT и DPO, который обосновывает галлюцинации в первом и падение победителя во втором. Выводы: просадка победителя может быть связана с тем, что мы пытаемся уменьшать вероятности для проигравшего, когда они уже и так низкие.<br><br><em>Интересные постеры увидели </em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji><em> Екатерина Редина, Степан Каргальцев, Павел Темирчев, Дмитрий Ульянов</em><br><br>#YaICLR<br><br><a href="https://t.me/+Re4xecmM_LxiMTNi" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/106_480.webp" srcset="../assets/media/thumbs/106_480.webp 480w, ../assets/media/106.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="106" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/107_480.webp" srcset="../assets/media/thumbs/107_480.webp 480w, ../assets/media/107.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="106" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/108_480.webp" srcset="../assets/media/thumbs/108_480.webp 480w, ../assets/media/108.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="106" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/109_480.webp" srcset="../assets/media/thumbs/109_480.webp 480w, ../assets/media/109.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="106" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/110_480.webp" srcset="../assets/media/thumbs/110_480.webp 480w, ../assets/media/110.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="106" data-image-index="4" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/111_480.webp" srcset="../assets/media/thumbs/111_480.webp 480w, ../assets/media/111.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="106" data-image-index="5" /></div></div>
      <div class="actions">
        <span>2 573 просмотров · 25 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/106" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/106.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="105" data-search="добрались до конца пятницы! но конец iclr 2025 ещё не скоро, а это значит, что нас ждёт больше интересных статей и разборов. не переключайтесь! #yaiclr добрались до конца пятницы! но конец iclr 2025 ещё не скоро, а это значит, что нас ждёт больше интересных статей и разборов. не переключайтесь! #yaiclr">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-25T15:37:29+00:00" href="./posts/105.html">2025-04-25 15:37 UTC</a></div>
      </div>
      <div class="post-body"><strong>Добрались до конца пятницы!</strong><br><br>Но конец ICLR 2025 ещё не скоро, а это значит, что нас ждёт больше интересных статей и разборов. Не переключайтесь!<br><br>#YaICLR<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/105_480.webp" srcset="../assets/media/thumbs/105_480.webp 480w, ../assets/media/105.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="105" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 572 просмотров · 16 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/105" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/105.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="101" data-search="постеры второго дня iclr 2025 возвращаемся с полей конференции и несем новую порцию постеров. spar: self-play with tree-search refinement to improve instruction-following in large language models статья о dpo в self-play-цикле. есть обучаемая на лету llm-as-judge, которая здесь называется refiner. модель генерирует ответ на запрос, и если он неправильный, то исправляем его, стараясь сделать наименьшее число изменений. исправляем с помощью refiner и поиска по дереву. на таких парах учим dpo. paramδ for direct mixing: post-train large language model at zero cost авторы предлагают не учить посттрейны, а прибавлять к новому претрейну дельту. или линейную комбинацию дельт. получаются смеси доменно адаптированных моделей или просто дешёвый быстрый алайнмент нового претрейна (с несильным ухудшением качества). mitigating reward over-optimization in rlhf via behavior-supported regularization в статье предлагают приделать к rm авторегрессионную голову и учить её на sft. логиты при этом предлагается использовать внутри rl-алгоритма — занижать реворды ответам с низким правдоподобием по мнению этой авторегрессионной головы. таким образом, реворд не будет расти в ood для rm-примерах, а мы будем меньше страдать от доменного сдвига. on-the-fly preference alignment via principle-guided decoding авторы рассказывают, как заставить модель исполнять системный промпт не подкладыванием его в промпт, а с помощью модификации процедуры инференса. системный промпт здесь называют принципом. идея похожа на classier-free guidance: — считаем вероятности всех токенов на шаге t с системным промптом и без него (два форварда); — считаем реворд по формуле (логарифм соотношения вероятностей); — находим оптимальное распределение для такого реворда по аналитической формуле; — поскольку реворд тут жадный и распределение над токенами (а не над траекториями как в dpo) аналитическое решение явно считается. на этом всё. дальше просто семплируем из этого распределения токен для шага t и повторяем. говорят, это лучше, чем положить системный промпт в подводку. интересные постеры увидели ❣ павел темирчев и николай скачков #yaiclr душный nlp постеры второго дня iclr 2025 возвращаемся с полей конференции и несем новую порцию постеров. spar: self-play with tree-search refinement to improve instruction-following in large language models статья о dpo в self-play-цикле. есть обучаемая на лету llm-as-judge, которая здесь называется refiner. модель генерирует ответ на запрос, и если он неправильный, то исправляем его, стараясь сделать наименьшее число изменений. исправляем с помощью refiner и поиска по дереву. на таких парах учим dpo. paramδ for direct mixing: post-train large language model at zero cost авторы предлагают не учить посттрейны, а прибавлять к новому претрейну дельту. или линейную комбинацию дельт. получаются смеси доменно адаптированных моделей или просто дешёвый быстрый алайнмент нового претрейна (с несильным ухудшением качества). mitigating reward over-optimization in rlhf via behavior-supported regularization в статье предлагают приделать к rm авторегрессионную голову и учить её на sft. логиты при этом предлагается использовать внутри rl-алгоритма — занижать реворды ответам с низким правдоподобием по мнению этой авторегрессионной головы. таким образом, реворд не будет расти в ood для rm-примерах, а мы будем меньше страдать от доменного сдвига. on-the-fly preference alignment via principle-guided decoding авторы рассказывают, как заставить модель исполнять системный промпт не подкладыванием его в промпт, а с помощью модификации процедуры инференса. системный промпт здесь называют принципом. идея похожа на classier-free guidance: — считаем вероятности всех токенов на шаге t с системным промптом и без него (два форварда); — считаем реворд по формуле (логарифм соотношения вероятностей); — находим оптимальное распределение для такого реворда по аналитической формуле; — поскольку реворд тут жадный и распределение над токенами (а не над траекториями как в dpo) аналитическое решение явно считается. на этом всё. дальше просто семплируем из этого распределения токен для шага t и повторяем. говорят, это лучше, чем положить системный промпт в подводку. интересные постеры увидели ❣ павел темирчев и николай скачков #yaiclr душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-25T10:07:40+00:00" href="./posts/101.html">2025-04-25 10:07 UTC</a></div>
      </div>
      <div class="post-body"><strong>Постеры второго дня ICLR 2025 </strong><br><br>Возвращаемся с полей конференции и несем новую порцию постеров. <br><br><a href="https://arxiv.org/abs/2412.11605" rel="nofollow noopener noreferrer"><strong>SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models</strong></a><br><br>Статья о DPO в self-play-цикле. Есть обучаемая на лету llm-as-judge, которая здесь называется Refiner. Модель генерирует ответ на запрос, и если он неправильный, то исправляем его, стараясь сделать наименьшее число изменений. Исправляем с помощью Refiner и поиска по дереву. На таких парах учим DPO.<br><strong><br></strong><a href="https://openreview.net/forum?id=vqbd2OQnGp" rel="nofollow noopener noreferrer"><strong>ParamΔ for Direct Mixing: Post-Train Large Language Model At Zero Cost</strong></a><br><br>Авторы предлагают не учить посттрейны, а прибавлять к новому претрейну дельту. Или линейную комбинацию дельт. Получаются смеси доменно адаптированных моделей или просто дешёвый быстрый алайнмент нового претрейна (с несильным ухудшением качества).<br><br><a href="https://arxiv.org/abs/2503.18130" rel="nofollow noopener noreferrer"><strong>Mitigating Reward Over-Optimization in RLHF via Behavior-Supported Regularization</strong></a><br><br>В статье предлагают приделать к RM авторегрессионную голову и учить её на SFT. Логиты при этом предлагается использовать внутри RL-алгоритма — занижать реворды ответам с низким правдоподобием по мнению этой авторегрессионной головы. Таким образом, реворд не будет расти в OOD для RM-примерах, а мы будем меньше страдать от доменного сдвига.<br><br><a href="https://arxiv.org/abs/2502.14204" rel="nofollow noopener noreferrer"><strong>On-the-fly Preference Alignment via Principle-Guided Decoding</strong></a><br><br>Авторы рассказывают, как заставить модель исполнять системный промпт не подкладыванием его в промпт, а с помощью модификации процедуры инференса. Системный промпт здесь называют принципом. <br><br>Идея похожа на classier-free guidance: <br>— считаем вероятности всех токенов на шаге t с системным промптом и без него (два форварда);<br>— считаем реворд по формуле (логарифм соотношения вероятностей);<br>— находим оптимальное распределение для такого реворда по аналитической формуле;<br>— поскольку реворд тут жадный и распределение над токенами (а не над траекториями как в DPO) аналитическое решение явно считается.<br><br>На этом всё. Дальше просто семплируем из этого распределения токен для шага t и повторяем. Говорят, это лучше, чем положить системный промпт в подводку.<br><br><em>Интересные постеры увидели </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Павел Темирчев и Николай Скачков</em><br><br>#YaICLR<br><br><a href="https://t.me/+nkYkavHZOTVhNTYy" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/101_480.webp" srcset="../assets/media/thumbs/101_480.webp 480w, ../assets/media/101.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="101" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/102_480.webp" srcset="../assets/media/thumbs/102_480.webp 480w, ../assets/media/102.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="101" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/103_480.webp" srcset="../assets/media/thumbs/103_480.webp 480w, ../assets/media/103.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="101" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/104_480.webp" srcset="../assets/media/thumbs/104_480.webp 480w, ../assets/media/104.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="101" data-image-index="3" /></div></div>
      <div class="actions">
        <span>6 181 просмотров · 20 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/101" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/101.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="100" data-search="первый день iclr 2025 — всё а вот как он начинался — с больших очередей на регистрацию. первый день iclr 2025 — всё а вот как он начинался — с больших очередей на регистрацию.">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-24T16:10:45+00:00" href="./posts/100.html">2025-04-24 16:10 UTC</a></div>
      </div>
      <div class="post-body"><strong>Первый день ICLR 2025 — ВСЁ</strong><br><br>А вот как он начинался — с больших очередей на регистрацию.<div class="media"><video controls preload="metadata" src="../assets/media/100_video_2025-04-24_16-21-34.mp4"></video></div></div>
      <div class="actions">
        <span>2 937 просмотров · 32 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/100" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/100.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="99" data-search="интересные постеры первого дня iclr 2025 конференция в самом разгаре — доклады и постеры сыпятся на нас как из рога изобилия. а мы выбираем самые любопытные и рассказываем вам. earlier tokens contribute more: learning direct preference optimization from temporal decay perspective статья посвящена решению проблемы, при которой модель после dpo генерирует ответы длиннее, чем референсная модель. это связано с тем, dpo отдаёт предпочтение последним токенам, а не первым. чтобы исправить проблему, авторы предлагают добавить множитель \gamma \in (0, 1) в лосс. лосс для токена в позиции t умножается на \gamma^t. аналогия с классическим rl ясна, хотя нужно понимать, что это не discount factor, а просто что-то похожее по смыслу. такая политика мало отличается от оптимальной, а задача выбора гаммы выпуклая (около 0,99 будет достаточно, но лучше подобрать для каждого случая отдельно). progressive mixed-precision decoding for efficient llm inference квантизация, отмечают авторы, хороший способ снизить требования llm к вычислительным мощностям. однако применение низкой точности (2-3 бита) ведёт к сильному ухудшению качества. авторы предлагают новый фазо-ориентированный метод, который избирательно распределяет точность между различными фазами инференса и вводят технику, позволяющую постепенно снижать точность по мере углубления в сгенерированную последовательность. по сути для каждого токена выбирается битность. у авторов есть обучаемый шедулер, который предсказывает, когда надо переключаться на меньшую битность. он очень чувствителен к гиперпараметрам, датасету и обучению. в будущем его хотят интегрировать внутрь самой llm. а саму квантизацию наследуют из статьи any-precision llm. в ней префикс веса нужной битности — это ключ в lookup-таблице весов, что позволяет не использовать дополнительную память под разные битности. what is wrong with perplexity for long-context language modeling? очень простая идея для длинного контекста — считать лосс в основном на key-токенах, где лосс, обусловленный на длинный контекст, сильно отличается от короткого. на long-бенче у авторов получилась значительная корреляция со скором по сравнению с обычной ppl, что немного смущает, и улучшения от такого тюна. rrm: robust reward model training mitigates reward hacking авторы заявляют, что современные реворд-модели не способны эффективно различать контекстуальные сигналы и нерелевантные артефакты при определении предпочтений. в статье предлагается обучать предпочтения, независимые от подобных артефактов, а также новая техника аугментации данных, специально разработанную для их устранения. авторы делают случайную перестановку датасета и расширяют его всеми возможными комбинациями i оригинальной тройки и sigma_i — тройки на позиции i после перестановки. всего комбинаций 16 штук. победитель в полученных парах определяется так: — если в паре один ответ на этот запрос, а второй от другого запроса, то побеждает всегда тот, который отвечает на «свой» запрос; — если оба ответа от другого запроса, то это ничья. получается огромный датасет, в котором много тривиальных пар, где плохой ответ явно не от того запроса. авторы фильтруют этот датасет с помощью предыдущей версии rm, оставляя только негативы и неуверенные. how new data permeates llm knowledge and how to dilute it авторы показывают, что при усвоении новой информации llm проявляют эффект «прайминга»: изучение нового факта может привести к тому, что модель начнёт некорректно применять это знание в несвязанных контекстах. чтобы это исправить, предлагают игнорировать самые большие градиенты — то есть не обновлять тот процент весов, который получил бы самый большой градиентный апдейт. интересные постеры увидели ❣ павел темирчев, екатерина редина, роман горб, степан каргалицев #yaiclr интересные постеры первого дня iclr 2025 конференция в самом разгаре — доклады и постеры сыпятся на нас как из рога изобилия. а мы выбираем самые любопытные и рассказываем вам. earlier tokens contribute more: learning direct preference optimization from temporal decay perspective статья посвящена решению проблемы, при которой модель после dpo генерирует ответы длиннее, чем референсная модель. это связано с тем, dpo отдаёт предпочтение последним токенам, а не первым. чтобы исправить проблему, авторы предлагают добавить множитель \gamma \in (0, 1) в лосс. лосс для токена в позиции t умножается на \gamma^t. аналогия с классическим rl ясна, хотя нужно понимать, что это не discount factor, а просто что-то похожее по смыслу. такая политика мало отличается от оптимальной, а задача выбора гаммы выпуклая (около 0,99 будет достаточно, но лучше подобрать для каждого случая отдельно). progressive mixed-precision decoding for efficient llm inference квантизация, отмечают авторы, хороший способ снизить требования llm к вычислительным мощностям. однако применение низкой точности (2-3 бита) ведёт к сильному ухудшению качества. авторы предлагают новый фазо-ориентированный метод, который избирательно распределяет точность между различными фазами инференса и вводят технику, позволяющую постепенно снижать точность по мере углубления в сгенерированную последовательность. по сути для каждого токена выбирается битность. у авторов есть обучаемый шедулер, который предсказывает, когда надо переключаться на меньшую битность. он очень чувствителен к гиперпараметрам, датасету и обучению. в будущем его хотят интегрировать внутрь самой llm. а саму квантизацию наследуют из статьи any-precision llm . в ней префикс веса нужной битности — это ключ в lookup-таблице весов, что позволяет не использовать дополнительную память под разные битности. what is wrong with perplexity for long-context language modeling? очень простая идея для длинного контекста — считать лосс в основном на key-токенах, где лосс, обусловленный на длинный контекст, сильно отличается от короткого. на long-бенче у авторов получилась значительная корреляция со скором по сравнению с обычной ppl, что немного смущает, и улучшения от такого тюна. rrm: robust reward model training mitigates reward hacking авторы заявляют, что современные реворд-модели не способны эффективно различать контекстуальные сигналы и нерелевантные артефакты при определении предпочтений. в статье предлагается обучать предпочтения, независимые от подобных артефактов, а также новая техника аугментации данных, специально разработанную для их устранения. авторы делают случайную перестановку датасета и расширяют его всеми возможными комбинациями i оригинальной тройки и sigma_i — тройки на позиции i после перестановки. всего комбинаций 16 штук. победитель в полученных парах определяется так: — если в паре один ответ на этот запрос, а второй от другого запроса, то побеждает всегда тот, который отвечает на «свой» запрос; — если оба ответа от другого запроса, то это ничья. получается огромный датасет, в котором много тривиальных пар, где плохой ответ явно не от того запроса. авторы фильтруют этот датасет с помощью предыдущей версии rm, оставляя только негативы и неуверенные. how new data permeates llm knowledge and how to dilute it авторы показывают, что при усвоении новой информации llm проявляют эффект «прайминга»: изучение нового факта может привести к тому, что модель начнёт некорректно применять это знание в несвязанных контекстах. чтобы это исправить, предлагают игнорировать самые большие градиенты — то есть не обновлять тот процент весов, который получил бы самый большой градиентный апдейт. интересные постеры увидели ❣ павел темирчев, екатерина редина, роман горб, степан каргалицев #yaiclr">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-24T08:52:03+00:00" href="./posts/99.html">2025-04-24 08:52 UTC</a></div>
      </div>
      <div class="post-body"><strong>Интересные постеры первого дня ICLR 2025</strong><br><br>Конференция в самом разгаре — доклады и постеры сыпятся на нас как из рога изобилия. А мы выбираем самые любопытные и рассказываем вам. <br><br><a href="https://arxiv.org/abs/2502.14340" rel="nofollow noopener noreferrer"><strong>Earlier Tokens Contribute More: Learning Direct Preference Optimization From Temporal Decay Perspective</strong></a><strong><br></strong><br>Статья посвящена решению проблемы, при которой модель после DPO генерирует ответы длиннее, чем референсная модель. Это связано с тем, DPO отдаёт предпочтение последним токенам, а не первым. Чтобы исправить проблему, авторы предлагают добавить множитель \gamma \in (0, 1) в лосс. Лосс для токена в позиции t умножается на \gamma^t. Аналогия с классическим RL ясна, хотя нужно понимать, что это не discount factor, а просто что-то похожее по смыслу. Такая политика мало отличается от оптимальной, а задача выбора гаммы выпуклая (около 0,99 будет достаточно, но лучше подобрать для каждого случая отдельно).<br><br><a href="https://arxiv.org/abs/2410.13461" rel="nofollow noopener noreferrer"><strong>Progressive Mixed-Precision Decoding for Efficient LLM Inference</strong></a><br><br>Квантизация, отмечают авторы, хороший способ снизить требования LLM к вычислительным мощностям. Однако применение низкой точности (2-3 бита) ведёт к сильному ухудшению качества. Авторы предлагают новый фазо-ориентированный метод, который избирательно распределяет точность между различными фазами инференса и вводят технику, позволяющую постепенно снижать точность по мере углубления в сгенерированную последовательность. <br><br>По сути для каждого токена выбирается битность. У авторов есть обучаемый шедулер, который предсказывает, когда надо переключаться на меньшую битность. Он очень чувствителен к гиперпараметрам, датасету и обучению. В будущем его хотят интегрировать внутрь самой LLM. А саму квантизацию наследуют из статьи <a href="https://dl.acm.org/doi/10.5555/3692070.3693677" rel="nofollow noopener noreferrer">Any-precision LLM</a>. В ней префикс веса нужной битности — это ключ в lookup-таблице весов, что позволяет не использовать дополнительную память под разные битности.<br><br><a href="https://arxiv.org/abs/2410.23771" rel="nofollow noopener noreferrer"><strong>What is Wrong with Perplexity for Long-context Language Modeling?</strong></a><br><br>Очень простая идея для длинного контекста — считать лосс в основном на key-токенах, где лосс, обусловленный на длинный контекст, сильно отличается от короткого. На long-бенче у авторов получилась значительная корреляция со скором по сравнению с обычной ppl, что немного смущает, и улучшения от такого тюна.<br><br><a href="https://arxiv.org/abs/2409.13156" rel="nofollow noopener noreferrer"><strong>RRM: Robust Reward Model Training Mitigates Reward Hacking</strong></a><br><br>Авторы заявляют, что современные реворд-модели не способны эффективно различать контекстуальные сигналы и нерелевантные артефакты при определении предпочтений. В статье предлагается обучать предпочтения, независимые от подобных артефактов, а также новая техника аугментации данных, специально разработанную для их устранения.<br><br>Авторы делают случайную перестановку датасета и расширяют его всеми возможными комбинациями i оригинальной тройки и sigma_i — тройки на позиции i после перестановки. Всего комбинаций 16 штук.<br><br>Победитель в полученных парах определяется так:<br><br>— если в паре один ответ на этот запрос, а второй от другого запроса, то побеждает всегда тот, который отвечает на «свой» запрос;<br>— если оба ответа от другого запроса, то это ничья.<br><br>Получается огромный датасет, в котором много тривиальных пар, где плохой ответ явно не от того запроса. Авторы фильтруют этот датасет с помощью предыдущей версии RM, оставляя только негативы и неуверенные.<br><br><a href="https://arxiv.org/abs/2504.09522" rel="nofollow noopener noreferrer"><strong>How new data permeates LLM knowledge and how to dilute it</strong></a><br><br>Авторы показывают, что при усвоении новой информации LLM проявляют эффект «прайминга»: изучение нового факта может привести к тому, что модель начнёт некорректно применять это знание в несвязанных контекстах. Чтобы это исправить, предлагают игнорировать самые большие градиенты — то есть не обновлять тот процент весов, который получил бы самый большой градиентный апдейт.<br><br><em>Интересные постеры увидели </em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> <em>Павел Темирчев, Екатерина Редина, Роман Горб, Степан Каргалицев</em><br><br>#YaICLR</div>
      <div class="actions">
        <span>5 852 просмотров · 27 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/99" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/99.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="98" data-search="нет, это не футуристическая оранжерея, а аэропорт чанги в сингапуре, где завтра стартует конференция iclr 2025. наша команда уже на месте и скоро начнёт рассказывать о том, что происходит на мероприятии. будет интересно и солнечно! нет, это не футуристическая оранжерея, а аэропорт чанги в сингапуре, где завтра стартует конференция iclr 2025. наша команда уже на месте и скоро начнёт рассказывать о том, что происходит на мероприятии. будет интересно и солнечно!">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-23T15:27:13+00:00" href="./posts/98.html">2025-04-23 15:27 UTC</a></div>
      </div>
      <div class="post-body">Нет, это не футуристическая оранжерея, а аэропорт Чанги в Сингапуре, где завтра стартует конференция ICLR 2025. Наша команда уже на месте и скоро начнёт рассказывать о том, что происходит на мероприятии. Будет интересно и солнечно!<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/98_480.webp" srcset="../assets/media/thumbs/98_480.webp 480w, ../assets/media/98.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="98" data-image-index="0" /></div></div>
      <div class="actions">
        <span>3 058 просмотров · 64 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/98" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/98.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="97" data-search="large language diffusion models. часть 2/2 продолжаем разбирать, что внутри у китайской модели llada (начинали вот тут). обучение (иллюстрации (a) и (b)) диффузия, как известно, учится восстанавливать объекты из шума. и llada — не исключение. для каждого батча обучения она сперва генерирует долю токенов t (от 0 до 1), которые хотим зашумить. а затем маскирует токены в батче с этой вероятностью. далее модель обучается восстанавливать замаскированные токены. стадия предобучения и sft отличаются лишь тем, что в sft зашумляется только ответ, но не запрос. чтобы модель умела восстанавливать последовательности разной длины, в обучение специально подкладывается 1% текстов с длинами от 1 до 4096 токенов. генерация (иллюстрация (c)) модель начинает генерацию ответа с запроса и полностью замаскированного ответа — такое состояние соответствует моменту времени t = 1 (начальной стадии восстановления текста). на каждом шаге генерации все замаскированные токены восстанавливаются одним проходом модели (токены выбираются жадно). а затем часть предсказанных токенов вновь маскируется с вероятностью t. t постепенно уменьшается до тех пор, пока не дойдёт до 0. итеративный подход предсказал — зашумил позволяет модели лучше обдумать, что именно она собирается генерировать. также авторы хорошо отзываются о подходе, где маскирование предсказанных токенов происходит не случайно, с какой-то вероятностью, а детерминировано — маскируется доля t токенов, в которых модель наименее уверена. этот подход к генерации также совместим с classifier-free guidance, что не может не радовать. llada — далеко не первая модель, основанная на masked language modelling. хотя авторы и не предложили миру радикально новый подход, простота и изящность идеи позволила им догнать и перегнать весьма сильные авторегрессионные бейзлайны: llama 2 и 3. разбор подготовил ❣ cydoroga душный nlp large language diffusion models. часть 2/2 продолжаем разбирать, что внутри у китайской модели llada ( начинали вот тут ). обучение ( иллюстрации (a) и (b) ) диффузия, как известно, учится восстанавливать объекты из шума. и llada — не исключение. для каждого батча обучения она сперва генерирует долю токенов t (от 0 до 1), которые хотим зашумить. а затем маскирует токены в батче с этой вероятностью. далее модель обучается восстанавливать замаскированные токены. стадия предобучения и sft отличаются лишь тем, что в sft зашумляется только ответ, но не запрос. чтобы модель умела восстанавливать последовательности разной длины, в обучение специально подкладывается 1% текстов с длинами от 1 до 4096 токенов. генерация ( иллюстрация (c) ) модель начинает генерацию ответа с запроса и полностью замаскированного ответа — такое состояние соответствует моменту времени t = 1 (начальной стадии восстановления текста). на каждом шаге генерации все замаскированные токены восстанавливаются одним проходом модели (токены выбираются жадно). а затем часть предсказанных токенов вновь маскируется с вероятностью t. t постепенно уменьшается до тех пор, пока не дойдёт до 0. итеративный подход предсказал — зашумил позволяет модели лучше обдумать, что именно она собирается генерировать. также авторы хорошо отзываются о подходе, где маскирование предсказанных токенов происходит не случайно, с какой-то вероятностью, а детерминировано — маскируется доля t токенов, в которых модель наименее уверена. этот подход к генерации также совместим с classifier-free guidance, что не может не радовать. llada — далеко не первая модель, основанная на masked language modelling. хотя авторы и не предложили миру радикально новый подход, простота и изящность идеи позволила им догнать и перегнать весьма сильные авторегрессионные бейзлайны: llama 2 и 3. разбор подготовил ❣ cydoroga душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-21T08:14:53+00:00" href="./posts/97.html">2025-04-21 08:14 UTC</a></div>
      </div>
      <div class="post-body"><strong>Large Language Diffusion Models. Часть 2/2</strong><br><br>Продолжаем разбирать, что внутри у <a href="https://arxiv.org/abs/2502.09992" rel="nofollow noopener noreferrer">китайской модели LLaDA</a> (<a href="https://t.me/stuffyNLP/96" rel="nofollow noopener noreferrer">начинали вот тут</a>).<br><br><strong>Обучение (<em>иллюстрации (a) и (b)</em>)</strong><br><br>Диффузия, как известно, учится восстанавливать объекты из шума. И LLaDA — не исключение. Для каждого батча обучения она сперва генерирует долю токенов t (от 0 до 1), которые хотим зашумить. А затем маскирует токены в батче с этой вероятностью. <br><br>Далее модель обучается восстанавливать замаскированные токены. Стадия предобучения и SFT отличаются лишь тем, что в SFT зашумляется только ответ, но не запрос. Чтобы модель умела восстанавливать последовательности разной длины, в обучение специально подкладывается 1% текстов с длинами от 1 до 4096 токенов.<br><br><strong>Генерация (<em>иллюстрация (c)</em>)</strong><br><br>Модель начинает генерацию ответа с запроса и полностью замаскированного ответа — такое состояние соответствует моменту времени t = 1 (начальной стадии восстановления текста). На каждом шаге генерации все замаскированные токены восстанавливаются одним проходом модели (токены выбираются жадно). А затем часть предсказанных токенов вновь маскируется с вероятностью t. <br><br>t постепенно уменьшается до тех пор, пока не дойдёт до 0. Итеративный подход предсказал — зашумил позволяет модели лучше обдумать, что именно она собирается генерировать.<br><br>Также авторы хорошо отзываются о подходе, где маскирование предсказанных токенов происходит не случайно, с какой-то вероятностью, а детерминировано — маскируется доля t токенов, в которых модель наименее уверена. Этот подход к генерации также совместим с classifier-free guidance, что не может не радовать.<br><br>LLaDA — далеко не первая модель, основанная на Masked Language Modelling. Хотя авторы и не предложили миру радикально новый подход, простота и изящность идеи позволила им догнать и перегнать весьма сильные авторегрессионные бейзлайны: LLaMA 2 и 3.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Cydoroga</em><br><br><a href="https://t.me/+2ZdPpAU938oxNDAy" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/97_480.webp" srcset="../assets/media/thumbs/97_480.webp 480w, ../assets/media/97.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="97" data-image-index="0" /></div></div>
      <div class="actions">
        <span>3 729 просмотров · 28 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/97" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/97.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="96" data-search="large language diffusion models. часть 1/2 сегодня разберём статью о новой диффузионной модели от китайских коллег. модель относится к классу так называемых дискретных диффузий и очень похожа на bert в режиме masked language modelling. авторы называют свою разработку llada. на графике — диаграмма сравнения новой модели (красная кривая) с llama 3 8b (фиолетовая кривая) и llama 2 7b (синяя кривая). к скейлу осей есть вопросы: ∙ там, где llada показывает лучшие результаты — разница выглядит значительной; ∙ там, где llada хуже, — различия выглядят несущественными. возможно, на площади под кривыми смотреть бессмысленно и график не очень репрезентативный. но с долей критики по нему вполне можно ориентироваться в сильных и слабых сторонах новой модели. в языковом моделировании уже давно правит бал классическая авторегрессия, где каждый последующий токен моделируется вероятностным распределением и обусловлен на контекст. но такой подход не лишён недостатков: если первый токен, который породила модель, оказался не самым удачным, исправить ошибку уже не получится — модель продолжит генерировать следующие токены, оглядываясь на первый, и испортит весь ответ. бороться с этой проблемой можно, например, с помощью chain-of-thoughts. но существует и ортогональное решение — использовать диффузионный подход с некаузальной маской аттеншна. как? читайте в следующей части разбора. разбор подготовил ❣ cydoroga душный nlp large language diffusion models. часть 1/2 сегодня разберём статью о новой диффузионной модели от китайских коллег. модель относится к классу так называемых дискретных диффузий и очень похожа на bert в режиме masked language modelling. авторы называют свою разработку llada. на графике — диаграмма сравнения новой модели ( красная кривая ) с llama 3 8b ( фиолетовая кривая ) и llama 2 7b ( синяя кривая ). к скейлу осей есть вопросы: ∙ там, где llada показывает лучшие результаты — разница выглядит значительной; ∙ там, где llada хуже, — различия выглядят несущественными. возможно, на площади под кривыми смотреть бессмысленно и график не очень репрезентативный. но с долей критики по нему вполне можно ориентироваться в сильных и слабых сторонах новой модели. в языковом моделировании уже давно правит бал классическая авторегрессия, где каждый последующий токен моделируется вероятностным распределением и обусловлен на контекст. но такой подход не лишён недостатков: если первый токен, который породила модель, оказался не самым удачным, исправить ошибку уже не получится — модель продолжит генерировать следующие токены, оглядываясь на первый, и испортит весь ответ. бороться с этой проблемой можно, например, с помощью chain-of-thoughts. но существует и ортогональное решение — использовать диффузионный подход с некаузальной маской аттеншна. как? читайте в следующей части разбора. разбор подготовил ❣ cydoroga душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-14T10:48:25+00:00" href="./posts/96.html">2025-04-14 10:48 UTC</a></div>
      </div>
      <div class="post-body"><strong>Large Language Diffusion Models. Часть 1/2</strong><br><br>Сегодня разберём <a href="https://arxiv.org/abs/2502.09992" rel="nofollow noopener noreferrer">статью</a> о новой диффузионной модели от китайских коллег. Модель относится к классу так называемых дискретных диффузий и очень похожа на BERT в режиме Masked Language Modelling. <br><br>Авторы называют свою разработку LLaDA. На графике — диаграмма сравнения новой модели (<em>красная кривая</em>) с LLaMA 3 8B (<em>фиолетовая кривая</em>) и LLaMA 2 7B (<em>синяя кривая</em>). К скейлу осей есть вопросы: <br><br>∙ там, где LLaDA показывает лучшие результаты — разница выглядит значительной;<br>∙ там, где LLaDA хуже, — различия выглядят несущественными. <br><br>Возможно, на площади под кривыми смотреть бессмысленно и график не очень репрезентативный. Но с долей критики по нему вполне можно ориентироваться в сильных и слабых сторонах новой модели.<br><br>В языковом моделировании уже давно правит бал классическая авторегрессия, где каждый последующий токен моделируется вероятностным распределением и обусловлен на контекст. Но такой подход не лишён недостатков: если первый токен, который породила модель, оказался не самым удачным, исправить ошибку уже не получится — модель продолжит генерировать следующие токены, оглядываясь на первый, и испортит весь ответ. <br><br>Бороться с этой проблемой можно, например, с помощью chain-of-thoughts. Но существует и ортогональное решение — использовать диффузионный подход с некаузальной маской аттеншна.<br><br>Как? Читайте в следующей части разбора. <br><br>Разбор подготовил <tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> Cydoroga<br><br><a href="https://t.me/+rGYdUleMERVkZTUy" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/96_480.webp" srcset="../assets/media/thumbs/96_480.webp 480w, ../assets/media/96.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="96" data-image-index="0" /></div></div>
      <div class="actions">
        <span>4 332 просмотров · 47 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/96" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/96.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="95" data-search="infalign: алайнмент языковых моделей с учётом процедуры инференса метод rlhf (reinforcement learning from human feedback) доказал эффективность в задаче алайнмента языковых моделей. однако у него есть существенный недостаток: на практике возникает расхождение между процессом обучения и реальным использованием модели. например, после rlhf модель обычно старается избегать неверных ответов. но при использовании стратегии генерации best-of-n (выбор лучшего из нескольких сгенерированных ответов) такое жёсткое ограничение становится неоптимальным — модель могла бы давать лучшие ответы, разреши мы ей экспериментировать более агрессивно за счёт небольшой доли неверных ответов. для решения этого несоответствия авторы статьи разработали метод infalign, адаптирующий процесс обучения к конкретным процедурам генерации, используемым на практике. рассмотрим проблему детальнее. классический подход rlhf с учётом kl-регуляризации гарантирует оптимальность модели по средней награде, если ответы генерируются сэмплированием. на практике, однако, нам интересна не столько средняя награда, сколько доля запросов, на которых новая модель лучше старой. и уже для такой метрики (при фиксированной модели, по отношению к которой мы считаем винрейт) rlhf даёт субоптимальные результаты даже для простого сэмплирования — что уж говорить о более продвинутых методах. к счастью, авторам статьи удалось доказать, что оптимизация винрейта для некоторых процедур генерации, включая best-of-n, worst-of-n и сэмплирование, эквивалентна применению rlhf с модифицированной функцией награды. предложенный подход состоит из трёх основных этапов. 1. калибровка награды. на этом этапе исходные награды преобразуются в значения от 0 до 1 таким образом, чтобы распределение наград ответов модели стало равномерным на каждом запросе. это эквивалентно применению обусловленной на запрос функции распределения награды к самой награде. забавно, что в первой версии статьи авторы предложили использовать медианную аппроксимацию функции распределения, однако спустя месяц удалили все упоминания об этом методе и перешли к использованию эмпирической функции распределения. 2. трансформация награды. на следующем этапе откалиброванная награда адаптируется под конкретную процедуру генерации. например, для стратегии best-of-n применяется экспоненциальное преобразование, усиливающее различия между отличными и посредственными ответами, а для сэмплирования — логарифм, штрафующий за плохие ответы. заметим, что на самом деле логарифм и экспонента — это лишь хорошие приближения оптимального преобразования. но, как показывают эксперименты, погрешностью можно пренебречь ради простоты реализации. 3. обучение с модифицированной наградой. модель обучается при помощи классического rlhf, используя модифицированную награду, адаптированную под конкретную процедуру генерации. проведённые авторами эксперименты показали, что предложенный подход демонстрирует значительное улучшение качества генерации с точки зрения винрейта даже для процедуры сэмплирования. отметим, что сейчас метод infalign применим к весьма ограниченному набору реально используемых процедур генерации, таких как best-of-n, worst-of-n и сэмплирования. разбор подготовил ❣ федор лебедь душный nlp infalign: алайнмент языковых моделей с учётом процедуры инференса метод rlhf (reinforcement learning from human feedback) доказал эффективность в задаче алайнмента языковых моделей. однако у него есть существенный недостаток: на практике возникает расхождение между процессом обучения и реальным использованием модели. например, после rlhf модель обычно старается избегать неверных ответов. но при использовании стратегии генерации best-of-n (выбор лучшего из нескольких сгенерированных ответов) такое жёсткое ограничение становится неоптимальным — модель могла бы давать лучшие ответы, разреши мы ей экспериментировать более агрессивно за счёт небольшой доли неверных ответов. для решения этого несоответствия авторы статьи разработали метод infalign, адаптирующий процесс обучения к конкретным процедурам генерации, используемым на практике. рассмотрим проблему детальнее. классический подход rlhf с учётом kl-регуляризации гарантирует оптимальность модели по средней награде, если ответы генерируются сэмплированием. на практике, однако, нам интересна не столько средняя награда, сколько доля запросов, на которых новая модель лучше старой. и уже для такой метрики (при фиксированной модели, по отношению к которой мы считаем винрейт) rlhf даёт субоптимальные результаты даже для простого сэмплирования — что уж говорить о более продвинутых методах. к счастью, авторам статьи удалось доказать, что оптимизация винрейта для некоторых процедур генерации, включая best-of-n, worst-of-n и сэмплирование, эквивалентна применению rlhf с модифицированной функцией награды. предложенный подход состоит из трёх основных этапов. 1. калибровка награды. на этом этапе исходные награды преобразуются в значения от 0 до 1 таким образом, чтобы распределение наград ответов модели стало равномерным на каждом запросе. это эквивалентно применению обусловленной на запрос функции распределения награды к самой награде. забавно, что в первой версии статьи авторы предложили использовать медианную аппроксимацию функции распределения, однако спустя месяц удалили все упоминания об этом методе и перешли к использованию эмпирической функции распределения. 2. трансформация награды. на следующем этапе откалиброванная награда адаптируется под конкретную процедуру генерации. например, для стратегии best-of-n применяется экспоненциальное преобразование, усиливающее различия между отличными и посредственными ответами, а для сэмплирования — логарифм, штрафующий за плохие ответы. заметим, что на самом деле логарифм и экспонента — это лишь хорошие приближения оптимального преобразования. но, как показывают эксперименты, погрешностью можно пренебречь ради простоты реализации. 3. обучение с модифицированной наградой. модель обучается при помощи классического rlhf, используя модифицированную награду, адаптированную под конкретную процедуру генерации. проведённые авторами эксперименты показали, что предложенный подход демонстрирует значительное улучшение качества генерации с точки зрения винрейта даже для процедуры сэмплирования. отметим, что сейчас метод infalign применим к весьма ограниченному набору реально используемых процедур генерации, таких как best-of-n, worst-of-n и сэмплирования. разбор подготовил ❣ федор лебедь душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-02T10:00:20+00:00" href="./posts/95.html">2025-04-02 10:00 UTC</a></div>
      </div>
      <div class="post-body"><strong>InfAlign: алайнмент языковых моделей с учётом процедуры инференса</strong><br><br>Метод RLHF (Reinforcement Learning from Human Feedback) доказал эффективность в задаче алайнмента языковых моделей. Однако у него есть существенный недостаток: на практике возникает расхождение между процессом обучения и реальным использованием модели.<br><br>Например, после RLHF модель обычно старается избегать неверных ответов. Но при использовании стратегии генерации Best-of-N (выбор лучшего из нескольких сгенерированных ответов) такое жёсткое ограничение становится неоптимальным — модель могла бы давать лучшие ответы, разреши мы ей экспериментировать более агрессивно за счёт небольшой доли неверных ответов.<br><br>Для решения этого несоответствия авторы <a href="https://arxiv.org/abs/2412.19792v3" rel="nofollow noopener noreferrer">статьи </a>разработали метод InfAlign, адаптирующий процесс обучения к конкретным процедурам генерации, используемым на практике.<br><br>Рассмотрим проблему детальнее. Классический подход RLHF с учётом KL-регуляризации гарантирует оптимальность модели по средней награде, если ответы генерируются сэмплированием. На практике, однако, нам интересна не столько средняя награда, сколько доля запросов, на которых новая модель лучше старой. И уже для такой метрики (при фиксированной модели, по отношению к которой мы считаем винрейт) RLHF даёт субоптимальные результаты даже для простого сэмплирования — что уж говорить о более продвинутых методах.<br><br>К счастью, авторам статьи удалось доказать, что оптимизация винрейта для некоторых процедур генерации, включая Best-of-N, Worst-of-N и сэмплирование, эквивалентна применению RLHF с модифицированной функцией награды.<br><br>Предложенный подход состоит из трёх основных этапов.<br><br><strong>1.</strong> <strong>Калибровка награды. </strong>На этом этапе исходные награды преобразуются в значения от 0 до 1 таким образом, чтобы распределение наград ответов модели стало равномерным на каждом запросе. Это эквивалентно применению обусловленной на запрос функции распределения награды к самой награде. Забавно, что в первой версии статьи авторы предложили использовать медианную аппроксимацию функции распределения, однако спустя месяц удалили все упоминания об этом методе и перешли к использованию эмпирической функции распределения.<br><br><strong>2. Трансформация награды. </strong>На следующем этапе откалиброванная награда адаптируется под конкретную процедуру генерации. Например, для стратегии Best-of-N применяется экспоненциальное преобразование, усиливающее различия между отличными и посредственными ответами, а для сэмплирования — логарифм, штрафующий за плохие ответы. Заметим, что на самом деле логарифм и экспонента — это лишь хорошие приближения оптимального преобразования. Но, как показывают эксперименты, погрешностью можно пренебречь ради простоты реализации.<br><br><strong>3. Обучение с модифицированной наградой. </strong>Модель обучается при помощи классического RLHF, используя модифицированную награду, адаптированную под конкретную процедуру генерации.<br><br>Проведённые авторами эксперименты показали, что предложенный подход демонстрирует значительное улучшение качества генерации с точки зрения винрейта даже для процедуры сэмплирования.<br><br>Отметим, что сейчас метод InfAlign применим к весьма ограниченному набору реально используемых процедур генерации, таких как Best-of-N, Worst-of-N и сэмплирования.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Федор Лебедь</em><br><br><a href="https://t.me/+wYVdm6tUpYhlZmFi" rel="nofollow noopener noreferrer">Душный NLP</a></div>
      <div class="actions">
        <span>4 265 просмотров · 32 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/95" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/95.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="94" data-search="yandexgpt 5 lite instruct теперь в опенсорсе 🎉 в феврале в открытый доступ вышла pretrain-версия, а сейчас очередь дошла и до yandexgpt 5 lite instruct. это модель на 8 миллиардов параметров с размером контекстного окна в 32к токенов. о претрейне мы уже писали вот тут, а алайнмент аналогичен тому, через который проходит yandexgpt 5 pro. на этапе sft концентрировались на сложных запросах, а также методах фильтрации и ранжирования данных. в рамках rlhf комбинировали rl-подходы, которые дают лучшие результаты: dpo, logdpo и ppo. подробнее об этом читайте на хабре. по результатам внутреннего слепого попарного сравнения (side-by-side) новая модель yandexgpt 5 lite превосходит qwen-2.5-7b-instruct в 62% случаев и не уступает gpt-4o mini в решении стандартных задач сервисов яндекса. показатели бенчмарков можно посмотреть в таблице. а ещё обновили лицензию: теперь можно использовать модель не только в некоммерческих целях, но и в коммерческих до 10 миллионов выходных токенов в месяц. если ваши объёмы выше, напишите на почту, указанную в тексте лицензии. модель доступна на hugging face. там же есть и квантизованная версия с поддержкой gguf. yandexgpt 5 lite instruct совместима с llama.cpp и ollama. ml underhood yandexgpt 5 lite instruct теперь в опенсорсе 🎉 в феврале в открытый доступ вышла pretrain-версия, а сейчас очередь дошла и до yandexgpt 5 lite instruct. это модель на 8 миллиардов параметров с размером контекстного окна в 32к токенов. о претрейне мы уже писали вот тут , а алайнмент аналогичен тому, через который проходит yandexgpt 5 pro. на этапе sft концентрировались на сложных запросах, а также методах фильтрации и ранжирования данных. в рамках rlhf комбинировали rl-подходы, которые дают лучшие результаты: dpo, logdpo и ppo. подробнее об этом читайте на хабре . по результатам внутреннего слепого попарного сравнения (side-by-side) новая модель yandexgpt 5 lite превосходит qwen-2.5-7b-instruct в 62% случаев и не уступает gpt-4o mini в решении стандартных задач сервисов яндекса. показатели бенчмарков можно посмотреть в таблице. а ещё обновили лицензию: теперь можно использовать модель не только в некоммерческих целях, но и в коммерческих до 10 миллионов выходных токенов в месяц. если ваши объёмы выше, напишите на почту, указанную в тексте лицензии. модель доступна на hugging face . там же есть и квантизованная версия с поддержкой gguf . yandexgpt 5 lite instruct совместима с llama.cpp и ollama. ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-03-31T09:00:42+00:00" href="./posts/94.html">2025-03-31 09:00 UTC</a></div>
      </div>
      <div class="post-body"><strong>YandexGPT 5 Lite Instruct теперь в опенсорсе 🎉</strong><br><br>В феврале в открытый доступ вышла Pretrain-версия, а сейчас очередь дошла и до YandexGPT 5 Lite Instruct. Это модель на 8 миллиардов параметров с размером контекстного окна в 32К токенов. <br><br>О претрейне мы уже писали <a href="https://t.me/MLunderhood/70" rel="nofollow noopener noreferrer">вот тут</a>, а алайнмент аналогичен тому, через который проходит YandexGPT 5 Pro. На этапе SFT концентрировались на сложных запросах, а также методах фильтрации и ранжирования данных. В рамках RLHF комбинировали RL-подходы, которые дают лучшие результаты: DPO, LogDPO и PPO. Подробнее об этом <a href="https://habr.com/ru/companies/yandex/articles/885218/" rel="nofollow noopener noreferrer">читайте на Хабре</a>.<br><br>По результатам внутреннего слепого попарного сравнения (side-by-side) новая модель YandexGPT 5 Lite превосходит Qwen-2.5-7B-instruct в 62% случаев и не уступает GPT-4o mini в решении стандартных задач сервисов Яндекса. Показатели бенчмарков можно посмотреть в таблице.<br><br>А ещё обновили лицензию: теперь можно использовать модель не только в некоммерческих целях, но и в коммерческих до 10 миллионов выходных токенов в месяц. Если ваши объёмы выше, напишите на почту, указанную в тексте лицензии.<br><br>Модель доступна на <a href="https://huggingface.co/yandex/YandexGPT-5-Lite-8B-instruct" rel="nofollow noopener noreferrer">Hugging Face</a>. Там же есть и <a href="https://huggingface.co/yandex/YandexGPT-5-Lite-8B-instruct-GGUF" rel="nofollow noopener noreferrer">квантизованная версия с поддержкой GGUF</a><a href="https://huggingface.co/yandex/YandexGPT-5-Lite-8B-pretrain-GGUF" rel="nofollow noopener noreferrer">.</a> YandexGPT 5 Lite Instruct совместима с llama.cpp и Ollama. <br><br><a href="https://t.me/+x1iAxBkfmVZjNzIy" rel="nofollow noopener noreferrer">ML Underhood</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/94_480.webp" srcset="../assets/media/thumbs/94_480.webp 480w, ../assets/media/94.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="94" data-image-index="0" /></div></div>
      <div class="actions">
        <span>3 181 просмотров · 48 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/94" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/94.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="93" data-search="вслед за претрейн-версией yandexgpt 5 lite в опенсорс вышла и instruct-модель. в канале ml underhood — на который, к слову, рекомендуем подписаться — рассказываем главное о релизе. вслед за претрейн-версией yandexgpt 5 lite в опенсорс вышла и instruct-модель. в канале ml underhood — на который, к слову, рекомендуем подписаться — рассказываем главное о релизе.">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-03-31T09:00:42+00:00" href="./posts/93.html">2025-03-31 09:00 UTC</a></div>
      </div>
      <div class="post-body">Вслед за претрейн-версией YandexGPT 5 Lite в опенсорс вышла и Instruct-модель. В канале ML Underhood — на который, к слову, рекомендуем подписаться — рассказываем главное о релизе.</div>
      <div class="actions">
        <span>3 146 просмотров</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/93" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/93.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="90" data-search="метод борьбы с likelihood displacement в dpo датасет для direct preference optimization (dpo) состоит из инструкции, а также двух ответов: негативного — его хотим разучить — и позитивного, который мы хотим чаще получать. likelihood displacement — это явление, при котором модель разучивает оба варианта. о методе преодоления этой проблемы сегодняшняя статья. в своей работе авторы использовали датасет persona, промпты в котором сформулированны как вопросы вида «мог бы ты сказать следующее:...» (“is the following statement something you would say? [statement]”). то есть модели нужно было согласиться или не согласиться с утверждением, ответив «да», «нет», «никогда» или «возможно». эксперименты показали, что при попытках научить модель отвечать отрицательно, но не категорично («никогда» считался негативным вариантом на dpo, а «нет» — позитивным), вероятность токена «да» становится больше вероятности «нет». подобное происходит только тогда, когда оба типа ответов похожи (изображение 1). авторы считают, что likelihood displacement происходит из-за анэмбеддинг-геометрии токенов. анэмбеддинг-матрица позитивного и негативного токенов — разница между wy+ и wy- — содержит в себе большую компоненту, ортогональную позитивному ответу, по которой можно выучить даже противоположный ответ. справиться с этой проблемой авторы предлагают с помощью метрики для оценки похожих ответов. чтобы её вывести, нужно взять суммы эмбеддингов всех токенов в позитивном ответе и негативном ответе, посчитать их скалярное произведение, а затем вычесть норму позитивного ответа. эта метрика зависит от длины ответов, поэтому авторы предлагают делить скалярное произведение на произведение длин позитивных и негативных ответов, а норму — на квадрат длины позитивных ответов (изображение 2). с помощью метрики, которую назвали centered hidden embedding similarity (ches), отфильтровали выборку ответов из датасета. для эксперимента использовали sorry-bench, призванный научить модель отказывать пользователю в исполнении неэтичных, токсичных или преступных запросов. использование ches показало хорошие результаты (голубой столбец на графике), однако после фильтрации в выборке осталось всего 5% сэмплов. кроме того, модели в сравнении обучались не одинаковое количество шагов, что могло повлиять на результаты тестов. разбор подготовил ❣ карим галлямов душный nlp метод борьбы с likelihood displacement в dpo датасет для direct preference optimization (dpo) состоит из инструкции, а также двух ответов: негативного — его хотим разучить — и позитивного, который мы хотим чаще получать. likelihood displacement — это явление, при котором модель разучивает оба варианта. о методе преодоления этой проблемы сегодняшняя статья. в своей работе авторы использовали датасет persona, промпты в котором сформулированны как вопросы вида «мог бы ты сказать следующее:...» (“is the following statement something you would say? [statement]”). то есть модели нужно было согласиться или не согласиться с утверждением, ответив «да», «нет», «никогда» или «возможно». эксперименты показали, что при попытках научить модель отвечать отрицательно, но не категорично («никогда» считался негативным вариантом на dpo, а «нет» — позитивным), вероятность токена «да» становится больше вероятности «нет». подобное происходит только тогда, когда оба типа ответов похожи ( изображение 1 ). авторы считают, что likelihood displacement происходит из-за анэмбеддинг-геометрии токенов. анэмбеддинг-матрица позитивного и негативного токенов — разница между wy+ и wy- — содержит в себе большую компоненту, ортогональную позитивному ответу, по которой можно выучить даже противоположный ответ. справиться с этой проблемой авторы предлагают с помощью метрики для оценки похожих ответов. чтобы её вывести, нужно взять суммы эмбеддингов всех токенов в позитивном ответе и негативном ответе, посчитать их скалярное произведение, а затем вычесть норму позитивного ответа. эта метрика зависит от длины ответов, поэтому авторы предлагают делить скалярное произведение на произведение длин позитивных и негативных ответов, а норму — на квадрат длины позитивных ответов ( изображение 2 ). с помощью метрики, которую назвали centered hidden embedding similarity (ches), отфильтровали выборку ответов из датасета. для эксперимента использовали sorry-bench , призванный научить модель отказывать пользователю в исполнении неэтичных, токсичных или преступных запросов. использование ches показало хорошие результаты ( голубой столбец на графике ), однако после фильтрации в выборке осталось всего 5% сэмплов. кроме того, модели в сравнении обучались не одинаковое количество шагов, что могло повлиять на результаты тестов. разбор подготовил ❣ карим галлямов душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-03-25T11:37:26+00:00" href="./posts/90.html">2025-03-25 11:37 UTC</a></div>
      </div>
      <div class="post-body"><strong>Метод борьбы с likelihood displacement в DPO</strong><br><br>Датасет для Direct Preference Optimization (DPO) состоит из инструкции, а также двух ответов: негативного — его хотим разучить — и позитивного, который мы хотим чаще получать. Likelihood displacement — это явление, при котором модель разучивает оба варианта. О методе преодоления этой проблемы <a href="https://arxiv.org/abs/2410.08847" rel="nofollow noopener noreferrer">сегодняшняя статья.</a> <br><br>В своей работе авторы использовали датасет Persona, промпты в котором сформулированны как вопросы вида «Мог бы ты сказать следующее:...» (“Is the following statement something you would say? [STATEMENT]”). То есть модели нужно было согласиться или не согласиться с утверждением, ответив «да», «нет», «никогда» или «возможно». Эксперименты показали, что при попытках научить модель отвечать отрицательно, но не категорично («никогда» считался негативным вариантом на DPO, а «нет» — позитивным), вероятность токена «да» становится больше вероятности «нет». Подобное происходит только тогда, когда оба типа ответов похожи (<em>изображение 1</em>). <br><br>Авторы считают, что likelihood displacement происходит из-за анэмбеддинг-геометрии токенов. Анэмбеддинг-матрица позитивного и негативного токенов — разница между Wy+ и Wy- — содержит в себе большую компоненту, ортогональную позитивному ответу, по которой можно выучить даже противоположный ответ. <br><br>Справиться с этой проблемой авторы предлагают с помощью метрики для оценки похожих ответов. Чтобы её вывести, нужно взять суммы эмбеддингов всех токенов в позитивном ответе и негативном ответе, посчитать их скалярное произведение, а затем вычесть норму позитивного ответа. Эта метрика зависит от длины ответов, поэтому авторы предлагают делить скалярное произведение на произведение длин позитивных и негативных ответов, а норму — на квадрат длины позитивных ответов (<em>изображение 2</em>).<br><br>С помощью метрики, которую назвали centered hidden embedding similarity (CHES), отфильтровали выборку ответов из датасета. Для эксперимента использовали <a href="https://arxiv.org/abs/2406.14598" rel="nofollow noopener noreferrer">SORRY-bench</a>, призванный научить модель отказывать пользователю в исполнении неэтичных, токсичных или преступных запросов. Использование CHES показало хорошие результаты (<em>голубой столбец на графике</em>), однако после фильтрации в выборке осталось всего 5% сэмплов. Кроме того, модели в сравнении обучались не одинаковое количество шагов, что могло повлиять на результаты тестов. <br><br><em>Разбор подготовил </em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> <em>Карим Галлямов</em><br><br><a href="https://t.me/+SAkuxkiRmTk5MTAy" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/90_480.webp" srcset="../assets/media/thumbs/90_480.webp 480w, ../assets/media/90.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="90" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/91_480.webp" srcset="../assets/media/thumbs/91_480.webp 480w, ../assets/media/91.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="90" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/92_480.webp" srcset="../assets/media/thumbs/92_480.webp 480w, ../assets/media/92.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="90" data-image-index="2" /></div></div>
      <div class="actions">
        <span>3 525 просмотров · 26 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/90" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/90.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="88" data-search="механизм аттеншена nsa native sparse attention (nsa) — механизм разреженного аттеншена от инженеров из deepseek. утверждается, что nsa имеет качество, сопоставимое с обычным аттеншном на маленьких контекстах, и значительно опережает его на больших — в статье сравнение производится на 64k токенов. вместо того чтобы каждый новый query обращался ко всем предыдущим key и value, как это делается в традиционном аттеншне, авторы предлагают сжимать предыдущие ключи и значения в dense-представления. за счёт этого длина последовательности, над которой работает attention, уменьшается, что позволяет работать с контекстом в 64k токенов так, будто их всего 4k. в отличие от предыдущих sparse-методов аттеншна (например, quest), nsa применяется как при обучении, так и при инференсе. в статье предлагают три функции для сжатия представления: token compression, token selection и sliding window. для каждой из них считается аттеншен, а результаты складываются с коэффициентами от mlp-блока. token compression предполагает покрытие последовательностей ключей и значений блоками длины 32 с перекрытием по 16 токенов с последующим сжатием каждого блока в «один токен» с помощью mlp с внутриблочным позиционным энкодингом. на стадии token selection тоже происходит покрытие ключей и значений блоками, но теперь для каждого из них считается скор полезности. после чего выбираются top-16 блоков с максимальным скором. на оставшиеся блоки аттеншн не смотрит, а в выбранных внимание обращается на все ключи и значения. авторы отмечают, что в начале обучения сильно доминировали локальные паттерны. поэтому selection и compression больше фокусировались на последовательностях ближе к текущему токену. в конце, на длинных контекстах, возникали сложности с аттеншеном на начало последовательности. чтобы решить эту проблему, предлагается дополнительно использовать sliding window для аттеншена на ближайшие 512 токенов. метод проверяли на moe-модели на 27b параметров, из которых 3b — активные. у модели было 30 слоёв аттеншена и 64 головы с разной размерностью. число экспертов — 72, из них общих — 2. обучение происходило на 270b токенов с размером контекстного окна в 8k токенов. далее был sft с использованием техники yarn. результаты тестов показали, что на бенчмарках, где длинный контекст не так важен — например, mmlu или humaneval — деградации качества от использования nsa не происходит. на longbench же nsa показывает качество в среднем на 10% лучше, чем full attention. например, на lcc, где требуется дополнить сниппет кода на основе очень длинного контекста, nsa побеждает 0,232 на 0,163. кроме того, есть ощутимый прирост в скорости — вплоть до 9 раз на форварде и 6 раз на бекварде при сравнении с flashattention 2. это стало возможно за счёт эффективного triton-кернела, кодом которого разработчики не делятся, но в open source уже началась работа по его воспроизведению. разбор подготовил ❣ владислав савинов душный nlp механизм аттеншена nsa native sparse attention (nsa) — механизм разреженного аттеншена от инженеров из deepseek. утверждается, что nsa имеет качество, сопоставимое с обычным аттеншном на маленьких контекстах, и значительно опережает его на больших — в статье сравнение производится на 64k токенов. вместо того чтобы каждый новый query обращался ко всем предыдущим key и value, как это делается в традиционном аттеншне, авторы предлагают сжимать предыдущие ключи и значения в dense-представления. за счёт этого длина последовательности, над которой работает attention, уменьшается, что позволяет работать с контекстом в 64k токенов так, будто их всего 4k. в отличие от предыдущих sparse-методов аттеншна (например, quest), nsa применяется как при обучении, так и при инференсе. в статье предлагают три функции для сжатия представления: token compression, token selection и sliding window. для каждой из них считается аттеншен, а результаты складываются с коэффициентами от mlp-блока. token compression предполагает покрытие последовательностей ключей и значений блоками длины 32 с перекрытием по 16 токенов с последующим сжатием каждого блока в «один токен» с помощью mlp с внутриблочным позиционным энкодингом. на стадии token selection тоже происходит покрытие ключей и значений блоками, но теперь для каждого из них считается скор полезности. после чего выбираются top-16 блоков с максимальным скором. на оставшиеся блоки аттеншн не смотрит, а в выбранных внимание обращается на все ключи и значения. авторы отмечают, что в начале обучения сильно доминировали локальные паттерны. поэтому selection и compression больше фокусировались на последовательностях ближе к текущему токену. в конце, на длинных контекстах, возникали сложности с аттеншеном на начало последовательности. чтобы решить эту проблему, предлагается дополнительно использовать sliding window для аттеншена на ближайшие 512 токенов. метод проверяли на moe-модели на 27b параметров, из которых 3b — активные. у модели было 30 слоёв аттеншена и 64 головы с разной размерностью. число экспертов — 72, из них общих — 2. обучение происходило на 270b токенов с размером контекстного окна в 8k токенов. далее был sft с использованием техники yarn. результаты тестов показали, что на бенчмарках, где длинный контекст не так важен — например, mmlu или humaneval — деградации качества от использования nsa не происходит. на longbench же nsa показывает качество в среднем на 10% лучше, чем full attention. например, на lcc, где требуется дополнить сниппет кода на основе очень длинного контекста, nsa побеждает 0,232 на 0,163. кроме того, есть ощутимый прирост в скорости — вплоть до 9 раз на форварде и 6 раз на бекварде при сравнении с flashattention 2. это стало возможно за счёт эффективного triton-кернела, кодом которого разработчики не делятся, но в open source уже началась работа по его воспроизведению. разбор подготовил ❣ владислав савинов душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-03-21T13:24:48+00:00" href="./posts/88.html">2025-03-21 13:24 UTC</a></div>
      </div>
      <div class="post-body"><strong>Механизм аттеншена NSA</strong><br><br>Native Sparse Attention (NSA) — механизм разреженного аттеншена от инженеров из DeepSeek. Утверждается, что NSA имеет качество, сопоставимое с обычным аттеншном на маленьких контекстах, и значительно опережает его на больших — в <a href="https://arxiv.org/abs/2502.11089" rel="nofollow noopener noreferrer">статье</a> сравнение производится на 64K токенов.<br><br>Вместо того чтобы каждый новый query обращался ко всем предыдущим key и value, как это делается в традиционном аттеншне, авторы предлагают сжимать предыдущие ключи и значения в dense-представления. За счёт этого длина последовательности, над которой работает attention, уменьшается, что позволяет работать с контекстом в 64K токенов так, будто их всего 4K. В отличие от предыдущих sparse-методов аттеншна (например, Quest), NSA применяется как при обучении, так и при инференсе.<br><br>В статье предлагают три функции для сжатия представления: token compression, token selection и sliding window. Для каждой из них считается аттеншен, а результаты складываются с коэффициентами от MLP-блока.<br><br><strong>Token compression</strong> предполагает покрытие последовательностей ключей и значений блоками длины 32 с перекрытием по 16 токенов с последующим сжатием каждого блока в «один токен» с помощью MLP с внутриблочным позиционным энкодингом.<br><br>На стадии <strong>token selection</strong> тоже происходит покрытие ключей и значений блоками, но теперь для каждого из них считается скор полезности. После чего выбираются top-16 блоков с максимальным скором. На оставшиеся блоки аттеншн не смотрит, а в выбранных внимание обращается на все ключи и значения.<br><br>Авторы отмечают, что в начале обучения сильно доминировали локальные паттерны. Поэтому selection и compression больше фокусировались на последовательностях ближе к текущему токену. В конце, на длинных контекстах, возникали сложности с аттеншеном на начало последовательности. Чтобы решить эту проблему, предлагается дополнительно использовать <strong>sliding window</strong> для аттеншена на ближайшие 512 токенов. <br><br>Метод проверяли на MoE-модели на 27B параметров, из которых 3B — активные. У модели было 30 слоёв аттеншена и 64 головы с разной размерностью. Число экспертов — 72, из них общих — 2. Обучение происходило на 270B токенов с размером контекстного окна в 8K токенов. Далее был SFT с использованием <a href="https://arxiv.org/abs/2309.00071" rel="nofollow noopener noreferrer">техники YaRN.</a> <br><br>Результаты тестов показали, что на бенчмарках, где длинный контекст не так важен — например, MMLU или HumanEval — деградации качества от использования NSA не происходит. На LongBench же NSA показывает качество в среднем на 10% лучше, чем Full Attention. Например, на LCC, где требуется дополнить сниппет кода на основе очень длинного контекста, NSA побеждает 0,232 на 0,163.<br><br>Кроме того, есть ощутимый прирост в скорости — вплоть до 9 раз на форварде и 6 раз на бекварде при сравнении с FlashAttention 2. Это стало возможно за счёт эффективного Triton-кернела, кодом которого разработчики не делятся, но в open source уже <a href="https://github.com/lucidrains/native-sparse-attention-pytorch" rel="nofollow noopener noreferrer">началась</a> работа по его воспроизведению.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Владислав Савинов</em><br><br><a href="https://t.me/+0R0jXGV73BYwZTQ6" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/88_480.webp" srcset="../assets/media/thumbs/88_480.webp 480w, ../assets/media/88.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="88" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/89_480.webp" srcset="../assets/media/thumbs/89_480.webp 480w, ../assets/media/89.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="88" data-image-index="1" /></div></div>
      <div class="actions">
        <span>4 426 просмотров · 48 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/88" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/88.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="87" data-search="genarm — метод потокенного реворда сегодня разберём простую, но интересную статью. авторы сделали потокенный реворд, чтобы использовать его в тест-тайме для генерации ответов. попыток использовать реворд для генерации ответов предпринималось немало. скажем, можно использовать обученный на полных ответах реворд на частях генерации. а можно считать награду, генерируя полный ответ для каждого следующего токена. у таких подходов есть минусы. в первом случае при генерации могут возникать неточности из-за того, что реворд обучался только на полных ответах, во втором случае — существенно возрастает «стоимость» инференса. решением проблем, по мнению авторов, могло бы стать использование суммы авторегрессионного rm-скоринга для каждого токена-кандидата и llm-скоринга. на основе полученных результатов и должен выбираться ответ. инженеры задались целью создать именно такой реворд. авторы взяли sft-модель и данные предпочтений (preference data) в виде пар. это инструкция и два ответа — победный и проигрышный, — размеченные людьми. реворд-модель обучается на этих парах с использованием негативного лосса. идея авторов статьи заключается в том, чтобы представить итоговый реворд как сумму потокенных вероятностей для каждого токена. то есть каждый следующий токен получает какую-то оценку, эти оценки складываются для получения итоговой награды. эту параметризацию подставляют в лосс, чтобы обучить реворд-модель. в тест-тайме авторы получают скоры для каждого токена по формуле, которая учитывает предсказания базовой и реворд-моделей. это намного эффективнее, чем генерировать целые тексты и прогонять их через реворд. можно также использовать несколько ревордов — например, когда ответ должен быть одновременно и полезным, и этичным, или когда нужно склонить генерацию в какую-либо сторону. для обоих показателей нужно натренировать отдельную реворд-модель. эксперименты показали, что метод, предложенный авторами, оказывается лучше, чем другие известные бейзлайны — например, args и transfer q — по качеству и скорости инференса. однако он уступает dpo, который намного более сложен и дорог в исполнении, чем genarm. ещё из интересного: авторы заметили, что маленькие модели могут выступать хорошим ревордом у крупных при использовании genarm. эксперименты проводили на tulu2 с числом параметров 7b, 12b и 70b. и в этом случае метод из статьи превзошёл всё, кроме dpo. разбор подготовил ❣ илья черемушкин душный nlp genarm — метод потокенного реворда сегодня разберём простую, но интересную статью . авторы сделали потокенный реворд, чтобы использовать его в тест-тайме для генерации ответов. попыток использовать реворд для генерации ответов предпринималось немало. скажем, можно использовать обученный на полных ответах реворд на частях генерации. а можно считать награду, генерируя полный ответ для каждого следующего токена. у таких подходов есть минусы. в первом случае при генерации могут возникать неточности из-за того, что реворд обучался только на полных ответах, во втором случае — существенно возрастает «стоимость» инференса. решением проблем, по мнению авторов, могло бы стать использование суммы авторегрессионного rm-скоринга для каждого токена-кандидата и llm-скоринга. на основе полученных результатов и должен выбираться ответ. инженеры задались целью создать именно такой реворд. авторы взяли sft-модель и данные предпочтений (preference data) в виде пар. это инструкция и два ответа — победный и проигрышный, — размеченные людьми. реворд-модель обучается на этих парах с использованием негативного лосса. идея авторов статьи заключается в том, чтобы представить итоговый реворд как сумму потокенных вероятностей для каждого токена. то есть каждый следующий токен получает какую-то оценку, эти оценки складываются для получения итоговой награды. эту параметризацию подставляют в лосс, чтобы обучить реворд-модель. в тест-тайме авторы получают скоры для каждого токена по формуле, которая учитывает предсказания базовой и реворд-моделей. это намного эффективнее, чем генерировать целые тексты и прогонять их через реворд. можно также использовать несколько ревордов — например, когда ответ должен быть одновременно и полезным, и этичным, или когда нужно склонить генерацию в какую-либо сторону. для обоих показателей нужно натренировать отдельную реворд-модель. эксперименты показали, что метод, предложенный авторами, оказывается лучше, чем другие известные бейзлайны — например, args и transfer q — по качеству и скорости инференса. однако он уступает dpo, который намного более сложен и дорог в исполнении, чем genarm. ещё из интересного: авторы заметили, что маленькие модели могут выступать хорошим ревордом у крупных при использовании genarm. эксперименты проводили на tulu2 с числом параметров 7b, 12b и 70b. и в этом случае метод из статьи превзошёл всё, кроме dpo. разбор подготовил ❣ илья черемушкин душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-03-18T07:30:00+00:00" href="./posts/87.html">2025-03-18 07:30 UTC</a></div>
      </div>
      <div class="post-body"><strong>GenARM — метод потокенного реворда</strong><br><br>Сегодня разберём простую, но <a href="https://arxiv.org/abs/2410.08193" rel="nofollow noopener noreferrer">интересную статью</a>. Авторы сделали потокенный реворд, чтобы использовать его в тест-тайме для генерации ответов. <br><br>Попыток использовать реворд для генерации ответов предпринималось немало. Скажем, можно использовать обученный на полных ответах реворд на частях генерации. А можно считать награду, генерируя полный ответ для каждого следующего токена. У таких подходов есть минусы. В первом случае при генерации могут возникать неточности из-за того, что реворд обучался только на полных ответах, во втором случае — существенно возрастает «стоимость» инференса.<br><br>Решением проблем, по мнению авторов, могло бы стать использование суммы авторегрессионного RM-скоринга для каждого токена-кандидата и LLM-скоринга. На основе полученных результатов и должен выбираться ответ. Инженеры задались целью создать именно такой реворд.<br><br>Авторы взяли SFT-модель и данные предпочтений (preference data) в виде пар. Это инструкция и два ответа — победный и проигрышный, — размеченные людьми. Реворд-модель обучается на этих парах с использованием негативного лосса. <br><br>Идея авторов статьи заключается в том, чтобы представить итоговый реворд как сумму потокенных вероятностей для каждого токена. То есть каждый следующий токен получает какую-то оценку, эти оценки складываются для получения итоговой награды. Эту параметризацию подставляют в лосс, чтобы обучить реворд-модель.<br><br>В тест-тайме авторы получают скоры для каждого токена по формуле, которая учитывает предсказания базовой и реворд-моделей. Это намного эффективнее, чем генерировать целые тексты и прогонять их через реворд. <br><br>Можно также использовать несколько ревордов — например, когда ответ должен быть одновременно и полезным, и этичным, или когда нужно склонить генерацию в какую-либо сторону. Для обоих показателей нужно натренировать отдельную реворд-модель.<br><br>Эксперименты показали, что метод, предложенный авторами, оказывается лучше, чем другие известные бейзлайны — например, <a href="https://openreview.net/forum?id=shgx0eqdw6" rel="nofollow noopener noreferrer">ARGS</a> и <a href="https://arxiv.org/abs/2405.20495" rel="nofollow noopener noreferrer">Transfer Q</a> — по качеству и скорости инференса. Однако он уступает DPO, который намного более сложен и дорог в исполнении, чем GenARM.<br><br>Ещё из интересного: авторы заметили, что маленькие модели могут выступать хорошим ревордом у крупных при использовании GenARM. Эксперименты проводили на Tulu2 с числом параметров 7B, 12B и 70B. И в этом случае метод из статьи превзошёл всё, кроме DPO. <br><br><em>Разбор подготовил </em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> <em>Илья Черемушкин</em><br><br><a href="https://t.me/+ZGV6YF8jPlBlYjQy" rel="nofollow noopener noreferrer">Душный NLP</a></div>
      <div class="actions">
        <span>5 573 просмотров · 22 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/87" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/87.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    </div>
    
    <div class="pager static-pager" style="justify-content:center">
      <div class="page-links">
        <a class="nav-link" href="index.html">←</a>
        <a class="page-link" href="index.html">1</a> <a class="page-link current" href="page-2.html">2</a> <a class="page-link" href="page-3.html">3</a> <a class="page-link" href="page-4.html">4</a>
        <a class="nav-link" href="page-3.html">→</a>
      </div>
    </div>
    
  </main>

  <footer class="footer">
    <div class="container">
      <div class="footer-inner">
        <span>based on <a href="https://github.com/ml-brand/tg-to-gh-pages" target="_blank" rel="noopener">tg-to-gh-pages</a> (created by <a href="https://github.com/ml-brand" target="_blank" rel="noopener">ML Brand</a>)</span>
        <a id="repoLink" href="https://github.com/ml-brand/tg-to-gh-pages" target="_blank" rel="noopener">Do the same with your channel.</a>
        <span class="footer-links">
          static copy ·
          <a href="../feed.xml" target="_blank" rel="noopener">RSS</a> ·
          <a href="../atom.xml" target="_blank" rel="noopener">Atom</a>
        </span>
      </div>
    </div>
  </footer>

  <script>
    window.__STATIC_POSTS = [{"id": 155, "media": [{"kind": "photo", "path": "../assets/media/155.jpg", "thumb": "../assets/media/thumbs/155_480.webp", "size": 222129, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/156.jpg", "thumb": "../assets/media/thumbs/156_480.webp", "size": 228363, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/157.jpg", "thumb": "../assets/media/thumbs/157_480.webp", "size": 85720, "mime": "image/jpeg", "name": null}]}, {"id": 151, "media": [{"kind": "photo", "path": "../assets/media/151.jpg", "thumb": "../assets/media/thumbs/151_480.webp", "size": 262374, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/152.jpg", "thumb": "../assets/media/thumbs/152_480.webp", "size": 282717, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/153.jpg", "thumb": "../assets/media/thumbs/153_480.webp", "size": 244519, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/154.jpg", "thumb": "../assets/media/thumbs/154_480.webp", "size": 91869, "mime": "image/jpeg", "name": null}]}, {"id": 147, "media": [{"kind": "photo", "path": "../assets/media/147.jpg", "thumb": "../assets/media/thumbs/147_480.webp", "size": 257685, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/148.jpg", "thumb": "../assets/media/thumbs/148_480.webp", "size": 293817, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/149.jpg", "thumb": "../assets/media/thumbs/149_480.webp", "size": 252277, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/150.jpg", "thumb": "../assets/media/thumbs/150_480.webp", "size": 239209, "mime": "image/jpeg", "name": null}]}, {"id": 137, "media": [{"kind": "photo", "path": "../assets/media/137.jpg", "thumb": "../assets/media/thumbs/137_480.webp", "size": 158629, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/138.jpg", "thumb": "../assets/media/thumbs/138_480.webp", "size": 189128, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/139.jpg", "thumb": "../assets/media/thumbs/139_480.webp", "size": 162752, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/140.jpg", "thumb": "../assets/media/thumbs/140_480.webp", "size": 185682, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/141.jpg", "thumb": "../assets/media/thumbs/141_480.webp", "size": 177856, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/142.jpg", "thumb": "../assets/media/thumbs/142_480.webp", "size": 207855, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/143.jpg", "thumb": "../assets/media/thumbs/143_480.webp", "size": 208326, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/144.jpg", "thumb": "../assets/media/thumbs/144_480.webp", "size": 163434, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/145.jpg", "thumb": "../assets/media/thumbs/145_480.webp", "size": 161463, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/146.jpg", "thumb": "../assets/media/thumbs/146_480.webp", "size": 101020, "mime": "image/jpeg", "name": null}]}, {"id": 136, "media": []}, {"id": 135, "media": []}, {"id": 134, "media": [{"kind": "photo", "path": "../assets/media/134.jpg", "thumb": "../assets/media/thumbs/134_480.webp", "size": 100023, "mime": "image/jpeg", "name": null}]}, {"id": 133, "media": [{"kind": "photo", "path": "../assets/media/133.jpg", "thumb": "../assets/media/thumbs/133_480.webp", "size": 61947, "mime": "image/jpeg", "name": null}]}, {"id": 132, "media": [{"kind": "photo", "path": "../assets/media/132.jpg", "thumb": "../assets/media/thumbs/132_480.webp", "size": 73109, "mime": "image/jpeg", "name": null}]}, {"id": 131, "media": [{"kind": "photo", "path": "../assets/media/131.jpg", "thumb": "../assets/media/thumbs/131_480.webp", "size": 156967, "mime": "image/jpeg", "name": null}]}, {"id": 127, "media": [{"kind": "photo", "path": "../assets/media/127.jpg", "thumb": "../assets/media/thumbs/127_480.webp", "size": 21781, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/128.jpg", "thumb": "../assets/media/thumbs/128_480.webp", "size": 69089, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/129.jpg", "thumb": "../assets/media/thumbs/129_480.webp", "size": 96268, "mime": "image/jpeg", "name": null}]}, {"id": 126, "media": [{"kind": "photo", "path": "../assets/media/126.jpg", "thumb": "../assets/media/thumbs/126_480.webp", "size": 46599, "mime": "image/jpeg", "name": null}]}, {"id": 124, "media": [{"kind": "photo", "path": "../assets/media/124.jpg", "thumb": "../assets/media/thumbs/124_480.webp", "size": 87267, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/125.jpg", "thumb": "../assets/media/thumbs/125_480.webp", "size": 74318, "mime": "image/jpeg", "name": null}]}, {"id": 119, "media": [{"kind": "photo", "path": "../assets/media/119.jpg", "thumb": "../assets/media/thumbs/119_480.webp", "size": 279366, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/120.jpg", "thumb": "../assets/media/thumbs/120_480.webp", "size": 189381, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/121.jpg", "thumb": "../assets/media/thumbs/121_480.webp", "size": 177847, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/122.jpg", "thumb": "../assets/media/thumbs/122_480.webp", "size": 168900, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/123.jpg", "thumb": "../assets/media/thumbs/123_480.webp", "size": 189255, "mime": "image/jpeg", "name": null}]}, {"id": 118, "media": [{"kind": "photo", "path": "../assets/media/118.jpg", "thumb": "../assets/media/thumbs/118_480.webp", "size": 167786, "mime": "image/jpeg", "name": null}]}, {"id": 112, "media": [{"kind": "photo", "path": "../assets/media/112.jpg", "thumb": "../assets/media/thumbs/112_480.webp", "size": 198674, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/113.jpg", "thumb": "../assets/media/thumbs/113_480.webp", "size": 219310, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/114.jpg", "thumb": "../assets/media/thumbs/114_480.webp", "size": 233801, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/115.jpg", "thumb": "../assets/media/thumbs/115_480.webp", "size": 167314, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/116.jpg", "thumb": "../assets/media/thumbs/116_480.webp", "size": 236159, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/117.jpg", "thumb": "../assets/media/thumbs/117_480.webp", "size": 113881, "mime": "image/jpeg", "name": null}]}, {"id": 106, "media": [{"kind": "photo", "path": "../assets/media/106.jpg", "thumb": "../assets/media/thumbs/106_480.webp", "size": 196820, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/107.jpg", "thumb": "../assets/media/thumbs/107_480.webp", "size": 241115, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/108.jpg", "thumb": "../assets/media/thumbs/108_480.webp", "size": 179249, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/109.jpg", "thumb": "../assets/media/thumbs/109_480.webp", "size": 213395, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/110.jpg", "thumb": "../assets/media/thumbs/110_480.webp", "size": 236220, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/111.jpg", "thumb": "../assets/media/thumbs/111_480.webp", "size": 183394, "mime": "image/jpeg", "name": null}]}, {"id": 105, "media": [{"kind": "photo", "path": "../assets/media/105.jpg", "thumb": "../assets/media/thumbs/105_480.webp", "size": 178363, "mime": "image/jpeg", "name": null}]}, {"id": 101, "media": [{"kind": "photo", "path": "../assets/media/101.jpg", "thumb": "../assets/media/thumbs/101_480.webp", "size": 218659, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/102.jpg", "thumb": "../assets/media/thumbs/102_480.webp", "size": 260503, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/103.jpg", "thumb": "../assets/media/thumbs/103_480.webp", "size": 208417, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/104.jpg", "thumb": "../assets/media/thumbs/104_480.webp", "size": 206892, "mime": "image/jpeg", "name": null}]}, {"id": 100, "media": [{"kind": "video", "path": "../assets/media/100_video_2025-04-24_16-21-34.mp4", "thumb": null, "size": 6538655, "mime": "video/mp4", "name": "video_2025-04-24_16-21-34.mp4"}]}, {"id": 99, "media": []}, {"id": 98, "media": [{"kind": "photo", "path": "../assets/media/98.jpg", "thumb": "../assets/media/thumbs/98_480.webp", "size": 296171, "mime": "image/jpeg", "name": null}]}, {"id": 97, "media": [{"kind": "photo", "path": "../assets/media/97.jpg", "thumb": "../assets/media/thumbs/97_480.webp", "size": 50377, "mime": "image/jpeg", "name": null}]}, {"id": 96, "media": [{"kind": "photo", "path": "../assets/media/96.jpg", "thumb": "../assets/media/thumbs/96_480.webp", "size": 95788, "mime": "image/jpeg", "name": null}]}, {"id": 95, "media": []}, {"id": 94, "media": [{"kind": "photo", "path": "../assets/media/94.jpg", "thumb": "../assets/media/thumbs/94_480.webp", "size": 82114, "mime": "image/jpeg", "name": null}]}, {"id": 93, "media": []}, {"id": 90, "media": [{"kind": "photo", "path": "../assets/media/90.jpg", "thumb": "../assets/media/thumbs/90_480.webp", "size": 63905, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/91.jpg", "thumb": "../assets/media/thumbs/91_480.webp", "size": 21169, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/92.jpg", "thumb": "../assets/media/thumbs/92_480.webp", "size": 64428, "mime": "image/jpeg", "name": null}]}, {"id": 88, "media": [{"kind": "photo", "path": "../assets/media/88.jpg", "thumb": "../assets/media/thumbs/88_480.webp", "size": 101910, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/89.jpg", "thumb": "../assets/media/thumbs/89_480.webp", "size": 83400, "mime": "image/jpeg", "name": null}]}, {"id": 87, "media": []}];
    window.__STATIC_META = {"title": "Душный NLP", "username": "stuffyNLP", "channel": "stuffyNLP", "last_sync_utc": "2026-02-11T20:21:55Z", "posts_count": 113, "last_seen_message_id": 225, "stats": {"new": 105, "updated": 7, "media_downloaded": 105}, "avatar": "assets/channel_avatar.jpg", "meta_schema_version": "1.0.0", "posts_schema_version": "1.0.0"};
  </script>
  <script src="../common.js"></script>
  <script src="../static.js"></script>
</body>
</html>
