<!doctype html>
<html lang="ru">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Душный NLP — статическая версия (стр. 3/4)</title>
  <meta name="description" content="Статическая версия зеркала Telegram-канала" />
  <link rel="icon" href="../favicon.ico?v=2026-01-30T20%3A00%3A29Z" sizes="any" />
  <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32.png?v=2026-01-30T20%3A00%3A29Z" />
  <link rel="apple-touch-icon" href="../apple-touch-icon.png?v=2026-01-30T20%3A00%3A29Z" />

  <link rel="stylesheet" href="../style.css" />
  <script src="../metrika.js"></script>
</head>
<body>
  <header class="header">
    <div class="container">
      <div class="title-grid">
        <a class="grid-avatar" href="#" target="_blank" rel="noopener">
          <img id="channelAvatar" class="channel-avatar" src="../assets/channel_avatar.jpg" alt="Аватар канала"  />
        </a>
        <div class="grid-main">
          <div class="title-head">
            <div class="title-left">
              <a class="badge-chip" id="siteTitleWrap" href="#" target="_blank" rel="noopener"><h1 id="siteTitle">Душный NLP</h1></a>
            </div>
            <div class="hero-actions">
              <a id="subscribeBtn" class="subscribe-btn" href="https://t.me/+1Z41UptsLwszZDE6" target="_blank" rel="noopener" >Подписаться</a>
              <a class="icon-btn" href="../" aria-label="Перейти к динамической версии">↺</a>
              <button id="themeToggle" class="icon-btn" type="button" aria-label="Переключить тему"></button>
            </div>
          </div>
        </div>
        <div class="controls"></div>
      </div>
    </div>
  </header>

  
  <div id="promoBanner" class="promo-banner" hidden>
    <div class="container promo-inner">
      <span class="promo-text"><a href="https://t.me/addlist/5NH3RoVejEI1MGEy">Подпишись на все наши ML каналы. Они классные, отвечаем!</a></span>
      <button id="promoClose" class="promo-close" type="button" aria-label="Скрыть плашку">×</button>
    </div>
  </div>
  

  <main class="container">
    
    <div class="pager static-pager" style="justify-content:center">
      <div class="page-links">
        <a class="nav-link" href="page-2.html">←</a>
        <a class="page-link" href="index.html">1</a> <a class="page-link" href="page-2.html">2</a> <a class="page-link current" href="page-3.html">3</a> <a class="page-link" href="page-4.html">4</a>
        <a class="nav-link" href="page-4.html">→</a>
      </div>
    </div>
    
    <div id="posts" class="posts">
      
    <article class="post" data-post-id="85" data-search="spinquant: llm quantization with learned rotations. часть 1/2 решение из сегодняшней статьи от meta* — конкурент другой разработки по квантизации в низкую битность, quarot. но в spinquant, кроме весов и активаций, квантуется ещё и kv-кэш. иными словами, это sota-результат w4a4kv4-квантизации, который показывает очень хороший перфоманс даже на «макбуках». главная идея — победить проблемы выбросов (поканальных отклонений в активациях attention), добавив матрицы поворота до и после каждого линейного слоя модели. после этого квантизация проводится как обычно, но без потери качества — спасибо обучаемым, а не случайным, как в quarot, матрицам вращения (розовые r₁ на рисунке). но ничего не бывает бесплатно: умножение — отдельная операция, которая требует дополнительных ресурсов. чтобы сэкономить в момент инференса, матрицы вращения r₁ вмёрживаются в матрицы весов w умножением. но так получается сделать не для всех вращений: например, матрицы r₃ и r₄ вставляют в слой отдельной операцией и, как в статье quarot, — используют случайные матрицы адамара. *компания meta признана экстремистской организацией в россии. разбор подготовил ❣ роман горб душный nlp spinquant: llm quantization with learned rotations. часть 1/2 решение из сегодняшней статьи от meta* — конкурент другой разработки по квантизации в низкую битность, quarot. но в spinquant, кроме весов и активаций, квантуется ещё и kv-кэш. иными словами, это sota-результат w4a4kv4-квантизации, который показывает очень хороший перфоманс даже на «макбуках». главная идея — победить проблемы выбросов (поканальных отклонений в активациях attention), добавив матрицы поворота до и после каждого линейного слоя модели. после этого квантизация проводится как обычно, но без потери качества — спасибо обучаемым, а не случайным, как в quarot, матрицам вращения (розовые r₁ на рисунке). но ничего не бывает бесплатно: умножение — отдельная операция, которая требует дополнительных ресурсов. чтобы сэкономить в момент инференса, матрицы вращения r₁ вмёрживаются в матрицы весов w умножением. но так получается сделать не для всех вращений: например, матрицы r₃ и r₄ вставляют в слой отдельной операцией и, как в статье quarot, — используют случайные матрицы адамара. *компания meta признана экстремистской организацией в россии. разбор подготовил ❣ роман горб душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-03-07T10:04:50+00:00" href="./posts/85.html">2025-03-07 10:04 UTC</a></div>
      </div>
      <div class="post-body"><strong>SpinQuant: LLM quantization with learned rotations. Часть 1/2</strong><br><br>Решение из <a href="https://arxiv.org/abs/2405.16406" rel="nofollow noopener noreferrer">сегодняшней статьи</a> от Meta* — конкурент другой разработки по квантизации в низкую битность, <a href="https://arxiv.org/abs/2404.00456" rel="nofollow noopener noreferrer">QuaRot.</a> Но в SpinQuant, кроме весов и активаций, квантуется ещё и KV-кэш. Иными словами, это SOTA-результат w4a4kv4-квантизации, который показывает очень хороший перфоманс даже на «макбуках». <br><br>Главная идея — победить проблемы выбросов (поканальных отклонений в активациях attention), добавив матрицы поворота до и после каждого линейного слоя модели. После этого квантизация проводится как обычно, но без потери качества — спасибо обучаемым, а не случайным, как в QuaRot, матрицам вращения (розовые R₁ на рисунке).<br><br>Но ничего не бывает бесплатно: умножение — отдельная операция, которая требует дополнительных ресурсов. Чтобы сэкономить в момент инференса, матрицы вращения R₁ вмёрживаются в матрицы весов W умножением. Но так получается сделать не для всех вращений: например, матрицы R₃ и R₄ вставляют в слой отдельной операцией и, как в статье QuaRot, — используют случайные матрицы Адамара. <br><br><em>*Компания Meta признана экстремистской организацией в России. </em><br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Роман Горб</em> <br><br><a href="https://t.me/+rGYdUleMERVkZTUy" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/85_480.webp" srcset="../assets/media/thumbs/85_480.webp 480w, ../assets/media/85.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="85" data-image-index="0" /></div></div>
      <div class="actions">
        <span>4 534 просмотров · 35 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/85" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/85.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="84" data-search="технический отчёт qwen2.5-coder qwen2.5-coder — семейство моделей, предназначенных для генерации кода. его технический отчёт вышел ещё в прошлом году, но там много интересного, что стоит разобрать. этим и займёмся. модель учили работать как с отдельными файлами, так и с целыми репозиториями. что касается токенизации, то авторы взяли словарь qwen2.5 на 151,646 токенов, но добавили к нему спецтокены. например, для обучения в режиме fill-in-the-middle (fim). в датасете для допретрейна было пять типов данных: — код на 92 языках программирования — примеры брали с github до февраля 2024 года. использовали фильтры на эвристиках, чтобы отсеять некачественные данные; — text-code grounding data — уже не только код, но и документация, руководства. использовали итеративную иерархическую фильтрацию: сперва брали большой массив данных, но с каждым шагом фильтрации их становилось всё меньше; — синтетические данные — их генерировали с помощью codeqwen1.5. оставляли только выполнявшийся код; — математические данные — использовали претрейн-датасет qwen2.5-math; — текст — использовали данные из претрейна qwen2.5, но без кода. авторы хотели, чтобы модель получилась достаточно универсальной и умела решать различные задачи, а не только связанные с программированием. по результатам бенчмарков, лучше всего себя показала пропорция кода, математики и текста — 7:2:1. сперва был допретрейн на уровне файлов. контекстное окно тут составляло не более 8 тысяч токенов. здесь как раз и использовали fim. следом шло обучение на уровне репозиториев с контекстным окном до 32 тысяч токенов за счёт увеличения параметра rope base до миллиона. для экстраполяции до 128 тысяч применяли технику yarn. на посттрейне обучили модель codebert, чтобы классифицировать документы по примерно сотне языков программирования. при этом авторы сохраняли все найденные документы на популярных языках, а те, которые написаны на редких — частично удаляли. кроме того, убирали все примеры без кода, потому что на этапе инструктивного обучения они ухудшали показатели на бенчмарках. для репозиториев на github генерировали инструкции неназванной в отчёте llm, а с помощью другой модели — тоже неназванной — создавали ответы. третья llm занималась фильтрацией полученных пар. для повышения точности использовали фьюшот на опенсорсных инстракт-датасетах — например, mceval-instruct. для формирования мультиязычного sft-датасета использовали языкоспецефичных агентов, у каждого из которых, как заявляют авторы, был свой банк памяти с историей генераций. в целом, в техническом отчёте не хватает конкретики по sft-датасету. оценка получившихся пар примеров производилась по чеклисту из 9 параметров. среди них: соответствие ответа вопросу, сложность, наличие кода, его правильность и ясность. самый трудный с точки зрения разработки элемент — это мультиязычная «песочница». нужно было обеспечить поддержку каждого из 92 языков программирования, собрать репозиторий эталонного кода, создать генераторы юнит-тестов, движки выполнения и анализаторы результатов. «песочница» — вещь итеративная, которая нуждается в постоянной поддержке. для этого разработчики привлекли экспертов, что делает «песочницу» особенно ресурсозатратной. sft сперва осуществлялся на нескольких десятках миллионов не слишком качественных, но разнообразных примеров. затем с помощью rejection sampling выборка снижалась до нескольких миллионов лучших сэмплов. чтобы на этапе sft модель не разучилась работать с длинным контекстом, авторы использовали короткие инстракт-сэмплы с fim. при этом добавленный контекст — синтаксис — получался с помощью парсинга библиотекой tree-sitter-languages. примеров, обогащённых контекстом, было меньше, чем обычных sft-данных. в рамках dpo использовали rlef для сигналов о том, что код работает, а также юнит-тесты для некоторых примеров. были и примеры без кода, чтобы модель могла решать задачи, не связанные с программированием. в качестве llm-as-a-judge выступала 4о. разбор подготовил ❣ алексей малафеев душный nlp технический отчёт qwen2.5-coder qwen2.5-coder — семейство моделей, предназначенных для генерации кода. его технический отчёт вышел ещё в прошлом году, но там много интересного, что стоит разобрать. этим и займёмся. модель учили работать как с отдельными файлами, так и с целыми репозиториями. что касается токенизации, то авторы взяли словарь qwen2.5 на 151,646 токенов, но добавили к нему спецтокены. например, для обучения в режиме fill-in-the-middle (fim). в датасете для допретрейна было пять типов данных: — код на 92 языках программирования — примеры брали с github до февраля 2024 года. использовали фильтры на эвристиках, чтобы отсеять некачественные данные; — text-code grounding data — уже не только код, но и документация, руководства. использовали итеративную иерархическую фильтрацию: сперва брали большой массив данных, но с каждым шагом фильтрации их становилось всё меньше; — синтетические данные — их генерировали с помощью codeqwen1.5. оставляли только выполнявшийся код; — математические данные — использовали претрейн-датасет qwen2.5-math; — текст — использовали данные из претрейна qwen2.5, но без кода. авторы хотели, чтобы модель получилась достаточно универсальной и умела решать различные задачи, а не только связанные с программированием. по результатам бенчмарков, лучше всего себя показала пропорция кода, математики и текста — 7:2:1. сперва был допретрейн на уровне файлов. контекстное окно тут составляло не более 8 тысяч токенов. здесь как раз и использовали fim. следом шло обучение на уровне репозиториев с контекстным окном до 32 тысяч токенов за счёт увеличения параметра rope base до миллиона. для экстраполяции до 128 тысяч применяли технику yarn. на посттрейне обучили модель codebert, чтобы классифицировать документы по примерно сотне языков программирования. при этом авторы сохраняли все найденные документы на популярных языках, а те, которые написаны на редких — частично удаляли. кроме того, убирали все примеры без кода, потому что на этапе инструктивного обучения они ухудшали показатели на бенчмарках. для репозиториев на github генерировали инструкции неназванной в отчёте llm, а с помощью другой модели — тоже неназванной — создавали ответы. третья llm занималась фильтрацией полученных пар. для повышения точности использовали фьюшот на опенсорсных инстракт-датасетах — например, mceval-instruct. для формирования мультиязычного sft-датасета использовали языкоспецефичных агентов, у каждого из которых, как заявляют авторы, был свой банк памяти с историей генераций. в целом, в техническом отчёте не хватает конкретики по sft-датасету. оценка получившихся пар примеров производилась по чеклисту из 9 параметров. среди них: соответствие ответа вопросу, сложность, наличие кода, его правильность и ясность. самый трудный с точки зрения разработки элемент — это мультиязычная «песочница». нужно было обеспечить поддержку каждого из 92 языков программирования, собрать репозиторий эталонного кода, создать генераторы юнит-тестов, движки выполнения и анализаторы результатов. «песочница» — вещь итеративная, которая нуждается в постоянной поддержке. для этого разработчики привлекли экспертов, что делает «песочницу» особенно ресурсозатратной. sft сперва осуществлялся на нескольких десятках миллионов не слишком качественных, но разнообразных примеров. затем с помощью rejection sampling выборка снижалась до нескольких миллионов лучших сэмплов. чтобы на этапе sft модель не разучилась работать с длинным контекстом, авторы использовали короткие инстракт-сэмплы с fim. при этом добавленный контекст — синтаксис — получался с помощью парсинга библиотекой tree-sitter-languages. примеров, обогащённых контекстом, было меньше, чем обычных sft-данных. в рамках dpo использовали rlef для сигналов о том, что код работает, а также юнит-тесты для некоторых примеров. были и примеры без кода, чтобы модель могла решать задачи, не связанные с программированием. в качестве llm-as-a-judge выступала 4о. разбор подготовил ❣ алексей малафеев душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-02-28T09:48:10+00:00" href="./posts/84.html">2025-02-28 09:48 UTC</a></div>
      </div>
      <div class="post-body"><strong>Технический отчёт Qwen2.5-Coder</strong><br><br>Qwen2.5-Coder — семейство моделей, предназначенных для генерации кода. Его <a href="https://arxiv.org/abs/2409.12186" rel="nofollow noopener noreferrer">технический отчёт</a> вышел ещё в прошлом году, но там много интересного, что стоит разобрать. Этим и займёмся.  <br><br>Модель учили работать как с отдельными файлами, так и с целыми репозиториями. Что касается токенизации, то авторы взяли словарь Qwen2.5 на 151,646 токенов, но добавили к нему спецтокены. Например, для обучения в режиме <a href="https://arxiv.org/abs/2207.14255" rel="nofollow noopener noreferrer">Fill-in-the-Middle (FIM). </a><br><br>В датасете для допретрейна было пять типов данных:<br><br><strong>— код на 92 языках программирования</strong> — примеры брали с GitHub до февраля 2024 года. Использовали фильтры на эвристиках, чтобы отсеять некачественные данные;<br><strong>— text-code grounding data </strong>— уже не только код, но и документация, руководства. Использовали итеративную иерархическую фильтрацию: сперва брали большой массив данных, но с каждым шагом фильтрации их становилось всё меньше;<br><strong>— синтетические данные</strong> — их генерировали с помощью CodeQwen1.5. Оставляли только выполнявшийся код;<br><strong>— математические данные</strong> — использовали претрейн-датасет Qwen2.5-Math;<br><strong>— текст </strong>— использовали данные из претрейна Qwen2.5, но без кода. Авторы хотели, чтобы модель получилась достаточно универсальной и умела решать различные задачи, а не только связанные с программированием. По результатам бенчмарков, лучше всего себя показала пропорция кода, математики и текста — 7:2:1.<br><br>Сперва был допретрейн на уровне файлов. Контекстное окно тут составляло не более 8 тысяч токенов. Здесь как раз и использовали FIM. Следом шло обучение на уровне репозиториев с контекстным окном до 32 тысяч токенов за счёт увеличения параметра RoPE base до миллиона. Для экстраполяции до 128 тысяч применяли технику YARN. <br><br>На посттрейне обучили модель CodeBERT, чтобы классифицировать документы по примерно сотне языков программирования. При этом авторы сохраняли все найденные документы на популярных языках, а те, которые написаны на редких — частично удаляли. Кроме того, убирали все примеры без кода, потому что на этапе инструктивного обучения они ухудшали показатели на бенчмарках. <br><br>Для репозиториев на GitHub генерировали инструкции неназванной в отчёте LLM, а с помощью другой модели — тоже неназванной — создавали ответы. Третья LLM занималась фильтрацией полученных пар. Для повышения точности использовали фьюшот на опенсорсных инстракт-датасетах — например, <a href="https://huggingface.co/datasets/Multilingual-Multimodal-NLP/McEval-Instruct" rel="nofollow noopener noreferrer">McEval-Instruct.</a> <br><br>Для формирования мультиязычного SFT-датасета использовали языкоспецефичных агентов, у каждого из которых, как заявляют авторы, был свой банк памяти с историей генераций. В целом, в техническом отчёте не хватает конкретики по SFT-датасету. Оценка получившихся пар примеров производилась по чеклисту из 9 параметров. Среди них: соответствие ответа вопросу, сложность, наличие кода, его правильность и ясность. <br><br>Самый трудный с точки зрения разработки элемент — это мультиязычная «песочница». Нужно было обеспечить поддержку каждого из 92 языков программирования, собрать репозиторий эталонного кода, создать генераторы юнит-тестов, движки выполнения и анализаторы результатов. «Песочница» — вещь итеративная, которая нуждается в постоянной поддержке. Для этого разработчики привлекли экспертов, что делает «песочницу» особенно ресурсозатратной. <br><br>SFT сперва осуществлялся на нескольких десятках миллионов не слишком качественных, но разнообразных примеров. Затем с помощью rejection sampling выборка снижалась до нескольких миллионов лучших сэмплов. <br><br>Чтобы на этапе SFT модель не разучилась работать с длинным контекстом, авторы использовали короткие инстракт-сэмплы с FIM. При этом добавленный контекст — синтаксис — получался с помощью парсинга библиотекой Tree-sitter-languages. Примеров, обогащённых контекстом, было меньше, чем обычных SFT-данных.<br><br>В рамках DPO использовали RLEF для сигналов о том, что код работает, а также юнит-тесты для некоторых примеров. Были и примеры без кода, чтобы модель могла решать задачи, не связанные с программированием. В качестве LLM-as-a-judge выступала 4о. <br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Алексей Малафеев</em><br><br><a href="https://t.me/+EusdtA74_rRhMWRi" rel="nofollow noopener noreferrer">Душный NLP</a></div>
      <div class="actions">
        <span>6 078 просмотров · 38 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/84" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/84.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="81" data-search="yandexgpt 5 уже в опенсорсе и алисе сегодня яндекс показал миру новое поколение больших языковых моделей — yandexgpt 5. старшая модель yandexgpt 5 pro доступна в чате с алисой и yandex cloud через api. ну а претрейн-версия младшей модели yandexgpt 5 lite pretrain — уже лежит на hugging face. все подробности о процессе обучения можно прочитать в статье на хабре. а в этом посте — главные факты о свежей опенсорсной модели яндекса. yandexgpt 5 lite pretrain — модель на 8 миллиардов параметров с длиной контекста 32 тысячи токенов. претрейн проходил в два этапа: сначала модель обучили на 15 триллионах токенов текста на русском и английском языках, а потом использовали 320 миллиардов токенов высококачественных данных, включая образовательный контент. на первом этапе датасет больше чем на половину состоял из веб-документов, остальное — код, математика и специфичные данные. под последними подразумеваются синтетика (сгенерированные yandexgpt 4 вопросы на основе проверенных источников) и внутренние наработки компании (например, внутренняя база яндекса fact snippet и новый корпус данных переводчика). на втором этапе датасет на четверть состоял из веб-страниц и почти в равных пропорциях содержал математику, код и образовательные данные. также была небольшая часть аугментаций фактовых документов, другой синтетики и датасетов сервисов. по сравнению с моделью предыдущего поколения, yandexgpt 4 lite pretrain, новая модель показывает ощутимый рост качества в решении математических задач и написании кода. а в сравнении с зарубежными аналогами, такими как llama3.1-8b и qwen-2.5-7b-base, она лидирует почти во всех типах задач. ещё раз приглашаем пощупать модель, почитать статью на хабре с деталями обучения и не забыть поделиться впечатлениями в комментариях! ml underhood yandexgpt 5 уже в опенсорсе и алисе сегодня яндекс показал миру новое поколение больших языковых моделей — yandexgpt 5. старшая модель yandexgpt 5 pro доступна в чате с алисой и yandex cloud через api. ну а претрейн-версия младшей модели yandexgpt 5 lite pretrain — уже лежит на hugging face . все подробности о процессе обучения можно прочитать в статье на хабре . а в этом посте — главные факты о свежей опенсорсной модели яндекса. yandexgpt 5 lite pretrain — модель на 8 миллиардов параметров с длиной контекста 32 тысячи токенов. претрейн проходил в два этапа: сначала модель обучили на 15 триллионах токенов текста на русском и английском языках, а потом использовали 320 миллиардов токенов высококачественных данных, включая образовательный контент. на первом этапе датасет больше чем на половину состоял из веб-документов, остальное — код, математика и специфичные данные. под последними подразумеваются синтетика (сгенерированные yandexgpt 4 вопросы на основе проверенных источников) и внутренние наработки компании (например, внутренняя база яндекса fact snippet и новый корпус данных переводчика). на втором этапе датасет на четверть состоял из веб-страниц и почти в равных пропорциях содержал математику, код и образовательные данные. также была небольшая часть аугментаций фактовых документов, другой синтетики и датасетов сервисов. по сравнению с моделью предыдущего поколения, yandexgpt 4 lite pretrain, новая модель показывает ощутимый рост качества в решении математических задач и написании кода. а в сравнении с зарубежными аналогами, такими как llama3.1-8b и qwen-2.5-7b-base, она лидирует почти во всех типах задач. ещё раз приглашаем пощупать модель , почитать статью на хабре с деталями обучения и не забыть поделиться впечатлениями в комментариях! ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-02-25T10:54:00+00:00" href="./posts/81.html">2025-02-25 10:54 UTC</a></div>
      </div>
      <div class="post-body"><strong>YandexGPT 5 уже в опенсорсе и Алисе</strong><br><br>Сегодня Яндекс показал миру новое поколение больших языковых моделей — YandexGPT 5. Старшая модель YandexGPT 5 Pro доступна в чате с Алисой и Yandex Cloud через API. Ну а претрейн-версия младшей модели YandexGPT 5 Lite Pretrain — уже лежит <a href="https://huggingface.co/yandex/YandexGPT-5-Lite-8B-pretrain" rel="nofollow noopener noreferrer">на Hugging Face</a>.<br><br>Все подробности о процессе обучения можно прочитать <a href="https://habr.com/ru/companies/yandex/articles/885218/" rel="nofollow noopener noreferrer">в статье на Хабре</a>. А в этом посте — главные факты о свежей опенсорсной модели Яндекса.<br><br>YandexGPT 5 Lite Pretrain — модель на 8 миллиардов параметров с длиной контекста 32 тысячи токенов. Претрейн проходил в два этапа: сначала модель обучили на 15 триллионах токенов текста на русском и английском языках, а потом использовали 320 миллиардов токенов высококачественных данных, включая образовательный контент.<br><br>На первом этапе датасет больше чем на половину состоял из веб-документов, остальное — код, математика и специфичные данные. Под последними подразумеваются синтетика (сгенерированные YandexGPT 4 вопросы на основе проверенных источников) и внутренние наработки компании (например, внутренняя база Яндекса Fact Snippet и новый корпус данных Переводчика).<br><br>На втором этапе датасет на четверть состоял из веб-страниц и почти в равных пропорциях содержал математику, код и образовательные данные. Также была небольшая часть аугментаций фактовых документов, другой синтетики и датасетов сервисов. <br><br>По сравнению с моделью предыдущего поколения, YandexGPT 4 Lite Pretrain, новая модель показывает ощутимый рост качества в решении математических задач и написании кода. А в сравнении с зарубежными аналогами, такими как LLaMa3.1-8B и Qwen-2.5-7B-base, она лидирует почти во всех типах задач. <br><br>Ещё раз приглашаем <a href="https://huggingface.co/yandex/YandexGPT-5-Lite-8B-pretrain" rel="nofollow noopener noreferrer">пощупать модель</a>, <a href="https://habr.com/ru/companies/yandex/articles/885218/" rel="nofollow noopener noreferrer">почитать статью</a> на Хабре с деталями обучения и не забыть поделиться впечатлениями в комментариях!<br><br><br><a href="https://t.me/+x1iAxBkfmVZjNzIy" rel="nofollow noopener noreferrer">ML Underhood</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/81_480.webp" srcset="../assets/media/thumbs/81_480.webp 480w, ../assets/media/81.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="81" data-image-index="0" /></div></div>
      <div class="actions">
        <span>3 636 просмотров · 52 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/81" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/81.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="79" data-search="яндекс выпустила новое поколение llm — yandexgpt 5. модели можно опробовать в облаке, алисе и открытом доступе. делимся подробностями обучения и результатами бенчмарков. яндекс выпустила новое поколение llm — yandexgpt 5. модели можно опробовать в облаке, алисе и открытом доступе. делимся подробностями обучения и результатами бенчмарков.">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-02-25T10:53:19+00:00" href="./posts/79.html">2025-02-25 10:53 UTC</a></div>
      </div>
      <div class="post-body">Яндекс выпустила новое поколение LLM — YandexGPT 5. Модели можно опробовать в облаке, Алисе и открытом доступе. Делимся подробностями обучения и результатами бенчмарков.</div>
      <div class="actions">
        <span>2 886 просмотров · 15 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/79" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/79.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="78" data-search="документный llm-переводчик в яндексе яндекс запустил новую модель для документного перевода на основе yandexgpt. она уже работает в поиске, умной камере и нейропереводчике яндекс браузера, а также заняла первое место в бенчмарке dibimt по переводу с английского на русский. обо всех нюансах работы переводчика и о том, как его создавали, на хабре рассказал руководитель группы базового качества перевода николай карпачёв. а здесь — кратко о главном. документный перевод предполагает адаптацию на другой язык не каждого отдельного предложения, а всего текста. почему это важно? причин несколько. например, английское «you» может означать как «ты», так и «вы», но без контекста модель не понимает, какой вариант выбрать. термины и стилистика могут «прыгать» внутри текста, а пропущенные элементы, понятные носителю языка, в переводе превращаются в бессмысленный набор слов. люди воспринимают текст иначе: мы читаем книги, статьи, субтитры — всё целиком. значит, и машинный перевод должен работать так же. инженеры яндекса попробовали перевести тексты llm-моделью «из коробки», без дообучения, но столкнулись с типичными ошибками: пропущенные фрагменты, лишние добавления, галлюцинации. чтобы этого избежать, модель пришлось адаптировать. на первом этапе подготовили данные, включая не только классические парные предложения, но и переводы документов, полученные автоматическим выравниванием и с помощью синтетики. дообучение проходило в форматах lora и p-tuning. на следующем этапе модель дообучалась с помощью технологии alignment. разные варианты переводов сравнивались редакторами-профессионалами. полученные оценки использовали для оптимизации методом contrastive preference optimization (cpo). на этой стадии происходит исправление существующих ошибок и проблем llm-модели, найденных редакторами. это позволило минимизировать ошибки, связанные с потерей информации и несогласованностью. в итоге по метрике mqm новая модель переводит тексты почти так же хорошо, как человек. количество грубых ошибок сократилось в два раза по сравнению с предыдущей версией, а финальный результат оказался даже лучше gpt-4o. ml underhood документный llm-переводчик в яндексе яндекс запустил новую модель для документного перевода на основе yandexgpt. она уже работает в поиске, умной камере и нейропереводчике яндекс браузера, а также заняла первое место в бенчмарке dibimt по переводу с английского на русский. обо всех нюансах работы переводчика и о том, как его создавали, на хабре рассказал руководитель группы базового качества перевода николай карпачёв. а здесь — кратко о главном. документный перевод предполагает адаптацию на другой язык не каждого отдельного предложения, а всего текста. почему это важно? причин несколько. например, английское «you» может означать как «ты», так и «вы», но без контекста модель не понимает, какой вариант выбрать. термины и стилистика могут «прыгать» внутри текста, а пропущенные элементы, понятные носителю языка, в переводе превращаются в бессмысленный набор слов. люди воспринимают текст иначе: мы читаем книги, статьи, субтитры — всё целиком. значит, и машинный перевод должен работать так же. инженеры яндекса попробовали перевести тексты llm-моделью «из коробки», без дообучения, но столкнулись с типичными ошибками: пропущенные фрагменты, лишние добавления, галлюцинации. чтобы этого избежать, модель пришлось адаптировать. на первом этапе подготовили данные, включая не только классические парные предложения, но и переводы документов, полученные автоматическим выравниванием и с помощью синтетики. дообучение проходило в форматах lora и p-tuning. на следующем этапе модель дообучалась с помощью технологии alignment. разные варианты переводов сравнивались редакторами-профессионалами. полученные оценки использовали для оптимизации методом contrastive preference optimization (cpo). на этой стадии происходит исправление существующих ошибок и проблем llm-модели, найденных редакторами. это позволило минимизировать ошибки, связанные с потерей информации и несогласованностью. в итоге по метрике mqm новая модель переводит тексты почти так же хорошо, как человек. количество грубых ошибок сократилось в два раза по сравнению с предыдущей версией, а финальный результат оказался даже лучше gpt-4o. ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-02-21T10:12:16+00:00" href="./posts/78.html">2025-02-21 10:12 UTC</a></div>
      </div>
      <div class="post-body"><strong>Документный LLM-переводчик в Яндексе</strong><br><br>Яндекс запустил новую модель для документного перевода на основе YandexGPT. Она уже работает в Поиске, Умной камере и Нейропереводчике Яндекс Браузера, а также заняла первое место в <a href="https://nlp.uniroma1.it/dibimt/public/leaderboard" rel="nofollow noopener noreferrer">бенчмарке DiBiMT</a> по переводу с английского на русский. Обо всех нюансах работы переводчика и о том, как его создавали, <a href="https://habr.com/ru/companies/yandex/articles/884416/" rel="nofollow noopener noreferrer">на Хабре</a> рассказал руководитель группы базового качества перевода Николай Карпачёв. А здесь — кратко о главном. <br><br>Документный перевод предполагает адаптацию на другой язык не каждого отдельного предложения, а всего текста. Почему это важно? Причин несколько. Например, английское «you» может означать как «ты», так и «вы», но без контекста модель не понимает, какой вариант выбрать. Термины и стилистика могут «прыгать» внутри текста, а пропущенные элементы, понятные носителю языка, в переводе превращаются в бессмысленный набор слов. Люди воспринимают текст иначе: мы читаем книги, статьи, субтитры — всё целиком. Значит, и машинный перевод должен работать так же.<br><br>Инженеры Яндекса попробовали перевести тексты LLM-моделью «из коробки», без дообучения, но столкнулись с типичными ошибками: пропущенные фрагменты, лишние добавления, галлюцинации. Чтобы этого избежать, модель пришлось адаптировать. На первом этапе подготовили данные, включая не только классические парные предложения, но и переводы документов, полученные автоматическим выравниванием и с помощью синтетики. Дообучение проходило в форматах LoRA и P-Tuning. <br><br>На следующем этапе модель дообучалась с помощью технологии alignment. Разные варианты переводов сравнивались редакторами-профессионалами. Полученные оценки использовали для оптимизации методом Contrastive Preference Optimization (CPO). На этой стадии происходит исправление существующих ошибок и проблем LLM-модели, найденных редакторами. Это позволило минимизировать ошибки, связанные с потерей информации и несогласованностью. <br><br>В итоге по метрике MQM новая модель переводит тексты почти так же хорошо, как человек. Количество грубых ошибок сократилось в два раза по сравнению с предыдущей версией, а финальный результат оказался даже лучше GPT-4o.<br><br><a href="https://t.me/+x1iAxBkfmVZjNzIy" rel="nofollow noopener noreferrer">ML Underhood</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/78_480.webp" srcset="../assets/media/thumbs/78_480.webp 480w, ../assets/media/78.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="78" data-image-index="0" /></div></div>
      <div class="actions">
        <span>3 821 просмотров · 45 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/78" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/78.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="77" data-search="а в нашем канале ml underhood (подписывайтесь, если ещё не!) — краткий обзор большой статьи о новом llm-переводчике яндекса. отличный текст, чтобы с пользой провести пятничный вечер. а в нашем канале ml underhood (подписывайтесь, если ещё не!) — краткий обзор большой статьи о новом llm-переводчике яндекса. отличный текст, чтобы с пользой провести пятничный вечер.">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-02-21T10:12:16+00:00" href="./posts/77.html">2025-02-21 10:12 UTC</a></div>
      </div>
      <div class="post-body">А в нашем канале ML Underhood (подписывайтесь, если ещё не!) — краткий обзор большой статьи о новом LLM-переводчике Яндекса. Отличный текст, чтобы с пользой провести пятничный вечер.</div>
      <div class="actions">
        <span>3 193 просмотров · 0 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/77" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/77.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="75" data-search="сбалансированный метод семплирования min-p min-p — метод семплирования, который, по словам его создателей, позволяет найти баланс между креативностью и связностью ответов. сегодня разберём статью с описанием этого подхода. при использовании отсекающего семплирования вроде top-p или top-k, на каждом шаге генерации после отсечения может всё ещё оставаться ненужный нам «хвост» из маловероятных токенов. это приводит к тому, что вероятность допустить ошибку на следующем шаге генерации — не нулевая. а токен — не воробей, вылетит — не поймаешь. из-за этого может пострадать весь ответ. это происходит потому что top-p и top-k предполагают применение жёсткого порога отсечения, который никак не зависит от шага генерации и уверенности модели в следующем токене. а подобрать универсальный порог на все случаи жизни невозможно. метод min-p пытается решить эту проблему с помощью динамической настройки порога в зависимости от токена с самой высокой вероятностью. если модель уверена в токене, то порог обрезки будет высокий. если сомневается — то из распределения возьмётся больше токенов. как это работает: 1. выбирается токен с наибольшей вероятностью — pmax; 2. гиперпараметр метода — базовый порог вероятности, pbase (авторы рекомендуют выбирать между 0,05 и 0,1) — умножается на pmax, и получается порог отсечки — pscaled. по нему отсекаются токены, всё, что ниже, выкидывается; 3.формируется пул для семплирования из оставшихся токенов; 4. вероятности нормализуются. получается, что на каждом шаге генерации порог отсечки может меняться. при этом ресурсозатраты метода не намного выше, чем у top-p. преимущество min-p в том, что этот метод подходит для разных температур — даже при высоком значении в 3-5. важный момент: температура должна применяться после min-p. авторы не рекомендуют использовать min-p с другими методами семплирования, хотя они и могут сочетаться. авторы тестировали метод на mistral 7b в трёх бенчмарках: gpqa main, gsm8k cot — которые проверяют конкретные знания — и alpacaeval creative writing. на первых двух бенчмарках min-p может показывать результаты чуть хуже, чем top-p при низких температурах. зато в alpacaeval creative writing, где осуществляется sbs-сравнение на креативных задачах, он строго лучше. разбор подготовил ❣ алексей малафеев душный nlp сбалансированный метод семплирования min-p min-p — метод семплирования, который, по словам его создателей, позволяет найти баланс между креативностью и связностью ответов. сегодня разберём статью с описанием этого подхода. при использовании отсекающего семплирования вроде top-p или top-k, на каждом шаге генерации после отсечения может всё ещё оставаться ненужный нам «хвост» из маловероятных токенов. это приводит к тому, что вероятность допустить ошибку на следующем шаге генерации — не нулевая. а токен — не воробей, вылетит — не поймаешь. из-за этого может пострадать весь ответ. это происходит потому что top-p и top-k предполагают применение жёсткого порога отсечения, который никак не зависит от шага генерации и уверенности модели в следующем токене. а подобрать универсальный порог на все случаи жизни невозможно. метод min-p пытается решить эту проблему с помощью динамической настройки порога в зависимости от токена с самой высокой вероятностью. если модель уверена в токене, то порог обрезки будет высокий. если сомневается — то из распределения возьмётся больше токенов. как это работает: 1. выбирается токен с наибольшей вероятностью — p max ; 2. гиперпараметр метода — базовый порог вероятности, p base (авторы рекомендуют выбирать между 0,05 и 0,1) — умножается на p max , и получается порог отсечки — p scaled . по нему отсекаются токены, всё, что ниже, выкидывается; 3.формируется пул для семплирования из оставшихся токенов; 4. вероятности нормализуются. получается, что на каждом шаге генерации порог отсечки может меняться. при этом ресурсозатраты метода не намного выше, чем у top-p. преимущество min-p в том, что этот метод подходит для разных температур — даже при высоком значении в 3-5. важный момент: температура должна применяться после min-p. авторы не рекомендуют использовать min-p с другими методами семплирования, хотя они и могут сочетаться. авторы тестировали метод на mistral 7b в трёх бенчмарках: gpqa main, gsm8k cot — которые проверяют конкретные знания — и alpacaeval creative writing. на первых двух бенчмарках min-p может показывать результаты чуть хуже, чем top-p при низких температурах. зато в alpacaeval creative writing, где осуществляется sbs-сравнение на креативных задачах, он строго лучше. разбор подготовил ❣ алексей малафеев душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-02-20T12:59:16+00:00" href="./posts/75.html">2025-02-20 12:59 UTC</a></div>
      </div>
      <div class="post-body"><strong>Сбалансированный метод семплирования  Min-p</strong><br><br>Min-p — метод семплирования, который, по словам его создателей, позволяет найти баланс между креативностью и связностью ответов. Сегодня разберём <a href="https://arxiv.org/abs/2407.01082" rel="nofollow noopener noreferrer">статью</a> с описанием этого подхода. <br><br>При использовании отсекающего семплирования вроде top-p или top-k, на каждом шаге генерации после отсечения может всё ещё оставаться ненужный нам «хвост» из маловероятных токенов. Это приводит к тому, что вероятность допустить ошибку на следующем шаге генерации — не нулевая. А токен — не воробей, вылетит — не поймаешь. Из-за этого может пострадать весь ответ. <br><br>Это происходит потому что top-p и top-k предполагают применение жёсткого порога отсечения, который никак не зависит от шага генерации и уверенности модели в следующем токене. А подобрать универсальный порог на все случаи жизни невозможно. <br><br>Метод Min-p пытается решить эту проблему с помощью динамической настройки порога в зависимости от токена с самой высокой вероятностью. Если модель уверена в токене, то порог обрезки будет высокий. Если сомневается — то из распределения возьмётся больше токенов. <br><br>Как это работает:<br><br>1. выбирается токен с наибольшей вероятностью — P<em>max</em>;<br><br>2. гиперпараметр метода — базовый порог вероятности, P<em>base</em> (авторы рекомендуют выбирать между 0,05 и 0,1) — умножается на P<em>max</em>, и получается порог отсечки — P<em>scaled</em>. По нему отсекаются токены, всё, что ниже, выкидывается;<br><br>3.формируется пул для семплирования из оставшихся токенов;<br><br>4. вероятности нормализуются. <br> <br>Получается, что на каждом шаге генерации порог отсечки может меняться. При этом ресурсозатраты метода не намного выше, чем у Top-p. <br><br>Преимущество Min-p в том, что этот метод подходит для разных температур — даже при высоком значении в 3-5. Важный момент: температура должна применяться после Min-p. Авторы не рекомендуют использовать Min-p с другими методами семплирования, хотя они и могут сочетаться.<br><br>Авторы тестировали метод на Mistral 7B в трёх бенчмарках: GPQA Main, GSM8K CoT — которые проверяют конкретные знания — и AlpacaEval Creative Writing. На первых двух бенчмарках Min-p может показывать результаты чуть хуже, чем Top-p при низких температурах. Зато в AlpacaEval Creative Writing, где осуществляется SbS-сравнение на креативных задачах, он строго лучше.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Алексей Малафеев</em><br><br><a href="https://t.me/+fxQsFHftsR44YTIy" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/75_480.webp" srcset="../assets/media/thumbs/75_480.webp 480w, ../assets/media/75.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="75" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/76_480.webp" srcset="../assets/media/thumbs/76_480.webp 480w, ../assets/media/76.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="75" data-image-index="1" /></div></div>
      <div class="actions">
        <span>4 445 просмотров · 36 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/75" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/75.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="72" data-search="технический отчёт deepseek-r1 deepseek-r1 — опенсорсная модель, которая на равных конкурирует с o1 от openai. сегодня разберём технический отчёт её разработчиков. компания deepseek сделала ставку на rl. в качестве инита взяли deepseek-v3-base и применили метод group relative policy optimization (grpo). система наград включала в себя две составляющие: accuracy reward и format reward. в задачах, связанных с математикой и программированием, получившаяся модель deepseek-r1-zero выдаёт ответы, по качеству на бенчмарках сопоставимые с ответами openai-01-0912. однако из-за accuracy reward модель стала тратить больше времени на раздумья. кроме того, у deepseek-r1-zero возникли способности к рефлексии (reflection) и саморазвитию (self-evolution). это значит, что модель учится переосмыслять свои генерации и самостоятельно обнаруживать в них ошибки. разработчики отмечают, что сами не ожидали такого. проблема deepseek-r1-zero заключалась в том, что её ответы были трудночитаемыми — модель могла перескакивать с языка на язык в рамках одной генерации. к тому же, они могли быть очень большими — до 10 тысяч токенов. плюс из отчёта не очень понятно, как модель показывает себя в задачах, не связанных с математикой и кодом. весь пайплайн создания финальной версии deepseek-r1 разделили на четыре этапа. первый — cold start. в его рамках делали sft, чтобы повысить читаемость (readability) генераций. в sft-датасет входили в том числе ответы r1-zero, исправленные людьми — как отметили в deepseek, это дало прирост качества по сравнению с r1-zero. второй этап был аналогичен тому, как обучали r1-zero, однако здесь к accuracy reward и format reward добавили language consistency reward, чтобы генерации были на одном языке. качество ответов немного снизилось, однако они больше нравились людям-оценщикам. третий этап — rejection fine-tuning. авторы генерировали ответы на тщательно отобранный набор инструкций и отбирали лучшие, пользуясь, помимо прочего, генеративными наградами на основе deepseek-v3. отсеивались ответы на нескольких языках, слишком длинные генерации и генерации, содержащие код. в итоге получилось 600 тысяч reasoning-примеров. sft-датасет deepseek-v3 использовали как основу для не-reasoning данных, а для ответов на некоторые инструкции генерировали cot и добавляли его перед ответом. всего получилось 200 тысяч non-reasoning-примеров. последний этап — rlhf. для reasoning-данных применялся тот же алгоритм, что и в r1-zero. а для общих данных — стандартные reward-модели, которые оценивали полезность по краткому решению задачи и финальному ответу. а для оценки безвредности рассматривали весь ответ, включая процесс рассуждения. получившаяся версия deepseek-r1 выигрывает у o1-1217 в пяти из бенчмарках из 11 — в том числе, во всех математических (первая таблица). в deepseek также взяли sft с v3, сгенерировали ответы с помощью r1 и дистиллировали полученные данные в открытые модели. rl на них не производился. в результате, например, qwen-7b стала сопоставима по качеству с gpt-4o-0513 (вторая таблица). однако стоит учесть, что авторы статьи сообщают только о показателях в математических бенчмарках и бенчмарках, проверяющих способность к генерации кода. в целом, исследование показало, что дистилляция даёт лучшие результаты, чем rl (третья таблица). но и тут есть оговорка, потому что сравнивали дистилляцию из r1, а rl осуществляли методом, как у r1-zero. авторам статьи не удалось извлечь пользу из метода process reward model (prm) на этапе обучения. это было связано со сложностью определения шагов рассуждения их корректности, а также с проблемой «взлома наград». также в deepseek безуспешно пробовали использовать в обучении monte carlo tree search (mcts), как в alphago и alphazero. здесь препятствием стало огромное пространство поиска, которое несравнимо больше, чем в случае с шахматами. ограничение по top-k при этом приводит к локальному оптимуму. кроме того, возникают сложности с обучением value-модели. тем не менее mcts способен повысить качество на инференсе. разбор подготовил ❣ дмитрий мокеев душный nlp технический отчёт deepseek-r1 deepseek-r1 — опенсорсная модель, которая на равных конкурирует с o1 от openai. сегодня разберём технический отчёт её разработчиков. компания deepseek сделала ставку на rl. в качестве инита взяли deepseek-v3-base и применили метод group relative policy optimization (grpo) . система наград включала в себя две составляющие: accuracy reward и format reward. в задачах, связанных с математикой и программированием, получившаяся модель deepseek-r1-zero выдаёт ответы, по качеству на бенчмарках сопоставимые с ответами openai-01-0912. однако из-за accuracy reward модель стала тратить больше времени на раздумья. кроме того, у deepseek-r1-zero возникли способности к рефлексии (reflection) и саморазвитию (self-evolution). это значит, что модель учится переосмыслять свои генерации и самостоятельно обнаруживать в них ошибки. разработчики отмечают, что сами не ожидали такого. проблема deepseek-r1-zero заключалась в том, что её ответы были трудночитаемыми — модель могла перескакивать с языка на язык в рамках одной генерации. к тому же, они могли быть очень большими — до 10 тысяч токенов. плюс из отчёта не очень понятно, как модель показывает себя в задачах, не связанных с математикой и кодом. весь пайплайн создания финальной версии deepseek-r1 разделили на четыре этапа. первый — cold start. в его рамках делали sft, чтобы повысить читаемость (readability) генераций. в sft-датасет входили в том числе ответы r1-zero, исправленные людьми — как отметили в deepseek, это дало прирост качества по сравнению с r1-zero. второй этап был аналогичен тому, как обучали r1-zero, однако здесь к accuracy reward и format reward добавили language consistency reward, чтобы генерации были на одном языке. качество ответов немного снизилось, однако они больше нравились людям-оценщикам. третий этап — rejection fine-tuning. авторы генерировали ответы на тщательно отобранный набор инструкций и отбирали лучшие, пользуясь, помимо прочего, генеративными наградами на основе deepseek-v3. отсеивались ответы на нескольких языках, слишком длинные генерации и генерации, содержащие код. в итоге получилось 600 тысяч reasoning-примеров. sft-датасет deepseek-v3 использовали как основу для не-reasoning данных, а для ответов на некоторые инструкции генерировали cot и добавляли его перед ответом. всего получилось 200 тысяч non-reasoning-примеров. последний этап — rlhf. для reasoning-данных применялся тот же алгоритм, что и в r1-zero. а для общих данных — стандартные reward-модели, которые оценивали полезность по краткому решению задачи и финальному ответу. а для оценки безвредности рассматривали весь ответ, включая процесс рассуждения. получившаяся версия deepseek-r1 выигрывает у o1-1217 в пяти из бенчмарках из 11 — в том числе, во всех математических ( первая таблица ). в deepseek также взяли sft с v3, сгенерировали ответы с помощью r1 и дистиллировали полученные данные в открытые модели. rl на них не производился. в результате, например, qwen-7b стала сопоставима по качеству с gpt-4o-0513 ( вторая таблица ). однако стоит учесть, что авторы статьи сообщают только о показателях в математических бенчмарках и бенчмарках, проверяющих способность к генерации кода. в целом, исследование показало, что дистилляция даёт лучшие результаты, чем rl ( третья таблица ). но и тут есть оговорка, потому что сравнивали дистилляцию из r1, а rl осуществляли методом, как у r1-zero. авторам статьи не удалось извлечь пользу из метода process reward model (prm) на этапе обучения. это было связано со сложностью определения шагов рассуждения их корректности, а также с проблемой «взлома наград». также в deepseek безуспешно пробовали использовать в обучении monte carlo tree search (mcts), как в alphago и alphazero. здесь препятствием стало огромное пространство поиска, которое несравнимо больше, чем в случае с шахматами. ограничение по top-k при этом приводит к локальному оптимуму. кроме того, возникают сложности с обучением value-модели. тем не менее mcts способен повысить качество на инференсе. разбор подготовил ❣ дмитрий мокеев душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-02-13T14:20:41+00:00" href="./posts/72.html">2025-02-13 14:20 UTC</a></div>
      </div>
      <div class="post-body"><strong>Технический отчёт DeepSeek-R1</strong><br><br>DeepSeek-R1 — опенсорсная модель, которая на равных конкурирует с o1 от OpenAI. Сегодня разберём <a href="https://arxiv.org/abs/2501.12948" rel="nofollow noopener noreferrer">технический отчёт</a> её разработчиков. <br><br>Компания DeepSeek сделала ставку на RL. В качестве инита взяли DeepSeek-V3-Base и применили метод <a href="https://arxiv.org/abs/2402.03300" rel="nofollow noopener noreferrer">Group Relative Policy Optimization (GRPO)</a>. Система наград включала в себя две составляющие: accuracy reward и format reward. В задачах, связанных с математикой и программированием, получившаяся модель DeepSeek-R1-Zero выдаёт ответы, по качеству на бенчмарках сопоставимые с ответами OpenAI-01-0912. <br><br>Однако из-за accuracy reward модель стала тратить больше времени на раздумья. Кроме того, у DeepSeek-R1-Zero возникли способности к рефлексии (reflection) и саморазвитию (self-evolution). Это значит, что модель учится переосмыслять свои генерации и самостоятельно обнаруживать в них ошибки. Разработчики отмечают, что сами не ожидали такого. <br><br>Проблема DeepSeek-R1-Zero заключалась в том, что её ответы были трудночитаемыми — модель могла перескакивать с языка на язык в рамках одной генерации. К тому же, они могли быть очень большими — до 10 тысяч токенов. Плюс из отчёта не очень понятно, как модель показывает себя в задачах, не связанных с математикой и кодом. <br><br>Весь пайплайн создания финальной версии DeepSeek-R1 разделили на четыре этапа. Первый — Cold Start. В его рамках делали SFT, чтобы повысить читаемость (readability) генераций. В SFT-датасет входили в том числе ответы R1-Zero, исправленные людьми — как отметили в DeepSeek, это дало прирост качества по сравнению с R1-Zero. <br><br>Второй этап был аналогичен тому, как обучали R1-Zero, однако здесь к accuracy reward и format reward добавили language consistency reward, чтобы генерации были на одном языке. Качество ответов немного снизилось, однако они больше нравились людям-оценщикам. <br><br>Третий этап — Rejection Fine-Tuning. Авторы генерировали ответы на тщательно отобранный набор инструкций и отбирали лучшие, пользуясь, помимо прочего, генеративными наградами на основе DeepSeek-V3. Отсеивались ответы на нескольких языках, слишком длинные генерации и генерации, содержащие код. В итоге получилось 600 тысяч reasoning-примеров. SFT-датасет DeepSeek-V3 использовали как основу для не-reasoning данных, а для ответов на некоторые инструкции генерировали CoT и добавляли его перед ответом. Всего получилось 200 тысяч non-reasoning-примеров. <br><br>Последний этап — RLHF. Для reasoning-данных применялся тот же алгоритм, что и в R1-Zero. А для общих данных — стандартные reward-модели, которые оценивали полезность по краткому решению задачи и финальному ответу. А для оценки безвредности рассматривали весь ответ, включая процесс рассуждения. <br><br>Получившаяся версия DeepSeek-R1 выигрывает у o1-1217 в пяти из бенчмарках из 11 — в том числе, во всех математических (<em>первая таблица</em>). <br><br>В DeepSeek также взяли SFT с V3, сгенерировали ответы с помощью R1 и дистиллировали полученные данные в открытые модели. RL на них не производился. В результате, например, Qwen-7B стала сопоставима по качеству с GPT-4o-0513 (<em>вторая таблица</em>). Однако стоит учесть, что авторы статьи сообщают только о показателях в математических бенчмарках и бенчмарках, проверяющих способность к генерации кода. <br><br>В целом, исследование показало, что дистилляция даёт лучшие результаты, чем RL (<em>третья таблица</em>). Но и тут есть оговорка, потому что сравнивали дистилляцию из R1, а RL осуществляли методом, как у R1-Zero. <br><br>Авторам статьи не удалось извлечь пользу из метода Process Reward Model (PRM) на этапе обучения. Это было связано со сложностью определения шагов рассуждения их корректности, а также с проблемой «взлома наград». <br><br>Также в DeepSeek безуспешно пробовали использовать в обучении Monte Carlo Tree Search (MCTS), как в AlphaGo и AlphaZero. Здесь препятствием стало огромное пространство поиска, которое несравнимо больше, чем в случае с шахматами. Ограничение по top-k при этом приводит к локальному оптимуму. Кроме того, возникают сложности с обучением Value-модели. Тем не менее MCTS способен повысить качество на инференсе. <br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Дмитрий Мокеев </em><br><br><a href="https://t.me/+vABO5T1rX1AzYzcy" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/72_480.webp" srcset="../assets/media/thumbs/72_480.webp 480w, ../assets/media/72.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="72" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/73_480.webp" srcset="../assets/media/thumbs/73_480.webp 480w, ../assets/media/73.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="72" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/74_480.webp" srcset="../assets/media/thumbs/74_480.webp 480w, ../assets/media/74.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="72" data-image-index="2" /></div></div>
      <div class="actions">
        <span>4 196 просмотров · 45 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/72" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/72.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="68" data-search="лучшие публикации 2024-го по мнению инженеров яндекса в 2024 году вышло так много статей по ml, что читать — не перечитать. руководитель команды yandexgpt alignment андрей бут поделился с нами работами, которые запомнились именно ему. есть, чем заняться на досуге! а в комментариях рассказывайте, какие публикации ушедшего года запомнились вам и почему. статьи, которые упоминает андрей: — is dpo superior to ppo for llm alignment? a comprehensive study — o1 replication journey: a strategic progress report — o1 replication journey — part 2 — техотчёт llama 3 — техотчёт qwen2.5 — техотчёт deepseek-v3 душный nlp лучшие публикации 2024-го по мнению инженеров яндекса в 2024 году вышло так много статей по ml, что читать — не перечитать. руководитель команды yandexgpt alignment андрей бут поделился с нами работами, которые запомнились именно ему. есть, чем заняться на досуге! а в комментариях рассказывайте, какие публикации ушедшего года запомнились вам и почему. статьи, которые упоминает андрей: — is dpo superior to ppo for llm alignment? a comprehensive study — o1 replication journey: a strategic progress report — o1 replication journey — part 2 — техотчёт llama 3 — техотчёт qwen2.5 — техотчёт deepseek-v3 душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-02-03T13:18:11+00:00" href="./posts/68.html">2025-02-03 13:18 UTC</a></div>
      </div>
      <div class="post-body"><strong>Лучшие публикации 2024-го по мнению инженеров Яндекса</strong><br><br>В 2024 году вышло так много статей по ML, что читать — не перечитать. Руководитель команды YandexGPT Alignment Андрей Бут поделился с нами работами, которые запомнились именно ему. Есть, чем заняться на досуге! А в комментариях рассказывайте, какие публикации ушедшего года запомнились вам и почему.<br><br><strong>Статьи, которые упоминает Андрей:</strong><br><br>— <a href="https://arxiv.org/abs/2404.10719" rel="nofollow noopener noreferrer">Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study</a> <br>— <a href="https://arxiv.org/abs/2410.18982" rel="nofollow noopener noreferrer">O1 Replication Journey: A Strategic Progress Report</a> <br>— <a href="https://arxiv.org/abs/2411.16489v1" rel="nofollow noopener noreferrer">O1 Replication Journey — Part 2</a>  <br>— <a href="https://arxiv.org/abs/2407.21783" rel="nofollow noopener noreferrer">Техотчёт Llama 3</a>  <br>— <a href="https://arxiv.org/abs/2412.15115" rel="nofollow noopener noreferrer">Техотчёт Qwen2.5</a> <br>— <a href="https://arxiv.org/abs/2412.19437" rel="nofollow noopener noreferrer">Техотчёт DeepSeek-V3</a><br><br><a href="https://t.me/+kLa297HgCuQxYzA6" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/68_480.webp" srcset="../assets/media/thumbs/68_480.webp 480w, ../assets/media/68.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="68" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/69_480.webp" srcset="../assets/media/thumbs/69_480.webp 480w, ../assets/media/69.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="68" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/70_480.webp" srcset="../assets/media/thumbs/70_480.webp 480w, ../assets/media/70.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="68" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/71_480.webp" srcset="../assets/media/thumbs/71_480.webp 480w, ../assets/media/71.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="68" data-image-index="3" /></div></div>
      <div class="actions">
        <span>4 721 просмотров · 65 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/68" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/68.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="65" data-search="интересные решения из технического отчёта deepseek-v3 — часть ii продолжаем разбираться, как устроена deepseek-v3, изучая технический отчёт её создателей. в первой части речь шла о mla и moe, а сегодня поговорим о предсказании токенов и квантизации. ну и, наконец, коснёмся результатов на бенчмарках mtp метод multi-token prediction (mtp) предполагает предсказание сразу нескольких токенов на этапе претрейна. в mtp эмбеддинг после всех трансформерных слоёв и перед выходной головой отправляется в дополнительный трансформерный блок (первое изображение). через линейную проекцию формируется новое представление, которое затем обрабатывается этим дополнительным блоком, эквивалентным по архитектуре стандартным слоям трансформера. итоговое представление используется для предсказания сразу нескольких токенов. pipeline parallelism pipeline parallelism — это метод распределения работы модели, при котором разные слои исполняются на разных устройствах. такой подход позволяет увеличивать количество шагов аккумуляции градиентов, уменьшая суммарное число коммуникаций. однако при pipeline parallelism образуются «пузыри» — это периоды времени, когда устройства простаивают в ожидании данных от других устройств. в решении этой проблемы создатели deepseek-v3 ссылаются на статью zero bubble. её идея состоит в разбивании backward на два шага: на ту часть, которая считает сквозные градиенты, и отдельно градиенты на веса. это полностью решает проблему «пузырей» в случаях, если количество шагов аккумуляции градиента в два раза больше пайплайна. однако остаётся проблема коммуникаций — простои могут возникать, например, когда одно устройство передаёт другому активации со слоя на слой. кроме того, в этом методе нужно хранить достаточно много промежуточных градиентов для вычисления градиента по весам. в deepseek изобрели метод dualpipe (второе изображение). каждый фрагмент forward и backward делится на четыре части: attention, all-to-all dispatch, mlp и all-to-all combine. кроме того, вводится компонент коммуникации в рамках pipeline parallelism (pp communication). создатели меняют местами эти части и вручную регулируют, сколько ресурсов gpu выделять на коммуникации и вычисления. микробатчи при этом подаются с обоих концов пайплайна одновременно. всё это позволяет минимизировать задержки, перекрывая коммуникации с вычислениями. «пузыри» всё равно остаются, но, как отмечают в deepseek, они несущественны. fp8-квантизация fp8-квантизация не очень подходит для больших моделей из-за широкого диапазона весов, активаций и градиентов. использование fp8-квантизации в таких условиях может приводить к выбросам, а следовательно, и потере качества. чтобы справиться с выбросами, создатели deepseek-v3 применяли блочную квантизацию. для весов брались блоки 128x128 — и для каждого такого блока вычислялся свой скейл. а для активаций размер блока составлял 1x128, чтобы выбросы, если они происходят, затрагивали лишь небольшую часть активаций. разработчики также выяснили, что gemm, реализованная на h800, аккумулирует результаты матричного умножения во что-то близкое к 14 битам, хотя должно быть 32 бита. из-за этого одно матричное умножение может приводить к ошибке в 2%. нехватку точности вручную добавляли к обычному fp32-регистру. итог благодаря всем сделанным разработчиками трюкам deepseek-v3 показывает отличные результаты в бенчмарках (третье изображение). на mmlu модель получает 88,5 процентных пункта, проигрывая лишь llama 3.1 на 405b с 88,6 пп. на математических задачах (math-500, aime 2024, cnmo 2024) deepseek — абсолютный лидер. в части программирования модель незначительно уступает claude-3.5. таким образом, deepseek-v3 — это одна из лучших опенсорсных моделей прямо сейчас. разбор подготовил ❣ михаил хрущев душный nlp интересные решения из технического отчёта deepseek-v3 — часть ii продолжаем разбираться, как устроена deepseek-v3, изучая технический отчёт её создателей. в первой части речь шла о mla и moe, а сегодня поговорим о предсказании токенов и квантизации. ну и, наконец, коснёмся результатов на бенчмарках mtp метод multi-token prediction (mtp) предполагает предсказание сразу нескольких токенов на этапе претрейна. в mtp эмбеддинг после всех трансформерных слоёв и перед выходной головой отправляется в дополнительный трансформерный блок ( первое изображение ). через линейную проекцию формируется новое представление, которое затем обрабатывается этим дополнительным блоком, эквивалентным по архитектуре стандартным слоям трансформера. итоговое представление используется для предсказания сразу нескольких токенов. pipeline parallelism pipeline parallelism — это метод распределения работы модели, при котором разные слои исполняются на разных устройствах. такой подход позволяет увеличивать количество шагов аккумуляции градиентов, уменьшая суммарное число коммуникаций. однако при pipeline parallelism образуются «пузыри» — это периоды времени, когда устройства простаивают в ожидании данных от других устройств. в решении этой проблемы создатели deepseek-v3 ссылаются на статью zero bubble. её идея состоит в разбивании backward на два шага: на ту часть, которая считает сквозные градиенты, и отдельно градиенты на веса. это полностью решает проблему «пузырей» в случаях, если количество шагов аккумуляции градиента в два раза больше пайплайна. однако остаётся проблема коммуникаций — простои могут возникать, например, когда одно устройство передаёт другому активации со слоя на слой. кроме того, в этом методе нужно хранить достаточно много промежуточных градиентов для вычисления градиента по весам. в deepseek изобрели метод dualpipe ( второе изображение ). каждый фрагмент forward и backward делится на четыре части: attention, all-to-all dispatch, mlp и all-to-all combine. кроме того, вводится компонент коммуникации в рамках pipeline parallelism (pp communication). создатели меняют местами эти части и вручную регулируют, сколько ресурсов gpu выделять на коммуникации и вычисления. микробатчи при этом подаются с обоих концов пайплайна одновременно. всё это позволяет минимизировать задержки, перекрывая коммуникации с вычислениями. «пузыри» всё равно остаются, но, как отмечают в deepseek, они несущественны. fp8-квантизация fp8-квантизация не очень подходит для больших моделей из-за широкого диапазона весов, активаций и градиентов. использование fp8-квантизации в таких условиях может приводить к выбросам, а следовательно, и потере качества. чтобы справиться с выбросами, создатели deepseek-v3 применяли блочную квантизацию. для весов брались блоки 128x128 — и для каждого такого блока вычислялся свой скейл. а для активаций размер блока составлял 1x128, чтобы выбросы, если они происходят, затрагивали лишь небольшую часть активаций. разработчики также выяснили, что gemm, реализованная на h800, аккумулирует результаты матричного умножения во что-то близкое к 14 битам, хотя должно быть 32 бита. из-за этого одно матричное умножение может приводить к ошибке в 2%. нехватку точности вручную добавляли к обычному fp32-регистру. итог благодаря всем сделанным разработчиками трюкам deepseek-v3 показывает отличные результаты в бенчмарках ( третье изображение ). на mmlu модель получает 88,5 процентных пункта, проигрывая лишь llama 3.1 на 405b с 88,6 пп. на математических задачах (math-500, aime 2024, cnmo 2024) deepseek — абсолютный лидер. в части программирования модель незначительно уступает claude-3.5. таким образом, deepseek-v3 — это одна из лучших опенсорсных моделей прямо сейчас. разбор подготовил ❣ михаил хрущев душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-01-31T09:46:21+00:00" href="./posts/65.html">2025-01-31 09:46 UTC</a></div>
      </div>
      <div class="post-body"><strong>Интересные решения из технического отчёта DeepSeek-V3 — часть II</strong><br><br>Продолжаем разбираться, как устроена DeepSeek-V3, изучая <a href="https://arxiv.org/abs/2412.19437" rel="nofollow noopener noreferrer">технический отчёт её создателей.</a> В первой части речь шла о MLA и MoE, а сегодня поговорим о предсказании токенов и квантизации. Ну и, наконец, коснёмся результатов на бенчмарках <br><br><strong>MTP</strong><br><br>Метод Multi-Token Prediction (MTP) предполагает предсказание сразу нескольких токенов на этапе претрейна. В MTP эмбеддинг после всех трансформерных слоёв и перед выходной головой отправляется в дополнительный трансформерный блок (<em>первое изображение</em>). <br><br>Через линейную проекцию формируется новое представление, которое затем обрабатывается этим дополнительным блоком, эквивалентным по архитектуре стандартным слоям трансформера. Итоговое представление используется для предсказания сразу нескольких токенов. <br><br><strong>Pipeline parallelism</strong><br><br>Pipeline parallelism — это метод распределения работы модели, при котором разные слои исполняются на разных устройствах. Такой подход позволяет увеличивать количество шагов аккумуляции градиентов, уменьшая суммарное число коммуникаций. Однако при pipeline parallelism образуются «пузыри» — это периоды времени, когда устройства простаивают в ожидании данных от других устройств. <br><br>В решении этой проблемы создатели DeepSeek-V3 ссылаются на <a href="https://arxiv.org/abs/2401.10241" rel="nofollow noopener noreferrer">статью Zero Bubble.</a> Её идея состоит в разбивании backward на два шага: на ту часть, которая считает сквозные градиенты, и отдельно градиенты на веса. Это полностью решает проблему «пузырей» в случаях, если количество шагов аккумуляции градиента в два раза больше пайплайна. Однако остаётся проблема коммуникаций — простои могут возникать, например, когда одно устройство передаёт другому активации со слоя на слой. Кроме того, в этом методе нужно хранить достаточно много промежуточных градиентов для вычисления градиента по весам.<br><br>В DeepSeek изобрели метод DualPipe (<em>второе изображение</em>). Каждый фрагмент forward и backward делится на четыре части: attention, all-to-all dispatch, MLP и all-to-all combine. Кроме того, вводится компонент коммуникации в рамках pipeline parallelism (PP communication). Создатели меняют местами эти части и вручную регулируют, сколько ресурсов GPU выделять на коммуникации и вычисления. Микробатчи при этом подаются с обоих концов пайплайна одновременно. Всё это позволяет минимизировать задержки, перекрывая коммуникации с вычислениями. «Пузыри» всё равно остаются, но, как отмечают в DeepSeek, они несущественны. <br><br><strong>FP8-квантизация</strong> <br><br>FP8-квантизация не очень подходит для больших моделей из-за широкого диапазона весов, активаций и градиентов. Использование FP8-квантизации в таких условиях может приводить к выбросам, а следовательно, и потере качества. <br><br>Чтобы справиться с выбросами, создатели DeepSeek-V3 применяли блочную квантизацию. Для весов брались блоки 128x128 — и для каждого такого блока вычислялся свой скейл. А для активаций размер блока составлял 1x128, чтобы выбросы, если они происходят, затрагивали лишь небольшую часть активаций. <br><br>Разработчики также выяснили, что GEMM, реализованная на H800, аккумулирует результаты матричного умножения во что-то близкое к 14 битам, хотя должно быть 32 бита. Из-за этого одно матричное умножение может приводить к ошибке в 2%. Нехватку точности вручную добавляли к обычному FP32-регистру. <br><br><strong>Итог</strong><br><br>Благодаря всем сделанным разработчиками трюкам DeepSeek-V3 показывает отличные результаты в бенчмарках (<em>третье изображение</em>). На MMLU модель получает 88,5 процентных пункта, проигрывая лишь Llama 3.1 на 405B с 88,6 пп. На математических задачах (MATH-500, AIME 2024, CNMO 2024) DeepSeek — абсолютный лидер. В части программирования модель незначительно уступает Claude-3.5. Таким образом, DeepSeek-V3 — это одна из лучших опенсорсных моделей прямо сейчас.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Михаил Хрущев</em><br><br><a href="https://t.me/+a_C8ULYxmr8yMmMy" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/65_480.webp" srcset="../assets/media/thumbs/65_480.webp 480w, ../assets/media/65.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="65" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/66_480.webp" srcset="../assets/media/thumbs/66_480.webp 480w, ../assets/media/66.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="65" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/67_480.webp" srcset="../assets/media/thumbs/67_480.webp 480w, ../assets/media/67.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="65" data-image-index="2" /></div></div>
      <div class="actions">
        <span>4 211 просмотров · 63 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/65" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/65.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="64" data-search="интересные решения из технического отчёта deepseek-v3 — часть i в конце прошлого года вышел технический отчёт модели deepseek-v3. у неё 671 миллиардов параметров, из которых активные — 37 миллиардов (то есть меньше 1/16). обучение длилось два месяца на 2 тысячах gpu h800. впервые в истории llm обучали на fp8 и с высокой степенью разреженности (sparsity). полученная модель вошла в топ-10 на chatbot arena. кроме того, deepseek-v3 хорошо показывала себя в бенчмарках. изучили технический отчёт и рассказываем, какие необычные и даже новаторские решения в нём есть. обзор получился объёмным, поэтому мы поделили его на два поста. mla метод, который называется multi-head latent attention (mla), используют как альтернативу grouped query attention (gqa) для снижения объёма kv-кэша. этот подход апробировали ещё в июне 2024 года в deepseek-v2. как утверждают разработчики, по качеству mla превосходит gqa и multi-query attention и сопоставим с multi-head attention. суть mla заключается в сжатии скрытого представления в латентные вектора и хранении их на продакшене вместо key value. во время генерации токенов kv восстанавливается из латентных векторов, что требует отдельных, но не слишком затратных вычислений. такой подход позволяет здорово экономить память, однако лишает возможности применять rotary position embedding (rope) — способ несовместим с низкоранговым сжатием kv. чтобы обойти проблему, в deepseek прибегли к методу decoupled rotary position embedding. он предполагает добавление к каждой голове вектора с rope. в результате не происходит деградации качества из-за того, что для большей части каждой головы позиционные эмбеддинги не обрабатываются. при этом модель сохраняет способность учитывать очень длинные контексты, так как её производительность не ухудшается даже при значительном удалении токенов от начальной позиции. после претрейна разработчики расширили контекст, используя yarn-подход (yet another rope extension) — c 4 тысяч токенов до 128 тысяч. в тесте needle in a haystack, по условиям которого нужно найти ответ на вопрос в контексте на 128 тысяч токенов, deepseek-v3 в 100% случаев справлялась с задачей. сама по себе она несложная, но демонстрирует умение модели работать с большими контекстами. moe по сравнению с deepseek-v2 изменился подход к mixture-of-experts. здесь есть общие эксперты (shared experts), которые применяются ко всем входным токенам, и маршрутизируемые эксперты (routed experts), среди которых выбираются лучшие для решения конкретной задачи. специальный лосс для контроля загруженности экспертов не используется. вместо этого для каждого эксперта вводится определённый bias, через который и осуществляется балансировка. если эксперт вызывается слишком часто, то bias уменьшается, слишком редко — увеличивается. благодаря этому обучение получается более стабильным, чем при использовании лоссов. однако разработчикам всё-таки пришлось ввести балансировочный лосс. описанный выше способ приводит к тому, что эксперты становятся домен-специфичными. дополнительный лосс позволяет избежать этого, создавая разнообразие в выдаче. благодаря хорошей балансировке нагрузки в процессе обучения deepseek-v3 не отбрасывает ни одного токена. для снижения затрат на коммуникации использовали метод node-limited routing. он вводит ограничение на число хостов, куда может быть направлен токен. сперва выбираются хосты, которые содержат необходимых экспертов, а затем среди них с помощью алгоритма top-k routing выбираются лучшие для конкретного токена. во второй части расскажем о fp8-квантизации, результатах на бенчмарках и не только. не переключайтесь! разбор подготовил ❣ михаил хрущев душный nlp интересные решения из технического отчёта deepseek-v3 — часть i в конце прошлого года вышел технический отчёт модели deepseek-v3. у неё 671 миллиардов параметров, из которых активные — 37 миллиардов (то есть меньше 1/16). обучение длилось два месяца на 2 тысячах gpu h800. впервые в истории llm обучали на fp8 и с высокой степенью разреженности (sparsity). полученная модель вошла в топ-10 на chatbot arena. кроме того, deepseek-v3 хорошо показывала себя в бенчмарках. изучили технический отчёт и рассказываем, какие необычные и даже новаторские решения в нём есть. обзор получился объёмным, поэтому мы поделили его на два поста. mla метод, который называется multi-head latent attention (mla), используют как альтернативу grouped query attention (gqa) для снижения объёма kv-кэша. этот подход апробировали ещё в июне 2024 года в deepseek-v2. как утверждают разработчики, по качеству mla превосходит gqa и multi-query attention и сопоставим с multi-head attention. суть mla заключается в сжатии скрытого представления в латентные вектора и хранении их на продакшене вместо key value. во время генерации токенов kv восстанавливается из латентных векторов, что требует отдельных, но не слишком затратных вычислений. такой подход позволяет здорово экономить память, однако лишает возможности применять rotary position embedding (rope) — способ несовместим с низкоранговым сжатием kv. чтобы обойти проблему, в deepseek прибегли к методу decoupled rotary position embedding. он предполагает добавление к каждой голове вектора с rope. в результате не происходит деградации качества из-за того, что для большей части каждой головы позиционные эмбеддинги не обрабатываются. при этом модель сохраняет способность учитывать очень длинные контексты, так как её производительность не ухудшается даже при значительном удалении токенов от начальной позиции. после претрейна разработчики расширили контекст, используя yarn-подход (yet another rope extension) — c 4 тысяч токенов до 128 тысяч. в тесте needle in a haystack, по условиям которого нужно найти ответ на вопрос в контексте на 128 тысяч токенов, deepseek-v3 в 100% случаев справлялась с задачей. сама по себе она несложная, но демонстрирует умение модели работать с большими контекстами. moe по сравнению с deepseek-v2 изменился подход к mixture-of-experts. здесь есть общие эксперты (shared experts), которые применяются ко всем входным токенам, и маршрутизируемые эксперты (routed experts), среди которых выбираются лучшие для решения конкретной задачи. специальный лосс для контроля загруженности экспертов не используется. вместо этого для каждого эксперта вводится определённый bias, через который и осуществляется балансировка. если эксперт вызывается слишком часто, то bias уменьшается, слишком редко — увеличивается. благодаря этому обучение получается более стабильным, чем при использовании лоссов. однако разработчикам всё-таки пришлось ввести балансировочный лосс. описанный выше способ приводит к тому, что эксперты становятся домен-специфичными. дополнительный лосс позволяет избежать этого, создавая разнообразие в выдаче. благодаря хорошей балансировке нагрузки в процессе обучения deepseek-v3 не отбрасывает ни одного токена. для снижения затрат на коммуникации использовали метод node-limited routing. он вводит ограничение на число хостов, куда может быть направлен токен. сперва выбираются хосты, которые содержат необходимых экспертов, а затем среди них с помощью алгоритма top-k routing выбираются лучшие для конкретного токена. во второй части расскажем о fp8-квантизации, результатах на бенчмарках и не только. не переключайтесь! разбор подготовил ❣ михаил хрущев душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-01-28T11:39:57+00:00" href="./posts/64.html">2025-01-28 11:39 UTC</a></div>
      </div>
      <div class="post-body"><strong>Интересные решения из технического отчёта DeepSeek-V3 — часть I</strong><br><br>В конце прошлого года вышел <a href="https://arxiv.org/pdf/2412.19437" rel="nofollow noopener noreferrer">технический отчёт модели DeepSeek-V3.</a> У неё 671 миллиардов параметров, из которых активные — 37 миллиардов (то есть меньше 1/16). Обучение длилось два месяца на 2 тысячах GPU H800. Впервые в истории LLM обучали на FP8 и с высокой степенью разреженности (sparsity). Полученная модель вошла в топ-10 на Chatbot Arena. Кроме того, DeepSeek-V3 хорошо показывала себя в бенчмарках. <br><br>Изучили технический отчёт и рассказываем, какие необычные и даже новаторские решения в нём есть. Обзор получился объёмным, поэтому мы поделили его на два поста. <br><br><strong>MLA</strong><br><br>Метод, который называется Multi-head Latent Attention (MLA), используют как альтернативу Grouped Query Attention (GQA) для снижения объёма KV-кэша. Этот подход апробировали ещё в июне 2024 года в DeepSeek-V2. Как утверждают разработчики, по качеству MLA превосходит GQA и Multi-Query Attention и сопоставим с Multi-Head Attention. <br><br>Суть MLA заключается в сжатии скрытого представления в латентные вектора и хранении их на продакшене вместо Key Value. Во время генерации токенов KV восстанавливается из латентных векторов, что требует отдельных, но не слишком затратных вычислений. <br><br>Такой подход позволяет здорово экономить память, однако лишает возможности применять Rotary Position Embedding (RoPE) — способ несовместим с низкоранговым сжатием KV. Чтобы обойти проблему, в DeepSeek прибегли к методу Decoupled Rotary Position Embedding. Он предполагает добавление к каждой голове вектора с RoPE.<br><br>В результате не происходит деградации качества из-за того, что для большей части каждой головы позиционные эмбеддинги не обрабатываются. При этом модель сохраняет способность учитывать очень длинные контексты, так как её производительность не ухудшается даже при значительном удалении токенов от начальной позиции.<br><br>После претрейна разработчики расширили контекст, используя <a href="https://arxiv.org/abs/2309.00071" rel="nofollow noopener noreferrer">YaRN-подход (Yet another RoPE extension)</a> — c 4 тысяч токенов до 128 тысяч. В тесте Needle In A Haystack, по условиям которого нужно найти ответ на вопрос в контексте на 128 тысяч токенов, DeepSeek-V3 в 100% случаев справлялась с задачей. Сама по себе она несложная, но демонстрирует умение модели работать с большими контекстами. <br><br><strong>MoE</strong><br><br>По сравнению с DeepSeek-V2 изменился подход к Mixture-of-Experts. Здесь есть общие эксперты (Shared experts), которые применяются ко всем входным токенам, и маршрутизируемые эксперты (Routed experts), среди которых выбираются лучшие для решения конкретной задачи.<br> <br>Специальный лосс для контроля загруженности экспертов не используется. Вместо этого для каждого эксперта вводится определённый bias, через который и осуществляется балансировка. Если эксперт вызывается слишком часто, то bias уменьшается, слишком редко — увеличивается. Благодаря этому обучение получается более стабильным, чем при использовании лоссов. <br><br>Однако разработчикам всё-таки пришлось ввести балансировочный лосс. Описанный выше способ приводит к тому, что эксперты становятся домен-специфичными. Дополнительный лосс позволяет избежать этого, создавая разнообразие в выдаче. Благодаря хорошей балансировке нагрузки в процессе обучения DeepSeek-V3 не отбрасывает ни одного токена. <br><br>Для снижения затрат на коммуникации использовали метод Node-Limited Routing. Он вводит ограничение на число хостов, куда может быть направлен токен. Сперва выбираются хосты, которые содержат необходимых экспертов, а затем среди них с помощью алгоритма top-K routing выбираются лучшие для конкретного токена. <br><br>Во второй части расскажем о FP8-квантизации, результатах на бенчмарках и не только. Не переключайтесь!<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Михаил Хрущев</em><br><br><a href="https://t.me/+jC7SNy45dgk1ZmYy" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/64_480.webp" srcset="../assets/media/thumbs/64_480.webp 480w, ../assets/media/64.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="64" data-image-index="0" /></div></div>
      <div class="actions">
        <span>4 609 просмотров · 103 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/64" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/64.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="63" data-search="физика языковых моделей: серия статей от fair в прошлом году не успели поделиться с вами разбором полезной работы от исследователя лаборатории fair (facebook ai research) цзэюань аллен-чжу. исправляемся! в двухчасовом докладе и серии статей описаны свойства языковых моделей и приведено множество инсайтов. самые интересные идеи: — llm способна выучивать графы причинно-следственных связей для решения сложных задач. (пример задачи: «в школе 10 аудиторий, в каждой по 15 парт, за каждой партой — два стула. сколько всего стульев в школе?»). если задача более запутанная, со сложным графом зависимостей, модель может использовать топологическую сортировку, чтобы понять порядок вычислений. для повышения точности этого процесса предлагается обучать модель на дополнительных синтетических задачах. описанный подход называется level-1 reasoning. — перед генерацией первого токена llm заранее просчитывает все промежуточные данные для построения ответа. иногда модель допускает ошибки: вычисляет лишние данные или пытается оперировать тем, что ещё не вычислено. этот процесс называется level-2 reasoning. — llm способна определить, где произошла ошибка при генерации ответа. для этого можно обучить linear probe, который с вероятностью около 99% предскажет место ошибки. автор также предлагает добавить в обучающую выборку примеры сhain-of-thought, где модель не только ошибается, но и исправляет свои ошибки. чем больше таких данных, тем выше становится качество модели. — ещё одна синтетическая задача для обучения модели — определение принадлежности последовательности грамматике хомского. задача формулируется так: даны правила раскрытия токенов (например: 5 → 43, 4 → 22, 3 → 11), и нужно понять, можно ли с их помощью сгенерировать последовательность 1122 (можно) или 1212 (нельзя). оказалось, gpt не только успешно обучается на такую задачу, но и умеет определять, из каких нетерминальных токенов были сгенерированы подпоследовательности. при этом bert, хотя и справляется с задачей, не оперирует понятием нетерминальных токенов. дополнительные инсайты: — претрейн должен содержать qa и другие задачи извлечения знаний (knowledge extraction, ne). если только файнтюн содержит ne, модель будет плохо обобщаться на подобные задачи. — ошибки в данных на претрейне снижают качество, и стадия файнтюнинга не исправляет ситуацию. — аугментации заметно улучшают обобщающую способность модели. это неудивительно, ведь они работают с перестановкой предложений, переформулировкой, стилистикой, переводами. — модели типа bert работают хуже, чем gpt-архитектуры. это объясняется авторегрессионной природой вторых, которая позволяет эффективнее генерировать и предсказывать последовательности (см. пункт о грамматике хомского). — использование сhain-of-thought улучшает точность модели. — gatedffn работает хуже, чем обычный ffn. — универсальный закон: эффективность хранения информации (выученной из тренировочного датасета) в хорошо обученной llm равна двум битам на один параметр модели. этот закон действует для широкого диапазона размеров llm. разбор подготовил ❣ александр шишеня душный nlp — meta признана экстремистской организацией, а facebook и instagram запрещены на территории рф физика языковых моделей: серия статей от fair в прошлом году не успели поделиться с вами разбором полезной работы от исследователя лаборатории fair (facebook ai research) цзэюань аллен-чжу. исправляемся! в двухчасовом докладе и серии статей описаны свойства языковых моделей и приведено множество инсайтов. самые интересные идеи: — llm способна выучивать графы причинно-следственных связей для решения сложных задач. (пример задачи: «в школе 10 аудиторий, в каждой по 15 парт, за каждой партой — два стула. сколько всего стульев в школе?»). если задача более запутанная, со сложным графом зависимостей, модель может использовать топологическую сортировку, чтобы понять порядок вычислений. для повышения точности этого процесса предлагается обучать модель на дополнительных синтетических задачах. описанный подход называется level-1 reasoning. — перед генерацией первого токена llm заранее просчитывает все промежуточные данные для построения ответа. иногда модель допускает ошибки: вычисляет лишние данные или пытается оперировать тем, что ещё не вычислено. этот процесс называется level-2 reasoning. — llm способна определить, где произошла ошибка при генерации ответа. для этого можно обучить linear probe, который с вероятностью около 99% предскажет место ошибки. автор также предлагает добавить в обучающую выборку примеры сhain-of-thought, где модель не только ошибается, но и исправляет свои ошибки. чем больше таких данных, тем выше становится качество модели. — ещё одна синтетическая задача для обучения модели — определение принадлежности последовательности грамматике хомского. задача формулируется так: даны правила раскрытия токенов (например: 5 → 43, 4 → 22, 3 → 11), и нужно понять, можно ли с их помощью сгенерировать последовательность 1122 (можно) или 1212 (нельзя). оказалось, gpt не только успешно обучается на такую задачу, но и умеет определять, из каких нетерминальных токенов были сгенерированы подпоследовательности. при этом bert, хотя и справляется с задачей, не оперирует понятием нетерминальных токенов. дополнительные инсайты: — претрейн должен содержать qa и другие задачи извлечения знаний (knowledge extraction, ne). если только файнтюн содержит ne, модель будет плохо обобщаться на подобные задачи. — ошибки в данных на претрейне снижают качество, и стадия файнтюнинга не исправляет ситуацию. — аугментации заметно улучшают обобщающую способность модели. это неудивительно, ведь они работают с перестановкой предложений, переформулировкой, стилистикой, переводами. — модели типа bert работают хуже, чем gpt-архитектуры. это объясняется авторегрессионной природой вторых, которая позволяет эффективнее генерировать и предсказывать последовательности (см. пункт о грамматике хомского). — использование сhain-of-thought улучшает точность модели. — gatedffn работает хуже, чем обычный ffn. — универсальный закон: эффективность хранения информации (выученной из тренировочного датасета) в хорошо обученной llm равна двум битам на один параметр модели. этот закон действует для широкого диапазона размеров llm. разбор подготовил ❣ александр шишеня душный nlp — meta признана экстремистской организацией, а facebook и instagram запрещены на территории рф">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-01-17T07:36:30+00:00" href="./posts/63.html">2025-01-17 07:36 UTC</a></div>
      </div>
      <div class="post-body"><strong>Физика языковых моделей: серия статей от FAIR</strong><br><br>В прошлом году не успели поделиться с вами разбором полезной <a href="https://physics.allen-zhu.com/home" rel="nofollow noopener noreferrer">работы</a> от исследователя лаборатории FAIR (Facebook AI Research) Цзэюань Аллен-Чжу. Исправляемся! В двухчасовом докладе и серии статей описаны свойства языковых моделей и приведено множество инсайтов.<br><br><strong>Самые интересные идеи:</strong><br><br>— LLM способна выучивать графы причинно-следственных связей для решения сложных задач. <em>(Пример задачи: «В школе 10 аудиторий, в каждой по 15 парт, за каждой партой — два стула. Сколько всего стульев в школе?»). </em>Если задача более запутанная, со сложным графом зависимостей, модель может использовать топологическую сортировку, чтобы понять порядок вычислений. Для повышения точности этого процесса предлагается обучать модель на дополнительных синтетических задачах. Описанный подход называется Level-1 reasoning.<br><br>— Перед генерацией первого токена LLM заранее просчитывает все промежуточные данные для построения ответа. Иногда модель допускает ошибки: вычисляет лишние данные или пытается оперировать тем, что ещё не вычислено. Этот процесс называется Level-2 reasoning.<br><br>— LLM способна определить, где произошла ошибка при генерации ответа. Для этого можно обучить Linear Probe, который с вероятностью около 99% предскажет место ошибки. Автор также предлагает добавить в обучающую выборку примеры Сhain-of-Thought, где модель не только ошибается, но и исправляет свои ошибки. Чем больше таких данных, тем выше становится качество модели.<br><br>— Ещё одна синтетическая задача для обучения модели — определение принадлежности последовательности грамматике Хомского. Задача формулируется так: даны правила раскрытия токенов (например: 5 → 43, 4 → 22, 3 → 11), и нужно понять, можно ли с их помощью сгенерировать последовательность 1122 (можно) или 1212 (нельзя). Оказалось, GPT не только успешно обучается на такую задачу, но и умеет определять, из каких нетерминальных токенов были сгенерированы подпоследовательности. При этом BERT, хотя и справляется с задачей, не оперирует понятием нетерминальных токенов.<br><br><strong>Дополнительные инсайты:</strong><br><br>— Претрейн должен содержать QA и другие задачи извлечения знаний (Knowledge Extraction, NE). Если только файнтюн содержит NE, модель будет плохо обобщаться на подобные задачи.<br>— Ошибки в данных на претрейне снижают качество, и стадия файнтюнинга не исправляет ситуацию.<br>— Аугментации заметно улучшают обобщающую способность модели. Это неудивительно, ведь они работают с перестановкой предложений, переформулировкой, стилистикой, переводами.<br>— Модели типа BERT работают хуже, чем GPT-архитектуры. Это объясняется авторегрессионной природой вторых, которая позволяет эффективнее генерировать и предсказывать последовательности (см. пункт о грамматике Хомского). <br>— Использование Сhain-of-Thought улучшает точность модели.<br>— GatedFFN работает хуже, чем обычный FFN.<br>— Универсальный закон: эффективность хранения информации (выученной из тренировочного датасета) в хорошо обученной LLM равна двум битам на один параметр модели. Этот закон действует для широкого диапазона размеров LLM.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Александр</em> <em>Шишеня</em><br><br><a href="https://t.me/+xt57km_nQyA3YzIy" rel="nofollow noopener noreferrer">Душный NLP</a><br>—<br><em>Meta признана экстремистской организацией, а Facebook и Instagram запрещены на территории РФ</em></div>
      <div class="actions">
        <span>5 457 просмотров · 53 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/63" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/63.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="61" data-search="теория игр в rlhf врываемся в новый 2025-й с новым разбором. сегодня рассмотрим, как использование теории игр помогает в rlhf. авторы статьи назвали свой метод general preference modeling (gpm). он подходит для rlhf, но всё сказанное в статье применимо и к rl общего назначения с моделью предпочтений. rlhf-обучение состоит из двух шагов: получение модели предпочтений и обучение генеративной модели. авторы сосредоточились на улучшении модели предпочтений. традиционно такая модель учится как модель брэдли-терри. в ней каждому возможному ответу llm присваивается некоторый скаляр, отражающий его качество. утверждается, что пользователь предпочитает ответы с более высоким качеством. при этом подходе ответы получаются линейно упорядоченными и всегда можно найти лучший, что несомненный плюс. из недостатков — модель предполагает, что функция предпочтений человека всегда описывается достаточно простой моделью брэдли-терри, в которой всегда есть лучшие и худшие ответы. на практике же несколько ответов могут образовывать нетранзитивный цикл. скажем, при игре в «камень-ножницы-бумага» нет лучшего действия, однако модель всё равно попытается их упорядочить. чтобы решить эту проблему, авторы обращаются к теории игр. на самом деле эта идея не нова. скажем, были уже модели попарных сравнений, которые работали не от оценки одного ответа, а от сравнения двух вариантов. у такого подхода тоже есть существенные недостатки — это удар по производительности и отсутствие гарантии антисимметричности. то есть попарная модель должна оценивать, насколько первый ответ лучше второго противоположным числом оценки превосходства второго ответа над первым. свой подход авторы статьи строят на основе эмбеддингов. в отличие от модели брэдли терри, где каждому ответу приписывается скаляр, здесь приписывается некоторый эмбеддинг. после этого с помощью любой антисимметричной билинейной формы от эмбедингов двух ответов происходит проверка факта, что один из них лучше другого. в такой модели по построению выполняется свойство антисимметричности. также авторы предлагают конкретный вид для билинейной формы, обеспечивающий простоту вычисления. в случае, если эмбеддинг состоит из двух скаляров, авторы предлагают в качестве антисимметричного билинейного оператора использовать матрицу поворота на 90 градусов в двухмерном пространстве. а для случаев с большей размерностью эмбеддингов — строить блочно-диагональные матрицы с вышеописанными матрицами поворота на диагонали (вторая картинка выше). такой оператор способен моделировать циклы в предпочтениях пользователей, располагая эмбеддинги на окружности. далее оптимизация языковой модели в соответствии с предпочтениями пользователя представляется в виде матричной игры, где разные llm играют друг с другом. выигрыш считается в соответствии с полученной ранее моделью предпочтений как среднее значение предложенной билинейной формы от эмбэддингов ответов двух соревнующихся моделей. равновесием по нэшу для такой игры будет модель, которая в среднем обыгрывает все остальные. эту модель и предлагают искать авторы статьи, для чего адаптируют итеративный алгоритм из теории игр. оказывается, что шаг полученного алгоритма эквивалентен запуску классического rl-обучения. здесь reward для конкретного ответа считается как среднее значение билинейной формы предпочтений для первого ответа и ответа, сгенерированного моделью с предыдущего шага этого алгоритма. к сожалению, на практике среднее значение билинейной формы не берётся из-за необходимости сэмплировать ответы из модели с предыдущего шага. поэтому авторы в своём алгоритме используют монте-карло оценку этого среднего. тестирование показало, что обобщённая модель предпочтений даёт большую точность на циклических датасетах, но на стандартных результат сопоставим с тем, который показывают классические методы. однако при использовании новой reward-функции в rl качество генеративной модели значительно улучшается. разбор подготовил ❣ федор лебедь душный nlp теория игр в rlhf врываемся в новый 2025-й с новым разбором. сегодня рассмотрим, как использование теории игр помогает в rlhf. авторы статьи назвали свой метод general preference modeling (gpm). он подходит для rlhf, но всё сказанное в статье применимо и к rl общего назначения с моделью предпочтений. rlhf-обучение состоит из двух шагов: получение модели предпочтений и обучение генеративной модели. авторы сосредоточились на улучшении модели предпочтений. традиционно такая модель учится как модель брэдли-терри. в ней каждому возможному ответу llm присваивается некоторый скаляр, отражающий его качество. утверждается, что пользователь предпочитает ответы с более высоким качеством. при этом подходе ответы получаются линейно упорядоченными и всегда можно найти лучший, что несомненный плюс. из недостатков — модель предполагает, что функция предпочтений человека всегда описывается достаточно простой моделью брэдли-терри, в которой всегда есть лучшие и худшие ответы. на практике же несколько ответов могут образовывать нетранзитивный цикл. скажем, при игре в «камень-ножницы-бумага» нет лучшего действия, однако модель всё равно попытается их упорядочить. чтобы решить эту проблему, авторы обращаются к теории игр. на самом деле эта идея не нова. скажем, были уже модели попарных сравнений, которые работали не от оценки одного ответа, а от сравнения двух вариантов. у такого подхода тоже есть существенные недостатки — это удар по производительности и отсутствие гарантии антисимметричности. то есть попарная модель должна оценивать, насколько первый ответ лучше второго противоположным числом оценки превосходства второго ответа над первым. свой подход авторы статьи строят на основе эмбеддингов. в отличие от модели брэдли терри, где каждому ответу приписывается скаляр, здесь приписывается некоторый эмбеддинг. после этого с помощью любой антисимметричной билинейной формы от эмбедингов двух ответов происходит проверка факта, что один из них лучше другого. в такой модели по построению выполняется свойство антисимметричности. также авторы предлагают конкретный вид для билинейной формы, обеспечивающий простоту вычисления. в случае, если эмбеддинг состоит из двух скаляров, авторы предлагают в качестве антисимметричного билинейного оператора использовать матрицу поворота на 90 градусов в двухмерном пространстве. а для случаев с большей размерностью эмбеддингов — строить блочно-диагональные матрицы с вышеописанными матрицами поворота на диагонали (вторая картинка выше). такой оператор способен моделировать циклы в предпочтениях пользователей, располагая эмбеддинги на окружности. далее оптимизация языковой модели в соответствии с предпочтениями пользователя представляется в виде матричной игры, где разные llm играют друг с другом. выигрыш считается в соответствии с полученной ранее моделью предпочтений как среднее значение предложенной билинейной формы от эмбэддингов ответов двух соревнующихся моделей. равновесием по нэшу для такой игры будет модель, которая в среднем обыгрывает все остальные. эту модель и предлагают искать авторы статьи, для чего адаптируют итеративный алгоритм из теории игр. оказывается, что шаг полученного алгоритма эквивалентен запуску классического rl-обучения. здесь reward для конкретного ответа считается как среднее значение билинейной формы предпочтений для первого ответа и ответа, сгенерированного моделью с предыдущего шага этого алгоритма. к сожалению, на практике среднее значение билинейной формы не берётся из-за необходимости сэмплировать ответы из модели с предыдущего шага. поэтому авторы в своём алгоритме используют монте-карло оценку этого среднего. тестирование показало, что обобщённая модель предпочтений даёт большую точность на циклических датасетах, но на стандартных результат сопоставим с тем, который показывают классические методы. однако при использовании новой reward-функции в rl качество генеративной модели значительно улучшается. разбор подготовил ❣ федор лебедь душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-01-09T09:00:28+00:00" href="./posts/61.html">2025-01-09 09:00 UTC</a></div>
      </div>
      <div class="post-body"><strong>Теория игр в RLHF</strong><br><br>Врываемся в новый 2025-й с новым разбором. Сегодня рассмотрим, как использование теории игр помогает в RLHF. <a href="https://arxiv.org/abs/2410.02197v1" rel="nofollow noopener noreferrer">Авторы статьи</a> назвали свой метод General Preference Modeling (GPM). Он подходит для RLHF, но всё сказанное в статье применимо и к RL общего назначения с моделью предпочтений. <br><br>RLHF-обучение состоит из двух шагов: получение модели предпочтений и обучение генеративной модели. Авторы сосредоточились на улучшении модели предпочтений. <br><br>Традиционно такая модель учится как модель Брэдли-Терри. В ней каждому возможному ответу LLM присваивается некоторый скаляр, отражающий его качество. Утверждается, что пользователь предпочитает ответы с более высоким качеством. При этом подходе ответы получаются линейно упорядоченными и всегда можно найти лучший, что несомненный плюс. <br><br>Из недостатков — модель предполагает, что функция предпочтений человека всегда описывается достаточно простой моделью Брэдли-Терри, в которой всегда есть лучшие и худшие ответы. На практике же несколько ответов могут образовывать нетранзитивный цикл. Скажем, при игре в «Камень-ножницы-бумага» нет лучшего действия, однако модель всё равно попытается их упорядочить.<br><br>Чтобы решить эту проблему, авторы обращаются к теории игр. На самом деле эта идея не нова. Скажем, были уже модели попарных сравнений, которые работали не от оценки одного ответа, а от сравнения двух вариантов. У такого подхода тоже есть существенные недостатки — это удар по производительности и отсутствие гарантии антисимметричности. То есть попарная модель должна оценивать, насколько первый ответ лучше второго противоположным числом оценки превосходства второго ответа над первым. <br><br>Свой подход авторы статьи строят на основе эмбеддингов. В отличие от модели Брэдли Терри, где каждому ответу приписывается скаляр, здесь приписывается некоторый эмбеддинг. После этого с помощью любой антисимметричной билинейной формы от эмбедингов двух ответов происходит проверка факта, что один из них лучше другого. В такой модели по построению выполняется свойство антисимметричности. Также авторы предлагают конкретный вид для билинейной формы, обеспечивающий простоту вычисления.<br><br>В случае, если эмбеддинг состоит из двух скаляров, авторы предлагают в качестве антисимметричного билинейного оператора использовать матрицу поворота на 90 градусов в двухмерном пространстве. А для случаев с большей размерностью эмбеддингов — строить блочно-диагональные матрицы с вышеописанными матрицами поворота на диагонали (вторая картинка выше). Такой оператор способен моделировать циклы в предпочтениях пользователей, располагая эмбеддинги на окружности. <br><br>Далее оптимизация языковой модели в соответствии с предпочтениями пользователя представляется в виде матричной игры, где разные LLM играют друг с другом. Выигрыш считается в соответствии с полученной ранее моделью предпочтений как среднее значение предложенной билинейной формы от эмбэддингов ответов двух соревнующихся моделей. Равновесием по Нэшу для такой игры будет модель, которая в среднем обыгрывает все остальные. Эту модель и предлагают искать авторы статьи, для чего адаптируют итеративный алгоритм из теории игр.<br><br>Оказывается, что шаг полученного алгоритма эквивалентен запуску классического RL-обучения. Здесь reward для конкретного ответа считается как среднее значение билинейной формы предпочтений для первого ответа и ответа, сгенерированного моделью с предыдущего шага этого алгоритма. К сожалению, на практике среднее значение билинейной формы не берётся из-за необходимости сэмплировать ответы из модели с предыдущего шага. Поэтому авторы в своём алгоритме используют Монте-Карло оценку этого среднего.<br><br>Тестирование показало, что обобщённая модель предпочтений даёт большую точность на циклических датасетах, но на стандартных результат сопоставим с тем, который показывают классические методы. Однако при использовании новой reward-функции в RL качество генеративной модели значительно улучшается. <br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Федор Лебедь</em><br><br><a href="https://t.me/+xt57km_nQyA3YzIy" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/61_480.webp" srcset="../assets/media/thumbs/61_480.webp 480w, ../assets/media/61.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="61" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/62_480.webp" srcset="../assets/media/thumbs/62_480.webp 480w, ../assets/media/62.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="61" data-image-index="1" /></div></div>
      <div class="actions">
        <span>4 797 просмотров · 43 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/61" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/61.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="60" data-search="🏆 лучшее за год в душном nlp конец года — время подводить итоги и рассказывать о достижениях. спасибо, что подписывались, читали, комментировали и ставили реакции на посты. о самых популярных из них мы сейчас и расскажем. хороший способ вспомнить интересное или наверстать упущенное. масштабирование и параметризация разбор сразу двух статей о сохранении стабильности гиперпараметров при масштабировании модели. авторы одной работы предлагают озаботиться правильным масштабированием инициализаций и послойных lr, либо напрямую спектральной нормализацией матриц весов и их обновлений в процессе обучения. авторы другой статьи считают, что обновления весов в градиентном спуске имеют низкий ранг и хорошо согласуются с векторами активаций. specexec: cпекулятивное декодирование для запуска больших моделей на потребительских gpu метод specexec позволяет генерировать до 20 токенов за итерацию и достигает ускорения x15 при офлодинге. производительность достигается за счёт использования высокой пиковости распределений вероятностей токенов в современных llm. remax как альтернатива ppo авторы статьи предлагают замену алгоритму proximal policy optimization (ppo). в отличие от последнего, в remax в качестве бейзлайна не используется отдельная value-модель. благодаря этому снижение потребления gpu-памяти составляет до 46%. ограничения instruction tuning и как их преодолеть авторы рассматривают то, как lora (low-rank adaptation) может помочь нивелировать недостатки sft. эксперименты показывают, что lora-модель действительно даёт более точные ответы, при этом метод остаётся эффективным даже на небольших датасетах. tdpo — потокенный dpo или просто регуляризация? разбор статьи, авторы которой предложили метод потокеннного dpo. на деле всё оказывается не так просто. результаты действительно улучшились, но, похоже, что авторы просто добавили регуляризацию. mixture-of-agents — простой способ улучшения ответов llm суть метода mixture-of-agents заключается в использовании нескольких llm на разных слоях для генерации ответов на один и тот же вопрос. результат превосходит по качеству то, что выдавала бы одна модель. что такое дистилляция и как она применяется в llm дистилляция — это передача знаний от тяжёлой модели более лёгкой. в посте рассказали, как появилась дистилляция и какие методы в ней используют, например, применение датасета reward-модели и расширение набора данных с помощью генерации с разными параметрами. deepseek-v2 — moe-модель с технологией mla deepseek-v2 — модель на 236 миллиардов параметров. модифицированный attention mla (multi-head latent attention) позволяет ей значительно снизить объём ресурсов, необходимых для работы. карточки с интересными статьями с iclr и два поста с занимательными статьями с icml а ещё в этом году мы побывали на конференциях iclr и icml — и увидели там очень много интересных статей. обо всех них мы рассказывали в серии постов. душный nlp 🏆 лучшее за год в душном nlp конец года — время подводить итоги и рассказывать о достижениях. спасибо, что подписывались, читали, комментировали и ставили реакции на посты. о самых популярных из них мы сейчас и расскажем. хороший способ вспомнить интересное или наверстать упущенное. масштабирование и параметризация разбор сразу двух статей о сохранении стабильности гиперпараметров при масштабировании модели. авторы одной работы предлагают озаботиться правильным масштабированием инициализаций и послойных lr, либо напрямую спектральной нормализацией матриц весов и их обновлений в процессе обучения. авторы другой статьи считают, что обновления весов в градиентном спуске имеют низкий ранг и хорошо согласуются с векторами активаций. specexec: cпекулятивное декодирование для запуска больших моделей на потребительских gpu метод specexec позволяет генерировать до 20 токенов за итерацию и достигает ускорения x15 при офлодинге. производительность достигается за счёт использования высокой пиковости распределений вероятностей токенов в современных llm. remax как альтернатива ppo авторы статьи предлагают замену алгоритму proximal policy optimization (ppo). в отличие от последнего, в remax в качестве бейзлайна не используется отдельная value-модель. благодаря этому снижение потребления gpu-памяти составляет до 46%. ограничения instruction tuning и как их преодолеть авторы рассматривают то, как lora (low-rank adaptation) может помочь нивелировать недостатки sft. эксперименты показывают, что lora-модель действительно даёт более точные ответы, при этом метод остаётся эффективным даже на небольших датасетах. tdpo — потокенный dpo или просто регуляризация? разбор статьи, авторы которой предложили метод потокеннного dpo. на деле всё оказывается не так просто. результаты действительно улучшились, но, похоже, что авторы просто добавили регуляризацию. mixture-of-agents — простой способ улучшения ответов llm суть метода mixture-of-agents заключается в использовании нескольких llm на разных слоях для генерации ответов на один и тот же вопрос. результат превосходит по качеству то, что выдавала бы одна модель. что такое дистилляция и как она применяется в llm дистилляция — это передача знаний от тяжёлой модели более лёгкой. в посте рассказали, как появилась дистилляция и какие методы в ней используют, например, применение датасета reward-модели и расширение набора данных с помощью генерации с разными параметрами. deepseek-v2 — moe-модель с технологией mla deepseek-v2 — модель на 236 миллиардов параметров. модифицированный attention mla (multi-head latent attention) позволяет ей значительно снизить объём ресурсов, необходимых для работы. карточки с интересными статьями с iclr и два поста с занимательными статьями с icml а ещё в этом году мы побывали на конференциях iclr и icml — и увидели там очень много интересных статей. обо всех них мы рассказывали в серии постов. душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-12-25T08:42:11+00:00" href="./posts/60.html">2024-12-25 08:42 UTC</a></div>
      </div>
      <div class="post-body"><strong><tg-emoji emoji-id="5341761825170017804">🏆</tg-emoji></strong><strong> Лучшее за год в Душном NLP</strong><br><br>Конец года — время подводить итоги и рассказывать о достижениях. Спасибо, что подписывались, читали, комментировали и ставили реакции на посты. О самых популярных из них мы сейчас и расскажем. Хороший способ вспомнить интересное или наверстать упущенное.<br><br><a href="https://t.me/stuffyNLP/28" rel="nofollow noopener noreferrer"><strong>Масштабирование и параметризация</strong></a><br><br>Разбор сразу двух статей о сохранении стабильности гиперпараметров при масштабировании модели. Авторы одной работы предлагают озаботиться правильным масштабированием инициализаций и послойных LR, либо напрямую спектральной нормализацией матриц весов и их обновлений в процессе обучения. Авторы другой статьи считают, что обновления весов в градиентном спуске имеют низкий ранг и хорошо согласуются с векторами активаций.<br><br><a href="https://t.me/stuffyNLP/22" rel="nofollow noopener noreferrer"><strong>SpecExec: cпекулятивное декодирование для запуска больших моделей на потребительских GPU </strong></a><br><br>Метод SpecExec позволяет генерировать до 20 токенов за итерацию и достигает ускорения x15 при офлодинге. Производительность достигается за счёт использования высокой пиковости распределений вероятностей токенов в современных LLM.<br><br><a href="https://t.me/stuffyNLP/36" rel="nofollow noopener noreferrer"><strong>ReMax как альтернатива PPO </strong></a><br><br>Авторы статьи предлагают замену алгоритму Proximal Policy Optimization (PPO). В отличие от последнего, в ReMax в качестве бейзлайна не используется отдельная value-модель. Благодаря этому снижение потребления GPU-памяти составляет до 46%.<br><br><a href="https://t.me/stuffyNLP/39" rel="nofollow noopener noreferrer"><strong>Ограничения Instruction Tuning и как их преодолеть </strong></a><br><br>Авторы рассматривают то, как LoRA (Low-Rank Adaptation) может помочь нивелировать недостатки SFT. Эксперименты показывают, что LoRA-модель действительно даёт более точные ответы, при этом метод остаётся эффективным даже на небольших датасетах. <br><br><a href="https://t.me/stuffyNLP/43" rel="nofollow noopener noreferrer"><strong>TDPO — потокенный DPO или просто регуляризация? </strong></a><br><br>Разбор статьи, авторы которой предложили метод потокеннного DPO. На деле всё оказывается не так просто. Результаты действительно улучшились, но, похоже, что авторы просто добавили регуляризацию. <br><br><a href="https://t.me/stuffyNLP/25" rel="nofollow noopener noreferrer"><strong>Mixture-of-Agents — простой способ улучшения ответов LLM</strong></a><br><br>Суть метода Mixture-of-Agents заключается в использовании нескольких LLM на разных слоях для генерации ответов на один и тот же вопрос. Результат превосходит по качеству то, что выдавала бы одна модель. <br><br><a href="https://t.me/stuffyNLP/3" rel="nofollow noopener noreferrer"><strong>Что такое дистилляция и как она применяется в LLM</strong></a><br><br>Дистилляция — это передача знаний от тяжёлой модели более лёгкой. В посте рассказали, как появилась дистилляция и какие методы в ней используют, например, применение датасета reward-модели и расширение набора данных с помощью генерации с разными параметрами.<br><br><a href="https://t.me/stuffyNLP/27" rel="nofollow noopener noreferrer"><strong>DeepSeek-V2 — MoE-модель с технологией MLA </strong></a><br><br>DeepSeek-V2 — модель на 236 миллиардов параметров. Модифицированный attention MLA (Multi-Head Latent Attention) позволяет ей значительно снизить объём ресурсов, необходимых для работы. <br><br><a href="https://t.me/stuffyNLP/11" rel="nofollow noopener noreferrer"><strong>Карточки с интересными статьями с ICLR</strong></a><strong> и </strong><a href="https://t.me/stuffyNLP/29" rel="nofollow noopener noreferrer"><strong>два поста</strong></a><strong> с </strong><a href="https://t.me/stuffyNLP/30?single" rel="nofollow noopener noreferrer"><strong>занимательными статьями с ICML</strong></a><strong> </strong><br><br>А ещё в этом году мы побывали на конференциях ICLR и ICML — и увидели там очень много интересных статей. Обо всех них мы рассказывали в серии постов. <br><br><a href="https://t.me/+tz5DCjm4NoE3ZWVi" rel="nofollow noopener noreferrer">Душный NLP</a></div>
      <div class="actions">
        <span>5 264 просмотров · 55 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/60" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/60.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="59" data-search="технический отчёт qwen2.5 создатели линейки языковых моделей qwen2.5 представили технический отчёт. вот что мы из него узнали. претрейн на претрейне использовали датасет объёмом 18 триллионов токенов против 7 триллионов у qwen 2. в частности, были данные, применявшиеся для обучения qwen2.5-math и qwen2.5-coder, что позволило улучшить результаты модели в вопросах, связанных с математикой и программированием. также применяли синтетические данные, сгенерированные qwen2. scaling laws использовали для предсказания оптимальных гиперпараметров — например, для learning rate или вычисления размера батча. во время первой фазы претрейна длина контекста составляла 4096 токенов, а на второй и финальной — 32 768 токенов для всех моделей семейства, кроме qwen2.5-turbo. в её случае претрейн проходил в четыре этапа, начинаясь с 32 768 токенов и заканчивая 262 144 токенами. в каждой фазе претрейна qwen2.5-turbo максимального значения достигали только 40% данных, а остальные были короче. по словам авторов, это позволило модели плавно адаптироваться к новой длине контекста. благодаря стратегиям yarn и dual chunk attention удалось увеличить максимальную длину обрабатываемой на инференсе последовательности в четыре раза: до миллиона токенов у qwen2.5-turbo и до 131 072 токенов у других версий. алаймент sft-датасет состоял из более чем миллиона примеров. длина выхода qwen2.5 — 8192 токена, в то время как обычно она составляет менее 2000. улучшения удалось добиться благодаря наборам данных для длинных ответов. разработчики использовали back-translation, чтобы генерировать запросы на основе данных для предварительного обучения, ограничивали длину выхода и отфильтровывали низкокачественные пары с помощью qwen2. для задач, связанных с математикой, использовали cot-данные из qwen2.5-math. кроме того, применяли rejection sampling вместе с размеченными данными и моделью награды для пошагового рассуждения. что касается генерации кода, то здесь было несколько агентов и пары инструкций на примерно 40 языках программирования. в части instruction following модели генерировали инструкции, проверочные коды и юнит-тесты для перекрёстной проверки. это позволило llm лучше следовать промптам. а благодаря внедрению цепочек рассуждений в ответы, qwen2.5 стала лучше извлекать информацию из структурированных данных — например, таблиц. использовали также модель перевода инструкций с высокоресурсных на низкоресурсные языки. каждый полученный ответ проходил оценку на семантическое соответствие оригиналу, что позволило сохранить логическую структуру и стилистику текста. разработчики создали сотни системных промптов, чтобы обеспечить согласованность между ними и диалогами. для оценки качества ответов применяли несколько методов автоматической аннотации, включая специализированную модель-критика и систему коллективной оценки с участием нескольких агентов. сохраняли только те ответы, которые все системы оценки посчитали безупречными. на этапе dpo в качестве позитивных примеров использовали хорошие ответы с sft. те же, которые не прошли проверку на sft, стали негативными примерами. для создания датасета задействовали как автоматические, так и ручные методы оценки. в итоге получился набор данных из 150 тысяч пар. qwen2.5 обучалась на нём в течение одной эпохи с применением online merging optimizer c learning rate 7 × 10⁻⁷. reward-модель тренировали на двух наборах данных: общедоступном и проприетарном, содержащем запросы со сложной структурой. ответы генерировались с чекпоинтов qwen-моделей, прошедших файнтюнинг разными методами (sft, dpo, rl), и при разных температурах. в качестве алгоритма для онлайн rl использовали group relative policy optimization (grpo). набор запросов для обучения reward-модели был идентичен тому, что применялся на этапе обучения с подкреплением. для каждого запроса отбирали по 8 ответов. душный nlp технический отчёт qwen2.5 создатели линейки языковых моделей qwen2.5 представили технический отчёт . вот что мы из него узнали. претрейн на претрейне использовали датасет объёмом 18 триллионов токенов против 7 триллионов у qwen 2. в частности, были данные, применявшиеся для обучения qwen2.5-math и qwen2.5-coder, что позволило улучшить результаты модели в вопросах, связанных с математикой и программированием. также применяли синтетические данные, сгенерированные qwen2. scaling laws использовали для предсказания оптимальных гиперпараметров — например, для learning rate или вычисления размера батча. во время первой фазы претрейна длина контекста составляла 4096 токенов, а на второй и финальной — 32 768 токенов для всех моделей семейства, кроме qwen2.5-turbo. в её случае претрейн проходил в четыре этапа, начинаясь с 32 768 токенов и заканчивая 262 144 токенами. в каждой фазе претрейна qwen2.5-turbo максимального значения достигали только 40% данных, а остальные были короче. по словам авторов, это позволило модели плавно адаптироваться к новой длине контекста. благодаря стратегиям yarn и dual chunk attention удалось увеличить максимальную длину обрабатываемой на инференсе последовательности в четыре раза: до миллиона токенов у qwen2.5-turbo и до 131 072 токенов у других версий. алаймент sft-датасет состоял из более чем миллиона примеров. длина выхода qwen2.5 — 8192 токена, в то время как обычно она составляет менее 2000. улучшения удалось добиться благодаря наборам данных для длинных ответов. разработчики использовали back-translation, чтобы генерировать запросы на основе данных для предварительного обучения, ограничивали длину выхода и отфильтровывали низкокачественные пары с помощью qwen2. для задач, связанных с математикой, использовали cot-данные из qwen2.5-math. кроме того, применяли rejection sampling вместе с размеченными данными и моделью награды для пошагового рассуждения. что касается генерации кода, то здесь было несколько агентов и пары инструкций на примерно 40 языках программирования. в части instruction following модели генерировали инструкции, проверочные коды и юнит-тесты для перекрёстной проверки. это позволило llm лучше следовать промптам. а благодаря внедрению цепочек рассуждений в ответы, qwen2.5 стала лучше извлекать информацию из структурированных данных — например, таблиц. использовали также модель перевода инструкций с высокоресурсных на низкоресурсные языки. каждый полученный ответ проходил оценку на семантическое соответствие оригиналу, что позволило сохранить логическую структуру и стилистику текста. разработчики создали сотни системных промптов, чтобы обеспечить согласованность между ними и диалогами. для оценки качества ответов применяли несколько методов автоматической аннотации, включая специализированную модель-критика и систему коллективной оценки с участием нескольких агентов. сохраняли только те ответы, которые все системы оценки посчитали безупречными. на этапе dpo в качестве позитивных примеров использовали хорошие ответы с sft. те же, которые не прошли проверку на sft, стали негативными примерами. для создания датасета задействовали как автоматические, так и ручные методы оценки. в итоге получился набор данных из 150 тысяч пар. qwen2.5 обучалась на нём в течение одной эпохи с применением online merging optimizer c learning rate 7 × 10⁻⁷. reward-модель тренировали на двух наборах данных: общедоступном и проприетарном, содержащем запросы со сложной структурой. ответы генерировались с чекпоинтов qwen-моделей, прошедших файнтюнинг разными методами (sft, dpo, rl), и при разных температурах. в качестве алгоритма для онлайн rl использовали group relative policy optimization (grpo). набор запросов для обучения reward-модели был идентичен тому, что применялся на этапе обучения с подкреплением. для каждого запроса отбирали по 8 ответов. душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-12-20T14:38:50+00:00" href="./posts/59.html">2024-12-20 14:38 UTC</a></div>
      </div>
      <div class="post-body"><strong>Технический отчёт Qwen2.5</strong><br><br>Создатели линейки языковых моделей Qwen2.5 представили <a href="https://arxiv.org/abs/2412.15115" rel="nofollow noopener noreferrer">технический отчёт</a>. Вот что мы из него узнали. <br><br><strong>Претрейн</strong><br><br>На претрейне использовали датасет объёмом 18 триллионов токенов против 7 триллионов у Qwen 2. В частности, были данные, применявшиеся для обучения Qwen2.5-Math и Qwen2.5-Coder, что позволило улучшить результаты модели в вопросах, связанных с математикой и программированием. Также применяли синтетические данные, сгенерированные Qwen2. Scaling laws использовали для предсказания оптимальных гиперпараметров — например, для learning rate или вычисления размера батча. <br><br>Во время первой фазы претрейна длина контекста составляла 4096 токенов, а на второй и финальной — 32 768 токенов для всех моделей семейства, кроме Qwen2.5-Turbo. В её случае претрейн проходил в четыре этапа, начинаясь с 32 768 токенов и заканчивая 262 144 токенами. В каждой фазе претрейна Qwen2.5-Turbo максимального значения достигали только 40% данных, а остальные были короче. По словам авторов, это позволило модели плавно адаптироваться к новой длине контекста. <br><br>Благодаря стратегиям <a href="https://arxiv.org/abs/2309.00071" rel="nofollow noopener noreferrer">YaRN</a> и <a href="https://arxiv.org/abs/2402.17463" rel="nofollow noopener noreferrer">Dual Chunk Attention</a> удалось увеличить максимальную длину обрабатываемой на инференсе последовательности в четыре раза: до миллиона токенов у Qwen2.5-Turbo и до 131 072 токенов у других версий. <br><br><strong>Алаймент</strong><br><br>SFT-датасет состоял из более чем миллиона примеров. Длина выхода Qwen2.5 — 8192 токена, в то время как обычно она составляет менее 2000. Улучшения удалось добиться благодаря наборам данных для длинных ответов. Разработчики использовали back-translation, чтобы генерировать запросы на основе данных для предварительного обучения, ограничивали длину выхода и отфильтровывали низкокачественные пары с помощью Qwen2. <br><br>Для задач, связанных с математикой, использовали CoT-данные из Qwen2.5-Math. Кроме того, применяли rejection sampling вместе с размеченными данными и моделью награды для пошагового рассуждения. Что касается генерации кода, то здесь было несколько агентов и пары инструкций на примерно 40 языках программирования. <br><br>В части instruction following модели генерировали инструкции, проверочные коды и юнит-тесты для перекрёстной проверки. Это позволило LLM лучше следовать промптам. А благодаря внедрению цепочек рассуждений в ответы, Qwen2.5 стала лучше извлекать информацию из структурированных данных — например, таблиц. <br><br>Использовали также модель перевода инструкций с высокоресурсных на низкоресурсные языки. Каждый полученный ответ проходил оценку на семантическое соответствие оригиналу, что позволило сохранить логическую структуру и стилистику текста. <br><br>Разработчики создали сотни системных промптов, чтобы обеспечить согласованность между ними и диалогами. Для оценки качества ответов применяли несколько методов автоматической аннотации, включая специализированную модель-критика и систему коллективной оценки с участием нескольких агентов. Сохраняли только те ответы, которые все системы оценки посчитали безупречными.<br><br>На этапе DPO в качестве позитивных примеров использовали хорошие ответы с SFT. Те же, которые не прошли проверку на SFT, стали негативными примерами. <br><br>Для создания датасета задействовали как автоматические, так и ручные методы оценки. В итоге получился набор данных из 150 тысяч пар. Qwen2.5 обучалась на нём в течение одной эпохи с применением Online Merging Optimizer c learning rate 7 × 10⁻⁷. <br><br>Reward-модель тренировали на двух наборах данных: общедоступном и проприетарном, содержащем запросы со сложной структурой. Ответы генерировались с чекпоинтов Qwen-моделей, прошедших файнтюнинг разными методами (SFT, DPO, RL), и при разных температурах. В качестве алгоритма для онлайн RL использовали Group Relative Policy Optimization (GRPO). Набор запросов для обучения reward-модели был идентичен тому, что применялся на этапе обучения с подкреплением. Для каждого запроса отбирали по 8 ответов. <br><br><a href="https://t.me/+w94lL1lFHEFjMGUy" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/59_480.webp" srcset="../assets/media/thumbs/59_480.webp 480w, ../assets/media/59.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="59" data-image-index="0" /></div></div>
      <div class="actions">
        <span>6 672 просмотров · 64 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/59" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/59.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="58" data-search="интересные статьи с neurips 2024 devbench: a multimodal developmental benchmark for language learning одна из многих работ о бенчмарках. здесь авторы предлагают мультимодальный бенчмарк с информацией о том, как себя на нём проявляют люди разных возрастов. создатели стремятся проверить: правда ли модели учатся и растут примерно как дети. ответ положительный, однако люди лучше показывают себя в неоднозначных ситуациях, потому что понимают контекст. по словам авторов, их работа может давать представление о возможных путях развития языковых моделей. llm evaluators recognize and favor their own generations модели всё чаще используют для оценки их же ответов. такой метод называют self-evaluation, однако у него есть потенциальные проблемы. в частности, из-за самопредпочтения (self-preference) — llm может оценивать собственные ответы выше остальных. авторы статьи проверяют, влияет ли способность модели узнавать свои тексты на предпочтения. выясняется, что да — особенно у gpt-4 и llama 2. таким образом, чтобы использовать модель вместо ассесора для разметки, нужно выяснить, в чём она предвзята. выводы статьи могут быть полезны для определения сгенерированных текстов и нахождения схожести между моделями. lingoly: a benchmark of olympiad-level linguistic reasoning puzzles in low-resource and extinct languages ещё один текст о бенчмарке — на этот раз для оценки способностей к рассуждению. он состоит из задач о низкоресурсных — то есть таких, о которых мало данных для обучения — языках из олимпиад по лингвистике. всего в бенчмарке 1133 задачи по 90 языкам. lingoly получился сложным — лучше всего себя показала claude opus, но и она набрала менее 20%. not all tokens are what you need for pretraining авторы выдвигают гипотезу: не все токены на претрейне одинаково важны. чтобы доказать это, создают модель rho-1. с помощью неё считают перплексию для всего претрейна. а затем обучают модель на самых значимых токенах. на ряде бенчмарков действительно получили прирост в 20-30 процентных пунктов. бенчмарки были в основном математические и научные. интересное увидела ❣ анастасия беззубцева #yaneurips душный nlp интересные статьи с neurips 2024 devbench: a multimodal developmental benchmark for language learning одна из многих работ о бенчмарках. здесь авторы предлагают мультимодальный бенчмарк с информацией о том, как себя на нём проявляют люди разных возрастов. создатели стремятся проверить: правда ли модели учатся и растут примерно как дети. ответ положительный, однако люди лучше показывают себя в неоднозначных ситуациях, потому что понимают контекст. по словам авторов, их работа может давать представление о возможных путях развития языковых моделей. llm evaluators recognize and favor their own generations модели всё чаще используют для оценки их же ответов. такой метод называют self-evaluation, однако у него есть потенциальные проблемы. в частности, из-за самопредпочтения (self-preference) — llm может оценивать собственные ответы выше остальных. авторы статьи проверяют, влияет ли способность модели узнавать свои тексты на предпочтения. выясняется, что да — особенно у gpt-4 и llama 2. таким образом, чтобы использовать модель вместо ассесора для разметки, нужно выяснить, в чём она предвзята. выводы статьи могут быть полезны для определения сгенерированных текстов и нахождения схожести между моделями. lingoly: a benchmark of olympiad-level linguistic reasoning puzzles in low-resource and extinct languages ещё один текст о бенчмарке — на этот раз для оценки способностей к рассуждению. он состоит из задач о низкоресурсных — то есть таких, о которых мало данных для обучения — языках из олимпиад по лингвистике. всего в бенчмарке 1133 задачи по 90 языкам. lingoly получился сложным — лучше всего себя показала claude opus, но и она набрала менее 20%. not all tokens are what you need for pretraining авторы выдвигают гипотезу: не все токены на претрейне одинаково важны. чтобы доказать это, создают модель rho-1. с помощью неё считают перплексию для всего претрейна. а затем обучают модель на самых значимых токенах. на ряде бенчмарков действительно получили прирост в 20-30 процентных пунктов. бенчмарки были в основном математические и научные. интересное увидела ❣ анастасия беззубцева #yaneurips душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-12-19T09:09:34+00:00" href="./posts/58.html">2024-12-19 09:09 UTC</a></div>
      </div>
      <div class="post-body"><strong>Интересные статьи с NeurIPS 2024</strong><br><br><a href="https://arxiv.org/abs/2406.10215" rel="nofollow noopener noreferrer"><strong>DEVBENCH: A multimodal developmental benchmark for language learning</strong></a><br><br>Одна из многих работ о бенчмарках. Здесь авторы предлагают мультимодальный бенчмарк с информацией о том, как себя на нём проявляют люди разных возрастов. Создатели стремятся проверить: правда ли модели учатся и растут примерно как дети. Ответ положительный, однако люди лучше показывают себя в неоднозначных ситуациях, потому что понимают контекст. По словам авторов, их работа может давать представление о возможных путях развития языковых моделей. <br><br><a href="https://arxiv.org/abs/2404.13076" rel="nofollow noopener noreferrer"><strong>LLM Evaluators Recognize and Favor Their Own Generations</strong></a><br><br>Модели всё чаще используют для оценки их же ответов. Такой метод называют Self-evaluation, однако у него есть потенциальные проблемы. В частности, из-за самопредпочтения (self-preference) — LLM может оценивать собственные ответы выше остальных. Авторы статьи проверяют, влияет ли способность модели узнавать свои тексты на предпочтения. Выясняется, что да — особенно у GPT-4 и Llama 2. Таким образом, чтобы использовать модель вместо ассесора для разметки, нужно выяснить, в чём она предвзята. Выводы статьи могут быть полезны для определения сгенерированных текстов и нахождения схожести между моделями. <br><br><a href="https://arxiv.org/abs/2406.06196" rel="nofollow noopener noreferrer"><strong>LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in Low-Resource and Extinct Languages</strong></a><br><br>Ещё один текст о бенчмарке — на этот раз для оценки способностей к рассуждению. Он состоит из задач о низкоресурсных — то есть таких, о которых мало данных  для обучения — языках из олимпиад по лингвистике. Всего в бенчмарке 1133 задачи по 90 языкам. LINGOLY получился сложным — лучше всего себя показала Claude Opus, но и она набрала менее 20%. <br><br><a href="https://arxiv.org/abs/2404.07965" rel="nofollow noopener noreferrer"><strong>Not All Tokens Are What You Need for Pretraining</strong></a><br><br>Авторы выдвигают гипотезу: не все токены на претрейне одинаково важны. Чтобы доказать это, создают модель RHO-1. С помощью неё считают перплексию для всего претрейна. А затем обучают модель на самых значимых токенах. На ряде бенчмарков действительно получили прирост в 20-30 процентных пунктов. Бенчмарки были в основном математические и научные. <br><br><em>Интересное увидела </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Анастасия Беззубцева</em><br><br>#YaNeurIPS<br><br><a href="https://t.me/+PR-lLwc9aP42ZTJi" rel="nofollow noopener noreferrer">Душный NLP</a></div>
      <div class="actions">
        <span>3 749 просмотров · 34 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/58" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/58.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="53" data-search="toolkengpt и toolken+: расширение возможностей языковых моделей за счёт интеграции инструментов сегодня разбираем две статьи. первая описывает парадигму обучения инструментов toolkengpt. вторая представляет развитие этой концепции, предложенное константином яковлевым, сергеем николенко и андреем бутом из яндекса. toolkengpt: как научить модель напрямую вызывать внешние функции в первой работе исследователи предложили представить каждый внешний инструмент в виде токена — toolken (represents each tool as a token) — и выучивать его эмбеддинг. модель обучается работать с такими токенами так же, как с обычными текстовыми. в результате работу модели можно условно разделить на две стадии: 1) режим “reasoning” — генерация происходит, как обычно, с той лишь разницей, что добавленные toolken тоже рассматриваются в качестве вероятных токенов на каждом шаге генерации; 2) режим “tool” — когда следующим предсказанным токеном оказался toolken. в этом случае вызывается соответствующий инструмент в режиме “few-shot”. после того как вызов осуществляется внешним инструментом, модель возвращает ответ и переходит обратно в режим “reasoning”. авторы показали применимость подхода для математических операций на gsm8k-xl и funcqa. также рассмотрели задачи knowledge-based qa и генерации плана. toolken+: ранжирование инструментов и отказ от неподходящих функций концепция toolken+ решает две проблемы toolkengpt. во-первых, ранее модель не учитывала документацию по инструментам и часто выбирала неподходящий инструмент. во-вторых, модель иногда стремилась использовать инструмент там, где это не требовалось. toolken+ добавляет два улучшения: 1) переранжирование нескольких выбранных инструментов. модель сначала предлагает k вариантов, потом повторно оценивает и выбирает оптимальный. 2) опцию “reject” для отказа от вызова инструмента. модель может явно указать, что сейчас не стоит применять никакой инструмент, если вероятность подходящего вызова невысока. эти изменения позволяют минимизировать как ошибки ложноположительных срабатываний при вызове инструментов, так и ошибки неправильной классификации инструментов для toolkengpt, что позволяет улучшить робастность модели. результаты исследователи проверяли toolken+ на математическом бенчмарке gsm8k, на бенчмарках virtualhome и metatool. они показали, что добавление переранжирования и опции &quot;reject&quot; улучшает качество конечных ответов. при этом в metatool требуется только одна функция для заданного запроса, поэтому опция &quot;reject&quot; не нужна — таким образом, замер служит как аблейшн реранжирования гипотез. расскажите в комментариях, что думаете о подходах toolkengpt и toolken+. разбор подготовил ❣ андрей бут душный nlp toolkengpt и toolken+: расширение возможностей языковых моделей за счёт интеграции инструментов сегодня разбираем две статьи. первая описывает парадигму обучения инструментов toolkengpt. вторая представляет развитие этой концепции, предложенное константином яковлевым, сергеем николенко и андреем бутом из яндекса. toolkengpt: как научить модель напрямую вызывать внешние функции в первой работе исследователи предложили представить каждый внешний инструмент в виде токена — toolken (represents each tool as a to ken ) — и выучивать его эмбеддинг. модель обучается работать с такими токенами так же, как с обычными текстовыми. в результате работу модели можно условно разделить на две стадии: 1) режим “reasoning” — генерация происходит, как обычно, с той лишь разницей, что добавленные toolken тоже рассматриваются в качестве вероятных токенов на каждом шаге генерации; 2) режим “tool” — когда следующим предсказанным токеном оказался toolken. в этом случае вызывается соответствующий инструмент в режиме “few-shot”. после того как вызов осуществляется внешним инструментом, модель возвращает ответ и переходит обратно в режим “reasoning”. авторы показали применимость подхода для математических операций на gsm8k-xl и funcqa. также рассмотрели задачи knowledge-based qa и генерации плана. toolken+: ранжирование инструментов и отказ от неподходящих функций концепция toolken+ решает две проблемы toolkengpt. во-первых, ранее модель не учитывала документацию по инструментам и часто выбирала неподходящий инструмент. во-вторых, модель иногда стремилась использовать инструмент там, где это не требовалось. toolken+ добавляет два улучшения: 1) переранжирование нескольких выбранных инструментов. модель сначала предлагает k вариантов, потом повторно оценивает и выбирает оптимальный. 2) опцию “reject” для отказа от вызова инструмента. модель может явно указать, что сейчас не стоит применять никакой инструмент, если вероятность подходящего вызова невысока. эти изменения позволяют минимизировать как ошибки ложноположительных срабатываний при вызове инструментов, так и ошибки неправильной классификации инструментов для toolkengpt, что позволяет улучшить робастность модели. результаты исследователи проверяли toolken+ на математическом бенчмарке gsm8k, на бенчмарках virtualhome и metatool. они показали, что добавление переранжирования и опции &amp;quot;reject&amp;quot; улучшает качество конечных ответов. при этом в metatool требуется только одна функция для заданного запроса, поэтому опция &amp;quot;reject&amp;quot; не нужна — таким образом, замер служит как аблейшн реранжирования гипотез. расскажите в комментариях, что думаете о подходах toolkengpt и toolken+. разбор подготовил ❣ андрей бут душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-12-17T09:40:35+00:00" href="./posts/53.html">2024-12-17 09:40 UTC</a></div>
      </div>
      <div class="post-body"><strong>ToolkenGPT и Toolken+: расширение возможностей языковых моделей за счёт интеграции инструментов</strong><br><br>Сегодня разбираем две статьи. Первая описывает парадигму обучения инструментов ToolkenGPT. Вторая представляет развитие этой концепции, предложенное Константином Яковлевым, Сергеем Николенко и Андреем Бутом из Яндекса.<br><br><a href="https://arxiv.org/abs/2305.11554v4" rel="nofollow noopener noreferrer">ToolkenGPT: как научить модель напрямую вызывать внешние функции</a><br>В первой работе исследователи предложили представить каждый внешний инструмент в виде токена — toolken (represents each <strong>tool</strong> as a to<strong>ken</strong>) — и выучивать его эмбеддинг. Модель обучается работать с такими токенами так же, как с обычными текстовыми. <br><br>В результате работу модели можно условно разделить на две стадии:<br><br>1) режим “reasoning” — генерация происходит, как обычно, с той лишь разницей, что добавленные toolken тоже рассматриваются в качестве вероятных токенов на каждом шаге генерации;<br>2) режим “tool” — когда следующим предсказанным токеном оказался toolken. В этом случае вызывается соответствующий инструмент в режиме “few-shot”. После того как вызов осуществляется внешним инструментом, модель возвращает ответ и переходит обратно в режим “reasoning”.<br><br>Авторы показали применимость подхода для математических операций на GSM8K-XL и FuncQA. Также рассмотрели задачи knowledge-based QA и генерации плана.<br><br><a href="https://aclanthology.org/2024.findings-emnlp.345.pdf" rel="nofollow noopener noreferrer">Toolken+: ранжирование инструментов и отказ от неподходящих функций</a><br>Концепция Toolken+ решает две проблемы ToolkenGPT. Во-первых, ранее модель не учитывала документацию по инструментам и часто выбирала неподходящий инструмент. Во-вторых, модель иногда стремилась использовать инструмент там, где это не требовалось. <br><br>Toolken+ добавляет два улучшения:<br><br>1) Переранжирование нескольких выбранных инструментов. Модель сначала предлагает k вариантов, потом повторно оценивает и выбирает оптимальный.<br>2) Опцию “reject” для отказа от вызова инструмента. Модель может явно указать, что сейчас не стоит применять никакой инструмент, если вероятность подходящего вызова невысока.<br><br>Эти изменения позволяют минимизировать как ошибки ложноположительных срабатываний при вызове инструментов, так и ошибки неправильной классификации инструментов для ToolkenGPT, что позволяет улучшить робастность модели.<br><br><strong>Результаты</strong><br>Исследователи проверяли Toolken+ на математическом бенчмарке GSM8K, на бенчмарках VirtualHome и MetaTool. Они показали, что добавление переранжирования и опции &quot;reject&quot; улучшает качество конечных ответов. При этом в MetaTool требуется только одна функция для заданного запроса, поэтому опция &quot;reject&quot; не нужна — таким образом, замер служит как аблейшн реранжирования гипотез. <br><br>Расскажите в комментариях, что думаете о подходах ToolkenGPT и Toolken+.<br><em><br>Разбор подготовил </em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji> <em>Андрей Бут</em><br><a href="https://t.me/+qGlf1dMQe8AyNTEy" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/53_480.webp" srcset="../assets/media/thumbs/53_480.webp 480w, ../assets/media/53.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="53" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/54_480.webp" srcset="../assets/media/thumbs/54_480.webp 480w, ../assets/media/54.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="53" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/55_480.webp" srcset="../assets/media/thumbs/55_480.webp 480w, ../assets/media/55.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="53" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/56_480.webp" srcset="../assets/media/thumbs/56_480.webp 480w, ../assets/media/56.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="53" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/57_480.webp" srcset="../assets/media/thumbs/57_480.webp 480w, ../assets/media/57.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="53" data-image-index="4" /></div></div>
      <div class="actions">
        <span>3 418 просмотров · 31 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/53" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/53.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="52" data-search="prism — датасет с информацией о пользователе интересное с neurips по мнению анастасии беззубцевой, руководителя группы аналитики в яндексе: доклад о датасете the prism учитывает социодемографические характеристики вроде места проживания, этнической принадлежности, уровня дохода, вероисповедания и так далее. для сбора данных привлекли 1,5 тысячи человек из 75 стран. участники, которых искали с помощью сервиса prolific, провели более 8 тысяч диалогов с 21 языковой моделью. анализ данных показал, что разные группы людей ведут разные разговоры с llm. так, пожилые респонденты чаще молодых поднимали тему выборов, а чернокожие реже белых говорили об изменении климата. при ранжировании моделей индивидуальные характеристики пользователя оказываются очень важны. например, zephyr-7b хорошо показывает себя в обсуждении чувствительных тем, а mistral-7b больше других понравилась респондентам из африки. на постер-сессии анастасия пообщалась с докладчицей ханной роуз, которая получает phd в оксфорде и имеет экономический бэкграунд. я задала ей свой любимый вопрос о качестве данных, полученных краудом. роуз сказала, что проверка участников была минимальной. исполнителям настолько нравилось задание — просто поболтать с llm на заданную тему, — что на датасет потратили вдвое меньше денег, чем планировали, а результат авторов более чем устраивает. #yaneurips душный nlp prism — датасет с информацией о пользователе интересное с neurips по мнению анастасии беззубцевой, руководителя группы аналитики в яндексе: доклад о датасете the prism учитывает социодемографические характеристики вроде места проживания, этнической принадлежности, уровня дохода, вероисповедания и так далее. для сбора данных привлекли 1,5 тысячи человек из 75 стран. участники, которых искали с помощью сервиса prolific, провели более 8 тысяч диалогов с 21 языковой моделью. анализ данных показал, что разные группы людей ведут разные разговоры с llm. так, пожилые респонденты чаще молодых поднимали тему выборов, а чернокожие реже белых говорили об изменении климата. при ранжировании моделей индивидуальные характеристики пользователя оказываются очень важны. например, zephyr-7b хорошо показывает себя в обсуждении чувствительных тем, а mistral-7b больше других понравилась респондентам из африки. на постер-сессии анастасия пообщалась с докладчицей ханной роуз, которая получает phd в оксфорде и имеет экономический бэкграунд. я задала ей свой любимый вопрос о качестве данных, полученных краудом. роуз сказала, что проверка участников была минимальной. исполнителям настолько нравилось задание — просто поболтать с llm на заданную тему, — что на датасет потратили вдвое меньше денег, чем планировали, а результат авторов более чем устраивает. #yaneurips душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-12-14T08:44:02+00:00" href="./posts/52.html">2024-12-14 08:44 UTC</a></div>
      </div>
      <div class="post-body"><strong>PRISM — датасет с информацией о пользователе </strong><br><br>Интересное с NeurIPS по мнению Анастасии Беззубцевой, руководителя группы аналитики в Яндексе: доклад о <a href="https://arxiv.org/abs/2404.16019" rel="nofollow noopener noreferrer">датасете The PRISM</a> учитывает социодемографические характеристики вроде места проживания, этнической принадлежности, уровня дохода, вероисповедания и так далее. Для сбора данных привлекли 1,5 тысячи человек из 75 стран. Участники, которых искали с помощью сервиса Prolific, провели более 8 тысяч диалогов с 21 языковой моделью. <br><br>Анализ данных показал, что разные группы людей ведут разные разговоры с LLM. Так, пожилые респонденты чаще молодых поднимали тему выборов, а чернокожие реже белых говорили об изменении климата. При ранжировании моделей индивидуальные характеристики пользователя оказываются очень важны. Например, Zephyr-7b хорошо показывает себя в обсуждении чувствительных тем, а mistral-7b больше других понравилась респондентам из Африки. <br><br>На постер-сессии Анастасия пообщалась с докладчицей Ханной Роуз, которая получает PhD в Оксфорде и имеет экономический бэкграунд. <br><br><blockquote>Я задала ей свой любимый вопрос о качестве данных, полученных краудом. Роуз сказала, что проверка участников была минимальной. Исполнителям настолько нравилось задание — просто поболтать с LLM на заданную тему, — что на датасет потратили вдвое меньше денег, чем планировали, а результат авторов более чем устраивает.</blockquote><br><br>#YaNeurIPS<br><br><a href="https://t.me/+0s9rlomAkKI0YzBi" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/52_480.webp" srcset="../assets/media/thumbs/52_480.webp 480w, ../assets/media/52.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="52" data-image-index="0" /></div></div>
      <div class="actions">
        <span>4 312 просмотров · 28 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/52" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/52.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="51" data-search="впечатления от туториала об оценке моделей на neurips на повестке — туториал evaluating large language models — principles, approaches, and applications. он был посвящён оценке больших языковых моделей (llms). руководитель группы аналитики в яндексе анастасия беззубцева посетила туториал и рассказала, что интересного отметила для себя. первую часть представляла продакт-менеджер google ирина сиглер. она ввела общие понятия и объяснила базовые моменты. например, о важности валидации на датасете, который репрезентативен реальной бизнес-задаче. есть три способа собрать валидационное множество заданий для оценки модели: — manual — написание промптов вручную; — synthetic — генерация промптов с помощью llm; — traffic — использовать продуктовый поток. по словам сиглер, важно оценивать систему полностью, а не только llm под капотом. сама модель — это всего лишь один кирпичик в общей структуре. со второй частью выступала исследовательница и член консультативного совета центра инноваций в области искусственного интеллекта (caii) в университете иллинойса бо ли. она выделила три метода оценки: — computation — расчёт схожести между данным ответом и референсным; — human — оценка человеком; — llm-as-judge или autorater — оценка с помощью модели. ли бо рассказала, что автоматические методы оценки не слишком хорошо коррелируют с человеческими суждениями. модели могут ошибаться, отдавая предпочтение, например, собственным или самым длинным ответам. однако использование llm для оценки становится всё более частым явлением — главное, чтобы полученные результаты валидировали люди. чтобы нивелировать недостатки способа, на туториале предлагали переставлять опции и искать консенсус между несколькими ответами одной модели или ответами разных. этим туториал не ограничился — были еще практическая часть и часть, посвященная соответствию этическим нормам. с презентацией туториала вы можете ознакомиться по этой ссылке. #yaneurips душный nlp впечатления от туториала об оценке моделей на neurips на повестке — туториал evaluating large language models — principles, approaches, and applications. он был посвящён оценке больших языковых моделей (llms). руководитель группы аналитики в яндексе анастасия беззубцева посетила туториал и рассказала, что интересного отметила для себя. первую часть представляла продакт-менеджер google ирина сиглер. она ввела общие понятия и объяснила базовые моменты. например, о важности валидации на датасете, который репрезентативен реальной бизнес-задаче. есть три способа собрать валидационное множество заданий для оценки модели: — manual — написание промптов вручную; — synthetic — генерация промптов с помощью llm; — traffic — использовать продуктовый поток. по словам сиглер, важно оценивать систему полностью, а не только llm под капотом. сама модель — это всего лишь один кирпичик в общей структуре. со второй частью выступала исследовательница и член консультативного совета центра инноваций в области искусственного интеллекта (caii) в университете иллинойса бо ли. она выделила три метода оценки: — computation — расчёт схожести между данным ответом и референсным; — human — оценка человеком; — llm-as-judge или autorater — оценка с помощью модели. ли бо рассказала, что автоматические методы оценки не слишком хорошо коррелируют с человеческими суждениями. модели могут ошибаться, отдавая предпочтение, например, собственным или самым длинным ответам. однако использование llm для оценки становится всё более частым явлением — главное, чтобы полученные результаты валидировали люди. чтобы нивелировать недостатки способа, на туториале предлагали переставлять опции и искать консенсус между несколькими ответами одной модели или ответами разных. этим туториал не ограничился — были еще практическая часть и часть, посвященная соответствию этическим нормам. с презентацией туториала вы можете ознакомиться по этой ссылке. #yaneurips душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-12-13T08:57:19+00:00" href="./posts/51.html">2024-12-13 08:57 UTC</a></div>
      </div>
      <div class="post-body"><strong>Впечатления от туториала об оценке моделей на NeurIPS</strong><br><br>На повестке — туториал <a href="https://neurips.cc/virtual/2024/tutorial/99524" rel="nofollow noopener noreferrer">Evaluating Large Language Models — Principles, Approaches, and Applications.</a> Он был посвящён оценке больших языковых моделей (LLMs). Руководитель группы аналитики в Яндексе Анастасия Беззубцева посетила туториал и рассказала, что интересного отметила для себя. <br><br>Первую часть представляла продакт-менеджер Google Ирина Сиглер. Она ввела общие понятия и объяснила базовые моменты. Например, о важности валидации на датасете, который репрезентативен реальной бизнес-задаче. Есть три способа собрать валидационное множество заданий для оценки модели:<br><br>— Manual — написание промптов вручную;<br>— Synthetic — генерация промптов с помощью LLM;<br>— Traffic — использовать продуктовый поток.<br><br>По словам Сиглер, важно оценивать систему полностью, а не только LLM под капотом. Сама модель — это всего лишь один кирпичик в общей структуре.<br><br>Со второй частью выступала исследовательница и член консультативного совета Центра инноваций в области искусственного интеллекта (CAII) в университете Иллинойса Бо Ли. Она выделила три метода оценки:<br><br>— Computation — расчёт схожести между данным ответом и референсным;<br>— Human — оценка человеком;<br>— LLM-as-Judge или AutoRater — оценка с помощью модели.<br><br>Ли Бо рассказала, что автоматические методы оценки не слишком хорошо коррелируют с <a href="https://arxiv.org/pdf/2303.16634" rel="nofollow noopener noreferrer">человеческими суждениями.</a> Модели могут ошибаться, отдавая предпочтение, например, собственным или самым длинным ответам. Однако использование LLM для оценки становится всё более частым явлением — главное, чтобы полученные результаты валидировали люди. Чтобы нивелировать недостатки способа, на туториале предлагали переставлять опции и искать консенсус между несколькими ответами одной модели или ответами разных. <br><br>Этим туториал не ограничился — были еще практическая часть и часть, посвященная соответствию этическим нормам. С презентацией туториала вы можете ознакомиться <a href="https://neurips.cc/media/neurips-2024/Slides/99524.pdf" rel="nofollow noopener noreferrer">по этой ссылке. </a><br><br>#YaNeurIPS<br><br><a href="https://t.me/+qGlf1dMQe8AyNTEy" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/51_480.webp" srcset="../assets/media/thumbs/51_480.webp 480w, ../assets/media/51.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="51" data-image-index="0" /></div></div>
      <div class="actions">
        <span>4 187 просмотров · 31 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/51" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/51.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="49" data-search="астрологи объявили неделю neurips 2024 в ванкувере стартовала одна из крупнейших конференций по машинному обучению. а это значит, что в ближайшие дни мы будем делиться с вами самыми интересными статьями и яркими впечатлениями прямо с полей. в этом году на конференции: ⚪ 4497 статей, из которых 460 — по датасетам и бенчмаркам; ⚪ 13640 ревьюеров; ⚪ 56 воркшопов; ⚪ 14 туториалов. в числе прочего, представят восемь статей исследовательского подразделения яндекса, yandex research. мы рассказывали о каждой из них в канале ml underhood — подписывайтесь! с роботами знакомилась ❣ анастасия беззубцева #yaneurips душный nlp астрологи объявили неделю neurips 2024 в ванкувере стартовала одна из крупнейших конференций по машинному обучению. а это значит, что в ближайшие дни мы будем делиться с вами самыми интересными статьями и яркими впечатлениями прямо с полей. в этом году на конференции: ⚪ 4497 статей, из которых 460 — по датасетам и бенчмаркам; ⚪ 13640 ревьюеров; ⚪ 56 воркшопов; ⚪ 14 туториалов. в числе прочего, представят восемь статей исследовательского подразделения яндекса, yandex research. мы рассказывали о каждой из них в канале ml underhood — подписывайтесь! с роботами знакомилась ❣ анастасия беззубцева #yaneurips душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-12-11T13:01:56+00:00" href="./posts/49.html">2024-12-11 13:01 UTC</a></div>
      </div>
      <div class="post-body"><strong>Астрологи объявили неделю NeurIPS 2024 <br></strong><br>В Ванкувере стартовала одна из крупнейших конференций по машинному обучению. А это значит, что в ближайшие дни мы будем делиться с вами самыми интересными статьями и яркими впечатлениями прямо с полей. <br><br>В этом году на конференции:<br><br><tg-emoji emoji-id="5341470192595653209">⚪</tg-emoji> 4497 статей, из которых 460 — по датасетам и бенчмаркам;<br><tg-emoji emoji-id="5341470192595653209">⚪</tg-emoji> 13640 ревьюеров;<br><tg-emoji emoji-id="5341470192595653209">⚪</tg-emoji> 56 воркшопов;<br><tg-emoji emoji-id="5341470192595653209">⚪</tg-emoji> 14 туториалов.<br><br>В числе прочего, представят восемь статей исследовательского подразделения Яндекса, Yandex Research. Мы рассказывали о каждой из них в канале <a href="https://t.me/MLunderhood/27" rel="nofollow noopener noreferrer">ML Underhood — подписывайтесь!  </a><br><br><em>С роботами знакомилась </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Анастасия Беззубцева</em><br><br>#YaNeurIPS<br><br><a href="https://t.me/+WY0XMQWY9sBmYzFi" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><video controls preload="metadata" src="../assets/media/49_IMG_0737.MOV.mov"></video><video controls preload="metadata" src="../assets/media/50_video_2024-12-11_14-25-32.mp4"></video></div></div>
      <div class="actions">
        <span>3 804 просмотров · 37 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/49" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/49.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="46" data-search="как избавиться от value-функции в ppo сегодня — о двух методах стабилизации ppo. один вытекает из другого и каждому посвящена отдельная статья. о ppo подробнее мы уже рассказывали в другом нашем канале — ml underhood. здесь же сосредоточимся на частностях. традиционно в ppo считается некоторый advantage. он вычисляется для пары префикса и ответа и показывает, на сколько конкретный ответ лучше среднего. чтобы определить advantage нужно из суммарной награды префикса и ответа (q в первой формуле выше) вычесть среднюю награду (v), которую генератор набрал бы, если бы стартовал с этого префикса. value-функцию принято обучать отдельной моделью на прогнозирование средних наград. однако с v-моделью есть некоторые сложности. во-первых, она большая и сопоставима по размерам с генератором. во-вторых, её нужно инферить, на что требуются вычислительные ресурсы. а в-третьих, она обычно выдает не очень хорошие результаты. поэтому было бы круто придумать способ избавиться от v-модели в ppo, ведь она нужна только для снижения дисперсии оценки лосса. авторы обеих статей поставили перед собой именно эту задачу. авторы статьи deepseekmath предлагают метод, который называется group relative policy optimization (grpo). в его рамках две модификации: 1. не обучать v-модель. вместо этого оценить значение средней награды методом монте-карло. ничего сложного: вместо генерации одного ответа на запрос сгенерировать несколько ответов, а среднюю награду, полученную за эти ответы на запрос, использовать как v. при подсчете advantage из награды каждого ответа вычитается эта средняя награда. таким образом, от v-модели избавляются с помощью увеличения количества генераций (схема на втором изображении). 2. в ppo используется kl-штраф за отклонение от sft-модели. обычно этот штраф вычитают из награды, чтобы ppo одновременно наращивал награду и не отходил далеко от sft. авторы предлагают добавлять штраф прямо к лоссу — это лишает нас каких-то интересных теоретических свойств алгоритма, но делает процедуру оптимизации намного легче (третье изображение с формулой). авторы второй статьи — vineppo — опираются на deepseekmath и развивают grpo в контексте математических задач. в grpo, в отличие от классического ppo, v-функция для всех токенов ответа получается одинаковой. так устроен алгоритм, ведь туда записана просто средняя награда за несколько ответов. для ответов, в которых есть цепочки рассуждений, это может быть не очень репрезентативно: при решении математических задач, удачный ход в рассуждении должен значимо повышать ожидаемую награду за ответ, тогда как ошибка в рассуждениях — наоборот, понижать. авторы предлагают разбивать ответ на смысловые блоки — по переносам строки. точкам или запятым, которые находятся вне формул — и для каждого из них оценивать v-функцию так же, как это делается в grpo. то есть генерировать по несколько продолжений из частично готового ответа. хоть идея и проста, эффективно её реализовать довольно трудно. а ещё этот метод требует существенно большего числа генераций во время обучения. авторы честно признаются, что их метод медленнее обычного ppo, но показывает неплохие результаты. разбор подготовил ❣ павел темирчев душный nlp как избавиться от value-функции в ppo сегодня — о двух методах стабилизации ppo. один вытекает из другого и каждому посвящена отдельная статья. о ppo подробнее мы уже рассказывали в другом нашем канале — ml underhood . здесь же сосредоточимся на частностях. традиционно в ppo считается некоторый advantage. он вычисляется для пары префикса и ответа и показывает, на сколько конкретный ответ лучше среднего. чтобы определить advantage нужно из суммарной награды префикса и ответа ( q в первой формуле выше ) вычесть среднюю награду (v), которую генератор набрал бы, если бы стартовал с этого префикса. value-функцию принято обучать отдельной моделью на прогнозирование средних наград. однако с v-моделью есть некоторые сложности. во-первых, она большая и сопоставима по размерам с генератором. во-вторых, её нужно инферить, на что требуются вычислительные ресурсы. а в-третьих, она обычно выдает не очень хорошие результаты. поэтому было бы круто придумать способ избавиться от v-модели в ppo, ведь она нужна только для снижения дисперсии оценки лосса. авторы обеих статей поставили перед собой именно эту задачу. авторы статьи deepseekmath предлагают метод, который называется group relative policy optimization (grpo). в его рамках две модификации: 1. не обучать v-модель. вместо этого оценить значение средней награды методом монте-карло. ничего сложного: вместо генерации одного ответа на запрос сгенерировать несколько ответов, а среднюю награду, полученную за эти ответы на запрос, использовать как v. при подсчете advantage из награды каждого ответа вычитается эта средняя награда. таким образом, от v-модели избавляются с помощью увеличения количества генераций ( схема на втором изображении ). 2. в ppo используется kl-штраф за отклонение от sft-модели. обычно этот штраф вычитают из награды, чтобы ppo одновременно наращивал награду и не отходил далеко от sft. авторы предлагают добавлять штраф прямо к лоссу — это лишает нас каких-то интересных теоретических свойств алгоритма, но делает процедуру оптимизации намного легче ( третье изображение с формулой ). авторы второй статьи — vineppo — опираются на deepseekmath и развивают grpo в контексте математических задач. в grpo, в отличие от классического ppo, v-функция для всех токенов ответа получается одинаковой. так устроен алгоритм, ведь туда записана просто средняя награда за несколько ответов. для ответов, в которых есть цепочки рассуждений, это может быть не очень репрезентативно: при решении математических задач, удачный ход в рассуждении должен значимо повышать ожидаемую награду за ответ, тогда как ошибка в рассуждениях — наоборот, понижать. авторы предлагают разбивать ответ на смысловые блоки — по переносам строки. точкам или запятым, которые находятся вне формул — и для каждого из них оценивать v-функцию так же, как это делается в grpo. то есть генерировать по несколько продолжений из частично готового ответа. хоть идея и проста, эффективно её реализовать довольно трудно. а ещё этот метод требует существенно большего числа генераций во время обучения. авторы честно признаются, что их метод медленнее обычного ppo, но показывает неплохие результаты. разбор подготовил ❣ павел темирчев душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-12-10T08:02:01+00:00" href="./posts/46.html">2024-12-10 08:02 UTC</a></div>
      </div>
      <div class="post-body"><strong>Как избавиться от Value-функции в PPO</strong><br><br>Сегодня — о двух методах стабилизации PPO. Один вытекает из другого и каждому посвящена отдельная статья. <br><br>О PPO подробнее мы уже рассказывали в другом нашем канале — <a href="https://t.me/MLunderhood/29" rel="nofollow noopener noreferrer">ML Underhood</a>. Здесь же сосредоточимся на частностях. Традиционно в PPO считается некоторый advantage. Он вычисляется для пары префикса и ответа и показывает, на сколько конкретный ответ лучше среднего. Чтобы определить advantage нужно из суммарной награды префикса и ответа (<em>Q в первой формуле выше</em>) вычесть среднюю награду (V), которую генератор набрал бы, если бы стартовал с этого префикса. <br><br>Value-функцию принято обучать отдельной моделью на прогнозирование средних наград. Однако с V-моделью есть некоторые сложности. Во-первых, она большая и сопоставима по размерам с генератором. Во-вторых, её нужно инферить, на что требуются вычислительные ресурсы. А в-третьих, она обычно выдает не очень хорошие результаты. Поэтому было бы круто придумать способ избавиться от V-модели в PPO, ведь она нужна только для снижения дисперсии оценки лосса. Авторы обеих статей поставили перед собой именно эту задачу.<br><br>Авторы статьи <a href="https://arxiv.org/abs/2402.03300" rel="nofollow noopener noreferrer">DeepSeekMath</a> предлагают метод, который называется Group Relative Policy Optimization (GRPO). В его рамках две модификации:<br><br>1. Не обучать V-модель. Вместо этого оценить значение средней награды методом Монте-Карло. Ничего сложного: вместо генерации одного ответа на запрос сгенерировать несколько ответов, а среднюю награду, полученную за эти ответы на запрос, использовать как V. При подсчете advantage из награды каждого ответа вычитается эта средняя награда. Таким образом, от V-модели избавляются с помощью увеличения количества генераций (<em>схема на втором изображении</em>).<br><br>2. В PPO используется KL-штраф за отклонение от SFT-модели. Обычно этот штраф вычитают из награды, чтобы PPO одновременно наращивал награду и не отходил далеко от SFT. Авторы предлагают добавлять штраф прямо к лоссу — это лишает нас каких-то интересных теоретических свойств алгоритма, но делает процедуру оптимизации намного легче (<em>третье изображение с формулой</em>).<br><br>Авторы второй статьи — <a href="https://arxiv.org/abs/2410.01679" rel="nofollow noopener noreferrer">VinePPO</a> — опираются на DeepSeekMath и развивают GRPO в контексте математических задач. В GRPO, в отличие от классического PPO, V-функция для всех токенов ответа получается одинаковой. Так устроен алгоритм, ведь туда записана просто средняя награда за несколько ответов. <br><br>Для ответов, в которых есть цепочки рассуждений, это может быть не очень репрезентативно: при решении математических задач, удачный ход в рассуждении должен значимо повышать ожидаемую награду за ответ, тогда как ошибка в рассуждениях — наоборот, понижать.<br><br>Авторы предлагают разбивать ответ на смысловые блоки — по переносам строки. точкам или запятым, которые находятся вне формул — и для каждого из них оценивать V-функцию так же, как это делается в GRPO. То есть генерировать по несколько продолжений из частично готового ответа.<br><br>Хоть идея и проста, эффективно её реализовать довольно трудно. А ещё этот метод требует существенно большего числа генераций во время обучения. Авторы честно признаются, что их метод медленнее обычного PPO, но показывает неплохие результаты.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Павел Темирчев</em><br><br><a href="https://t.me/+1sp0RiSOT-oyNDIy" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/46_480.webp" srcset="../assets/media/thumbs/46_480.webp 480w, ../assets/media/46.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="46" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/47_480.webp" srcset="../assets/media/thumbs/47_480.webp 480w, ../assets/media/47.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="46" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/48_480.webp" srcset="../assets/media/thumbs/48_480.webp 480w, ../assets/media/48.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="46" data-image-index="2" /></div></div>
      <div class="actions">
        <span>4 535 просмотров · 40 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/46" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/46.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="45" data-search="redrafter — быстрый метод спекулятивного декодирования сегодняшняя статья посвящена recurrent drafter (redrafter) — новому подходу к спекулятивному декодированию. авторы заявляют, что он позволяет значительно увеличить скорость моделей. само спекулятивное декодирование основано на использовании дополнительной «черновой» модели, которая предлагает варианты продолжения цепочек токенов. основная модель проверяет эти варианты, выбирая один с помощью специальной процедуры верификации. качество генераций остаётся таким же, как и без использования спекулятивного декодирования, потому что окончательное решение о принятии тех или иных токенов лежит на основной модели. в redrafter в качестве черновой модели используется rnn. благодаря рекуррентной структуре, учитывается зависимость между черновыми токенами. кроме того, rnn, помимо последнего токена, также видит и скрытое состояние из последнего слоя llm, что даёт ей возможность лучше следовать генерациям основной модели. с помощью алгоритма динамического древа внимания (dynamic tree attention algorithm), в сгенерированных rnn кандидатах убираются повторяющиеся префиксы. таким образом, в основную модель попадает меньше вариантов, а значит затраты вычислительных ресурсов становятся меньше. далее основная модель оценивает предложенные варианты (при этом, сразу несколько), выбирает лучший и процесс повторяется снова. rnn обучается с помощью дистилляции из основной модели. это позволяет rnn предсказывать токены с вероятностями, максимально приближенными к ожиданиям llm. таким образом, «черновая» модель реже предлагает токены, которые будут отклонены. авторы отмечают, что использование дистилляции дает лучшие результаты, чем обучение «черновой» модели на исходном датасете, на котором тренировали llm. так, скорость генерации и число принятых токенов за один шаг на vicuna 7b выросли примерно на 10%. redrafter показал лучшие результаты по сравнению с методами medusa и eagle на бенчмарках mt-bench и alpacaeval. при этом, по сравнению с жадной генерацией, генерация семплированием показывает ещё большее ускорение, чего обычно не показывают другие методы спекулятивного декодирования. разбор подготовил ❣ алексей гликин душный nlp redrafter — быстрый метод спекулятивного декодирования сегодняшняя статья посвящена recurrent drafter (redrafter) — новому подходу к спекулятивному декодированию. авторы заявляют, что он позволяет значительно увеличить скорость моделей. само спекулятивное декодирование основано на использовании дополнительной «черновой» модели, которая предлагает варианты продолжения цепочек токенов. основная модель проверяет эти варианты, выбирая один с помощью специальной процедуры верификации. качество генераций остаётся таким же, как и без использования спекулятивного декодирования, потому что окончательное решение о принятии тех или иных токенов лежит на основной модели. в redrafter в качестве черновой модели используется rnn. благодаря рекуррентной структуре, учитывается зависимость между черновыми токенами. кроме того, rnn, помимо последнего токена, также видит и скрытое состояние из последнего слоя llm, что даёт ей возможность лучше следовать генерациям основной модели. с помощью алгоритма динамического древа внимания (dynamic tree attention algorithm), в сгенерированных rnn кандидатах убираются повторяющиеся префиксы. таким образом, в основную модель попадает меньше вариантов, а значит затраты вычислительных ресурсов становятся меньше. далее основная модель оценивает предложенные варианты (при этом, сразу несколько), выбирает лучший и процесс повторяется снова. rnn обучается с помощью дистилляции из основной модели. это позволяет rnn предсказывать токены с вероятностями, максимально приближенными к ожиданиям llm. таким образом, «черновая» модель реже предлагает токены, которые будут отклонены. авторы отмечают, что использование дистилляции дает лучшие результаты, чем обучение «черновой» модели на исходном датасете, на котором тренировали llm. так, скорость генерации и число принятых токенов за один шаг на vicuna 7b выросли примерно на 10%. redrafter показал лучшие результаты по сравнению с методами medusa и eagle на бенчмарках mt-bench и alpacaeval. при этом, по сравнению с жадной генерацией, генерация семплированием показывает ещё большее ускорение, чего обычно не показывают другие методы спекулятивного декодирования. разбор подготовил ❣ алексей гликин душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-11-29T14:46:48+00:00" href="./posts/45.html">2024-11-29 14:46 UTC</a></div>
      </div>
      <div class="post-body"><strong>ReDrafter — быстрый метод спекулятивного декодирования</strong><br><br>Сегодняшняя статья посвящена <a href="https://arxiv.org/abs/2403.09919" rel="nofollow noopener noreferrer">Recurrent Drafter (ReDrafter)</a> — новому подходу к спекулятивному декодированию. Авторы заявляют, что он позволяет значительно увеличить скорость моделей. <br><br>Само спекулятивное декодирование основано на использовании дополнительной «черновой» модели, которая предлагает варианты продолжения цепочек токенов. Основная модель проверяет эти варианты, выбирая один с помощью специальной процедуры верификации. Качество генераций остаётся таким же, как и без использования спекулятивного декодирования, потому что окончательное решение о принятии тех или иных токенов лежит на основной модели. <br><br>В ReDrafter в качестве черновой модели используется RNN. Благодаря рекуррентной структуре, учитывается зависимость между черновыми токенами. Кроме того, RNN, помимо последнего токена, также видит и скрытое состояние из последнего слоя LLM, что даёт ей возможность лучше следовать генерациям основной модели. <br><br>С помощью алгоритма динамического древа внимания (dynamic tree attention algorithm), в сгенерированных RNN кандидатах убираются повторяющиеся префиксы. Таким образом, в основную модель попадает меньше вариантов, а значит затраты вычислительных ресурсов становятся меньше. Далее основная модель оценивает предложенные варианты (при этом, сразу несколько), выбирает лучший и процесс повторяется снова.<br><br>RNN обучается с помощью дистилляции из основной модели. Это позволяет RNN предсказывать токены с вероятностями, максимально приближенными к ожиданиям LLM. Таким образом, «черновая» модель реже предлагает токены, которые будут отклонены.<br><br>Авторы отмечают, что использование дистилляции дает лучшие результаты, чем обучение «черновой» модели на исходном датасете, на котором тренировали LLM. Так, скорость генерации и число принятых токенов за один шаг на Vicuna 7B выросли примерно на 10%.<br><br>ReDrafter показал лучшие результаты по сравнению с методами Medusa и EAGLE на бенчмарках MT-Bench и AlpacaEval. При этом, по сравнению с жадной генерацией, генерация семплированием показывает ещё большее ускорение, чего обычно не показывают другие методы спекулятивного декодирования. <br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Алексей Гликин</em><br><br><a href="https://t.me/+Wwn5cguqXDk3ZjIy" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/45_480.webp" srcset="../assets/media/thumbs/45_480.webp 480w, ../assets/media/45.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="45" data-image-index="0" /></div></div>
      <div class="actions">
        <span>4 446 просмотров · 45 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/45" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/45.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="44" data-search="warm — метод улучшения reward-моделей сегодняшняя статья — о методе усреднения весов reward-модели для устранения проблем, связанных с rl-обучением. но для начала напомним, как работает reward-модель. на вход она принимает промпты и ответы, а на выход выдаёт скаляры. по ним возможно ранжировать ответы от лучшего к худшему. всё это делается с помощью обучения на минимизацию лосса, который вытекает из модели брэдли-терри. как правило, reward-модели обучаются на датасете из преференсных данных — то есть таких, в которых ответы уже размечены асессорами или другой моделью. есть ряд проблем, с которыми можно столкнуться во время обучения reward-модели. во-первых, разметка может оказаться достаточно шумной — например, при расхождениях в оценках одного и того же ответа разными асессорами. кроме того, в некоторых случаях политика может генерировать ood-ответы для выбранной reward-модели. наконец, возможно и такое, что reward-модель выучится на какой-то черте данных — например, особенностях оформления. при этом на файнтюнинге модель научится генерировать те ответы, которые будут давать высокий скор именно из-за этой особенности, а не из-за качества самих ответов. скажем, будет отдавать приоритет хорошо оформленным, а не правильным ответам. существует несколько методов, призванных справится с вышеописанными проблемами. например, можно обучить много абсолютно разных reward-моделей и усреднить их логиты. этот метод называется prediction ensembling (ens), а его главный недостаток заключается в необходимости инферить сразу несколько моделей, что не очень экономично в условиях файнтюнинга. авторы статьи, в свою очередь, предлагают обучать reward-модель с помощью одного датасета с преференсными данными, но с разными гиперпараметрами, а также с разных чекпоинтов sft-обучения. в результате получается несколько моделей с одинаковой архитектурой. их веса следует усреднить в одну модель — weight average reward-model (warm), которая поступает как reward-функция в rl. проведенный авторами анализ показал, что warm — это аппроксимация ens. почему это должно работать? известно, что существует линейная связь в моделях, обученных из одного претрейна. она позволяет усреднять веса, не теряя при этом в качестве. однако это справедливо только для одного претрейна. проверки c использованием датасета tl;dr summarization показали, что warm запоминает меньше испорченных или некорректных данных разметки в датасете, чем ens. то же самое касается работы с ood-примернами. однако на «чистом» фрагменте датасета, где разметка без ошибок, ens выдаёт лучшие результаты. авторы заявляют, что преимущество их метода заключается в использовании всего одной модели в ходе файнтюнинга — это позволяет экономить время и вычислительные ресурсы. кроме того, warm решает некоторые проблемы, связанные с «грязными» данными. однако есть и ограничения. например, необходимость обучаться из одного претрейна и невозможность использовать разные архитектуры. разбор подготовил ❣ илья черемушкин душный nlp warm — метод улучшения reward-моделей сегодняшняя статья — о методе усреднения весов reward-модели для устранения проблем, связанных с rl-обучением. но для начала напомним, как работает reward-модель. на вход она принимает промпты и ответы, а на выход выдаёт скаляры. по ним возможно ранжировать ответы от лучшего к худшему. всё это делается с помощью обучения на минимизацию лосса, который вытекает из модели брэдли-терри. как правило, reward-модели обучаются на датасете из преференсных данных — то есть таких, в которых ответы уже размечены асессорами или другой моделью. есть ряд проблем, с которыми можно столкнуться во время обучения reward-модели. во-первых, разметка может оказаться достаточно шумной — например, при расхождениях в оценках одного и того же ответа разными асессорами. кроме того, в некоторых случаях политика может генерировать ood-ответы для выбранной reward-модели. наконец, возможно и такое, что reward-модель выучится на какой-то черте данных — например, особенностях оформления. при этом на файнтюнинге модель научится генерировать те ответы, которые будут давать высокий скор именно из-за этой особенности, а не из-за качества самих ответов. скажем, будет отдавать приоритет хорошо оформленным, а не правильным ответам. существует несколько методов, призванных справится с вышеописанными проблемами. например, можно обучить много абсолютно разных reward-моделей и усреднить их логиты. этот метод называется prediction ensembling (ens), а его главный недостаток заключается в необходимости инферить сразу несколько моделей, что не очень экономично в условиях файнтюнинга. авторы статьи, в свою очередь, предлагают обучать reward-модель с помощью одного датасета с преференсными данными, но с разными гиперпараметрами, а также с разных чекпоинтов sft-обучения. в результате получается несколько моделей с одинаковой архитектурой. их веса следует усреднить в одну модель — weight average reward-model (warm), которая поступает как reward-функция в rl. проведенный авторами анализ показал, что warm — это аппроксимация ens. почему это должно работать? известно, что существует линейная связь в моделях, обученных из одного претрейна. она позволяет усреднять веса, не теряя при этом в качестве. однако это справедливо только для одного претрейна. проверки c использованием датасета tl;dr summarization показали, что warm запоминает меньше испорченных или некорректных данных разметки в датасете, чем ens. то же самое касается работы с ood-примернами. однако на «чистом» фрагменте датасета, где разметка без ошибок, ens выдаёт лучшие результаты. авторы заявляют, что преимущество их метода заключается в использовании всего одной модели в ходе файнтюнинга — это позволяет экономить время и вычислительные ресурсы. кроме того, warm решает некоторые проблемы, связанные с «грязными» данными. однако есть и ограничения. например, необходимость обучаться из одного претрейна и невозможность использовать разные архитектуры. разбор подготовил ❣ илья черемушкин душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-11-20T10:36:19+00:00" href="./posts/44.html">2024-11-20 10:36 UTC</a></div>
      </div>
      <div class="post-body"><strong>WARM — метод улучшения reward-моделей</strong><br><br><a href="https://arxiv.org/abs/2401.12187" rel="nofollow noopener noreferrer">Сегодняшняя статья</a> — о методе усреднения весов reward-модели для устранения проблем, связанных с RL-обучением. Но для начала напомним, как работает Reward-модель. <br><br>На вход она принимает промпты и ответы, а на выход выдаёт скаляры. По ним возможно ранжировать ответы от лучшего к худшему. Всё это делается с помощью обучения на минимизацию лосса, который вытекает из модели Брэдли-Терри. Как правило, reward-модели обучаются на датасете из преференсных данных — то есть таких, в которых ответы уже размечены асессорами или другой моделью. <br><br>Есть ряд проблем, с которыми можно столкнуться во время обучения reward-модели. Во-первых, разметка может оказаться достаточно шумной — например, при расхождениях в оценках одного и того же ответа разными асессорами. Кроме того, в некоторых случаях политика может генерировать OOD-ответы для выбранной reward-модели. <br><br>Наконец, возможно и такое, что reward-модель выучится на какой-то черте данных — например, особенностях оформления. При этом на файнтюнинге модель научится генерировать те ответы, которые будут давать высокий скор именно из-за этой особенности, а не из-за качества самих ответов. Скажем, будет отдавать приоритет хорошо оформленным, а не правильным ответам. <br><br>Существует несколько методов, призванных справится с вышеописанными проблемами. Например, можно обучить много абсолютно разных reward-моделей и усреднить их логиты. Этот метод называется <a href="https://arxiv.org/abs/1612.01474" rel="nofollow noopener noreferrer">prediction ensembling</a> (ENS), а его главный недостаток заключается в необходимости инферить сразу несколько моделей, что не очень экономично в условиях файнтюнинга. <br><br>Авторы статьи, в свою очередь, предлагают обучать reward-модель с помощью одного датасета с преференсными данными, но с разными гиперпараметрами, а также с разных чекпоинтов SFT-обучения. В результате получается несколько моделей с одинаковой архитектурой. Их веса следует усреднить в одну модель — Weight Average Reward-Model (WARM), которая поступает как reward-функция в RL. Проведенный авторами анализ показал, что WARM — это аппроксимация ENS. <br><br>Почему это должно работать? Известно, что существует линейная связь в моделях, обученных из одного претрейна. Она позволяет усреднять веса, не теряя при этом в качестве. Однако это справедливо только для одного претрейна. <br><br>Проверки c использованием датасета TL;DR summarization показали, что WARM запоминает меньше испорченных или некорректных данных разметки в датасете, чем ENS. То же самое касается работы с OOD-примернами. Однако на «чистом» фрагменте датасета, где разметка без ошибок, ENS выдаёт лучшие результаты. <br><br>Авторы заявляют, что преимущество их метода заключается в использовании всего одной модели в ходе файнтюнинга — это позволяет экономить время и вычислительные ресурсы. Кроме того, WARM решает некоторые проблемы, связанные с «грязными» данными. Однако есть и ограничения. Например, необходимость обучаться из одного претрейна и невозможность использовать разные архитектуры. <br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Илья Черемушкин</em><br><br><a href="https://t.me/+EghBBJt9k_A1Njli" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/44_480.webp" srcset="../assets/media/thumbs/44_480.webp 480w, ../assets/media/44.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="44" data-image-index="0" /></div></div>
      <div class="actions">
        <span>4 826 просмотров · 41 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/44" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/44.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="43" data-search="tdpo — потокенный dpo или просто регуляризация? авторы сегодняшней статьи предлагают метод потокенного direct preference optimization (dpo), который на бумаге должен исправить некоторые проблемы оффлайн-обучения с подкреплением. но на деле все оказывается не так просто. dpo — метод обучения, не полагающийся на reward-модель. здесь применяют датасет с размеченными парами запросов и ответов, чтобы натренировать генератор на контрастный лосс. проблема в том, что в случае с dpo мы работаем с вероятностями последовательностей целиком. метод ограниченно контролирует поведение модели на уровне отдельных токенов. это приводит к тому, что модель может ошибочно сильно повышать или понижать вероятность отдельных токенов значительно после совершенных ошибок. эту проблему можно нивелировать, если сделать dpo потокенным. авторы статьи пытаются добиться этого. для начала они предлагают ввести необычное ограничение — сделать так, чтобы сумма наград всех токенов-продолжений для произвольного префикса была равна 0. это довольно сильное допущение: например, если мы решаем задачу копирования какого-то куска текста, то будем сильно штрафовать модель за любое отклонение. как результат — награда за правильный токен окажется очень большой. в этом случае, если при выборе между длинной и короткой строкой, модель будет склоняться к длинной строке. такое ограничение позволило авторам в их расчётах лосса избавиться от нормировочной константы вероятностного распределения. чтобы ее вычислить, нужно суммировать награду по всем возможным ответам, а это невозможно, поэтому от константы при расчётах избавляются. в dpo нормировочная константа одинакова для победившего и проигравшего ответов, поэтому она сокращается в лоссе, но авторы статьи сделали это несколько иначе. из их математической модели выводится функция, которая очень похожа на dpo. но в отличие от dpo, авторы вычитают из неё разницу между seqkl проигравшего и победившего ответа. этот метод, названный token-level direct preference optimization (tdpo), обеспечил незначительное улучшение по сравнению с обычным dpo. на датасете anthropic hh точность увеличилась всего на 0,65%. далее авторы предлагают умножить на дополнительный коэффициент разницу seqkl и не пропускать градиенты для победившего варианта. это можно трактовать так: при росте seqkl проигравшего ответа всегда увеличивается лосс, в то время, как при росте seqkl победившего — лосс уменьшается. получается, что добавка к dpo, после остановки градиента для её части, по сути работает, как регуляризация. с ней метод получил название tdpo2 и он действительно неплохо улучшает показатели. на том же anthropic hh прирост по сравнению с dpo составил уже не 0,65%, а 7,9%. авторы действительно предложили лучшее решение. но возникает вопрос: насколько здесь велик вклад выведенной математической модели. по факту, авторы сильно меняют основные моменты в этой модели, а то, что остается, очень похоже на простую потокенную регуляризацию. но её идея не нова: часто к dpo добавляют negative log likelihood loss — например, при dpo-обучении llama 3.1, — что тоже является вариантом потокенной регуляризации. мы склоняемся к тому, что научный вклад этой статьи невелик, а ключевые выводы — ошибочны. разбор подготовил ❣ михаил хрущев душный nlp tdpo — потокенный dpo или просто регуляризация? авторы сегодняшней статьи предлагают метод потокенного direct preference optimization (dpo), который на бумаге должен исправить некоторые проблемы оффлайн-обучения с подкреплением. но на деле все оказывается не так просто. dpo — метод обучения, не полагающийся на reward-модель. здесь применяют датасет с размеченными парами запросов и ответов, чтобы натренировать генератор на контрастный лосс. проблема в том, что в случае с dpo мы работаем с вероятностями последовательностей целиком. метод ограниченно контролирует поведение модели на уровне отдельных токенов. это приводит к тому, что модель может ошибочно сильно повышать или понижать вероятность отдельных токенов значительно после совершенных ошибок. эту проблему можно нивелировать, если сделать dpo потокенным. авторы статьи пытаются добиться этого. для начала они предлагают ввести необычное ограничение — сделать так, чтобы сумма наград всех токенов-продолжений для произвольного префикса была равна 0. это довольно сильное допущение: например, если мы решаем задачу копирования какого-то куска текста, то будем сильно штрафовать модель за любое отклонение. как результат — награда за правильный токен окажется очень большой. в этом случае, если при выборе между длинной и короткой строкой, модель будет склоняться к длинной строке. такое ограничение позволило авторам в их расчётах лосса избавиться от нормировочной константы вероятностного распределения. чтобы ее вычислить, нужно суммировать награду по всем возможным ответам, а это невозможно, поэтому от константы при расчётах избавляются. в dpo нормировочная константа одинакова для победившего и проигравшего ответов, поэтому она сокращается в лоссе, но авторы статьи сделали это несколько иначе. из их математической модели выводится функция, которая очень похожа на dpo. но в отличие от dpo, авторы вычитают из неё разницу между seqkl проигравшего и победившего ответа. этот метод, названный token-level direct preference optimization (tdpo), обеспечил незначительное улучшение по сравнению с обычным dpo. на датасете anthropic hh точность увеличилась всего на 0,65%. далее авторы предлагают умножить на дополнительный коэффициент разницу seqkl и не пропускать градиенты для победившего варианта. это можно трактовать так: при росте seqkl проигравшего ответа всегда увеличивается лосс, в то время, как при росте seqkl победившего — лосс уменьшается. получается, что добавка к dpo, после остановки градиента для её части, по сути работает, как регуляризация. с ней метод получил название tdpo2 и он действительно неплохо улучшает показатели. на том же anthropic hh прирост по сравнению с dpo составил уже не 0,65%, а 7,9%. авторы действительно предложили лучшее решение. но возникает вопрос: насколько здесь велик вклад выведенной математической модели. по факту, авторы сильно меняют основные моменты в этой модели, а то, что остается, очень похоже на простую потокенную регуляризацию. но её идея не нова: часто к dpo добавляют negative log likelihood loss — например, при dpo-обучении llama 3.1 , — что тоже является вариантом потокенной регуляризации. мы склоняемся к тому, что научный вклад этой статьи невелик, а ключевые выводы — ошибочны. разбор подготовил ❣ михаил хрущев душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-11-15T10:00:08+00:00" href="./posts/43.html">2024-11-15 10:00 UTC</a></div>
      </div>
      <div class="post-body"><strong>TDPO — потокенный DPO или просто регуляризация?</strong><br><br>Авторы <a href="https://arxiv.org/abs/2404.11999" rel="nofollow noopener noreferrer">сегодняшней статьи</a> предлагают метод потокенного Direct Preference Optimization (DPO), который на бумаге должен исправить некоторые проблемы оффлайн-обучения с подкреплением. Но на деле все оказывается не так просто. <br><br>DPO — метод обучения, не полагающийся на reward-модель. Здесь применяют датасет с размеченными парами запросов и ответов, чтобы натренировать генератор на контрастный лосс. <br><br>Проблема в том, что в случае с DPO мы работаем с вероятностями последовательностей целиком. Метод ограниченно контролирует поведение модели на уровне отдельных токенов. Это приводит к тому, что модель может ошибочно сильно повышать или понижать вероятность отдельных токенов значительно после совершенных ошибок. <br><br>Эту проблему можно нивелировать, если сделать DPO потокенным. Авторы статьи пытаются добиться этого. <br><br>Для начала они предлагают ввести необычное ограничение — сделать так, чтобы сумма наград всех токенов-продолжений для произвольного префикса была равна 0. Это довольно сильное допущение: например, если мы решаем задачу копирования какого-то куска текста, то будем сильно штрафовать модель за любое отклонение. Как результат — награда за правильный токен окажется очень большой. В этом случае, если при выборе между длинной и короткой строкой, модель будет склоняться к длинной строке.<br><br>Такое ограничение позволило авторам в их расчётах лосса избавиться от нормировочной константы вероятностного распределения. Чтобы ее вычислить, нужно суммировать награду по всем возможным ответам, а это невозможно, поэтому от константы при расчётах избавляются. В DPO нормировочная константа одинакова для победившего и проигравшего ответов, поэтому она сокращается в лоссе, но авторы статьи сделали это несколько иначе. <br><br>Из их математической модели выводится функция, которая очень похожа на DPO. Но в отличие от DPO, авторы вычитают из неё разницу между SeqKL проигравшего и победившего ответа. Этот метод, названный Token-level Direct Preference Optimization (TDPO), обеспечил незначительное улучшение по сравнению с обычным DPO. На датасете Anthropic HH точность увеличилась всего на 0,65%. <br><br>Далее авторы предлагают умножить на дополнительный коэффициент разницу SeqKL и не пропускать градиенты для победившего варианта. Это можно трактовать так: при росте SeqKL проигравшего ответа всегда увеличивается лосс, в то время, как при росте SeqKL победившего — лосс уменьшается. Получается, что добавка к DPO, после остановки градиента для её части, по сути работает, как регуляризация.<br><br>С ней метод получил название TDPO2 и он действительно неплохо улучшает показатели. На том же Anthropic HH прирост по сравнению с DPO составил уже не 0,65%, а 7,9%. <br><br>Авторы действительно предложили лучшее решение. Но возникает вопрос: насколько здесь велик вклад выведенной математической модели. По факту, авторы сильно меняют основные моменты в этой модели, а то, что остается, очень похоже на простую потокенную регуляризацию. Но её идея не нова: часто к DPO добавляют negative log likelihood loss — например, <a href="https://arxiv.org/abs/2407.21783" rel="nofollow noopener noreferrer">при DPO-обучении Llama 3.1</a>, — что тоже является вариантом потокенной регуляризации. Мы склоняемся к тому, что научный вклад этой статьи невелик, а ключевые выводы — ошибочны.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Михаил Хрущев</em><br><br><a href="https://t.me/+wr2ycu4eJck1Nzgy" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/43_480.webp" srcset="../assets/media/thumbs/43_480.webp 480w, ../assets/media/43.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="43" data-image-index="0" /></div></div>
      <div class="actions">
        <span>14 331 просмотров · 35 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/43" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/43.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="41" data-search="инфраструктура llama 3.1 продолжаем серию постов о модели meta* рассказом об инфраструктуре. на чём же училась llama? претрейн 405b-модели осуществлялся на 16 тысячах h100, с потреблением электроэнергии в 700 вт каждая. использовалась кастомная платформа с liquid cooling-хостами. у meta есть general-purpose-хранилище на основе tectonicfs. изначально его использовали и для обучения ии, и для других процессов и клиентов. однако создание чекпоинтов оказывало очень большую нагрузку на хранилище. поэтому инженеры создали отдельное хранилище исключительно для тренировок модели. что касается сети, то в meta сделали сразу два кластера: с roce для большой модели и с infiniband для моделей поменьше. в каждой стойке по два хоста, а каждом хосте — по восемь gpu. всего в кластере 24 тысячи gpu, из которых 16 отведены под обучение. внутри каждого из восьми модулей на 3072 gpu максимальная пропускная способность сети. а между модулями — она в семь раз меньше. планировщик распределяет задачи по хостам, тем самым минимизируя сетевые коммуникации между модулями. также используется ncclx — кастомная версия библиотеки для коммуникаций nccl. как показатель эффективности использовали model flops utilization (mfu) — это коэффициент отношения наблюдаемого числа обработанных токенов к теоретическому максимальному числу. он достигает от 38% до 43% — в зависимости от сетапа. подробнее — в таблице выше. о надежности. за 54 дня претрейна случилось 419 непредвиденных остановок — то есть примерно по 8 в день. из строя ежедневно выходило 0,3-0,4% оборудования. статистику по падениям можно посмотреть во второй таблице. боролись с неполадками и предотвращали их с помощью частых чекпоинтов, быстрых рестартов, инструментов для диагностики проблем. кроме того, инженеры — не прерывая обучение — могли менять настройки и уровень логирования. напоминаем, что у нас есть и другие посты о llama 3.1: — о претрейн-датасете; — архитектуре модели; — алайменте. а в канале cv time вы найдёте пост о том, как llama 3.1 работает с изображениями, а также много других интересных разборов и репортажи с профильных конференций. подписывайтесь! душный nlp — meta признана экстремистской организацией, а facebook и instagram запрещены на территории рф инфраструктура llama 3.1 продолжаем серию постов о модели meta* рассказом об инфраструктуре. на чём же училась llama ? претрейн 405b-модели осуществлялся на 16 тысячах h100, с потреблением электроэнергии в 700 вт каждая. использовалась кастомная платформа с liquid cooling-хостами. у meta есть general-purpose-хранилище на основе tectonicfs. изначально его использовали и для обучения ии, и для других процессов и клиентов. однако создание чекпоинтов оказывало очень большую нагрузку на хранилище. поэтому инженеры создали отдельное хранилище исключительно для тренировок модели. что касается сети, то в meta сделали сразу два кластера: с roce для большой модели и с infiniband для моделей поменьше. в каждой стойке по два хоста, а каждом хосте — по восемь gpu. всего в кластере 24 тысячи gpu, из которых 16 отведены под обучение. внутри каждого из восьми модулей на 3072 gpu максимальная пропускная способность сети. а между модулями — она в семь раз меньше. планировщик распределяет задачи по хостам, тем самым минимизируя сетевые коммуникации между модулями. также используется ncclx — кастомная версия библиотеки для коммуникаций nccl. как показатель эффективности использовали model flops utilization (mfu) — это коэффициент отношения наблюдаемого числа обработанных токенов к теоретическому максимальному числу. он достигает от 38% до 43% — в зависимости от сетапа. подробнее — в таблице выше. о надежности. за 54 дня претрейна случилось 419 непредвиденных остановок — то есть примерно по 8 в день. из строя ежедневно выходило 0,3-0,4% оборудования. статистику по падениям можно посмотреть во второй таблице. боролись с неполадками и предотвращали их с помощью частых чекпоинтов, быстрых рестартов, инструментов для диагностики проблем. кроме того, инженеры — не прерывая обучение — могли менять настройки и уровень логирования. напоминаем, что у нас есть и другие посты о llama 3.1: — о претрейн-датасете; — архитектуре модели; — алайменте. а в канале cv time вы найдёте пост о том, как llama 3.1 работает с изображениями , а также много других интересных разборов и репортажи с профильных конференций. подписывайтесь! душный nlp — meta признана экстремистской организацией, а facebook и instagram запрещены на территории рф">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-11-12T09:32:09+00:00" href="./posts/41.html">2024-11-12 09:32 UTC</a></div>
      </div>
      <div class="post-body"><strong>Инфраструктура LLaMA 3.1</strong><br><br>Продолжаем серию постов о модели Meta* рассказом об инфраструктуре. <a href="https://arxiv.org/abs/2407.21783" rel="nofollow noopener noreferrer">На чём же училась LLaMA</a>?<br><br>Претрейн 405B-модели осуществлялся на 16 тысячах H100, с потреблением электроэнергии в 700 Вт каждая. Использовалась кастомная платформа с Liquid Cooling-хостами. <br><br>У Meta есть general-purpose-хранилище на основе TectonicFS. Изначально его использовали и для обучения ИИ, и для других процессов и клиентов. Однако создание чекпоинтов оказывало очень большую нагрузку на хранилище. Поэтому инженеры создали отдельное хранилище исключительно для тренировок модели.<br><br>Что касается сети, то в Meta сделали сразу два кластера: с RoCE для большой модели и с Infiniband для моделей поменьше. В каждой стойке по два хоста, а каждом хосте — по восемь GPU. Всего в кластере 24 тысячи GPU, из которых 16 отведены под обучение. <br><br>Внутри каждого из восьми модулей на 3072 GPU максимальная пропускная способность сети. А между модулями — она в семь раз меньше. Планировщик распределяет задачи по хостам, тем самым минимизируя сетевые коммуникации между модулями. Также используется NCCLX — кастомная версия библиотеки для коммуникаций NCCL.<br><br>Как показатель эффективности использовали Model FLOPS Utilization (MFU) — это коэффициент отношения наблюдаемого числа обработанных токенов к теоретическому максимальному числу. Он достигает от 38% до 43% — в зависимости от сетапа. Подробнее — в таблице выше.<br><br>О надежности. За 54 дня претрейна случилось 419 непредвиденных остановок — то есть примерно по 8 в день. Из строя ежедневно выходило 0,3-0,4% оборудования. Статистику по падениям можно посмотреть во второй таблице. Боролись с неполадками и предотвращали их с помощью частых чекпоинтов, быстрых рестартов, инструментов для диагностики проблем. Кроме того, инженеры — не прерывая обучение — могли менять настройки и уровень логирования.<br><br>Напоминаем, что у нас есть и другие посты о LLaMA 3.1:<br>— <a href="https://t.me/stuffyNLP/37" rel="nofollow noopener noreferrer">о претрейн-датасете;</a><br>— <a href="https://t.me/stuffyNLP/38" rel="nofollow noopener noreferrer">архитектуре модели;</a><br>— <a href="https://t.me/stuffyNLP/40" rel="nofollow noopener noreferrer">алайменте.</a><br><br>А в канале CV Time вы найдёте пост о том, <a href="https://t.me/timeforcv/25" rel="nofollow noopener noreferrer">как LLaMA 3.1 работает с изображениями</a>, а также много других интересных разборов и репортажи с профильных конференций. Подписывайтесь! <br><br><a href="https://t.me/+X5zV194Mg7EwNjYy" rel="nofollow noopener noreferrer">Душный NLP</a><br>—<br><em>Meta признана экстремистской организацией, а Facebook и Instagram запрещены на территории РФ</em><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/41_480.webp" srcset="../assets/media/thumbs/41_480.webp 480w, ../assets/media/41.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="41" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/42_480.webp" srcset="../assets/media/thumbs/42_480.webp 480w, ../assets/media/42.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="41" data-image-index="1" /></div></div>
      <div class="actions">
        <span>5 130 просмотров · 49 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/41" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/41.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="40" data-search="алаймент llama 3.1 возвращаемся к llama 3.1 и продолжаем разбираться, как она устроена. в этот раз речь пойдёт об алайменте модели. по сравнению с llama 2 у третьей версии изменилась разметка пар. помимо стандартных chosen и rejected добавилась ещё метка edited. она ставится в тех случаях, когда победивший объект не слишком хорош и его переписывают. ответы оцениваются по семибалльной шкале. sft происходит в шесть раундов. если в llama 2 использовался ppo, то в llama 3 — dpo. разработчики отмечают, что это связано с тем, что ppo требует больше вычислительных ресурсов, а качество выходит хуже. ещё одно важное отличие — это специализация. на претрейне модель доучивают для решения специальных задач. потом делают отдельный алаймент, полученную специализированную модель используют для генерации новых обучающих данных стадии алайнмента, а также мержат веса нескольких специализированных модель в единую модель. reward-модель обучается над претрейном. margin term, который был в llama 2, в третьей версии отсутствует, так как, по словам разработчиков, он не даёт никакого прироста в качестве. как и в dpo, оставляют только те ответы, которые помечены как «сильно лучше» и «лучше». кроме того, в reward-модели есть отдельные награды для полезности и безопасности. за один раунд sft 405b-модель суммарно проходит 576 тысяч сэмплов. в dpo используют сэмплы от моделей с последних раундов (а в reward-модели — все). служебные токены, такие как eos или токены для вызовов функций, маскируют для стабилизации обучения. кроме того, к dpo добавляют nll (ce) с коэффициентом 0,2. это нужно, чтобы повысить вероятность chosen-ответов. промпты для обучения пишут люди, а ответы — модели. на один промпт выбирают лучший ответ из 10-30 поступивших. в llama 3.1 есть четыре уровня фильтрации данных: 1. rule-based — удаляет дисклеймеры, смайлики и восклицания; 2. quality — качественными считаются 25% ответов с наибольшим скором. кроме того, используется llm-as-judge. ответы оцениваются по трём критериям для обычных данных и двум — для кодинга. ответ считается качественным, если все критерии выполнены. сэмпл попадает в обучение, если хотя бы один из методов показал, что ответ качественный; 3. difficulty — оценивается по числу интентов в запросе: чем их больше, тем сложнее запрос. также модель оценивает сложность по трёхбальной шкале; 4. semdedup — этот метод используется для удаления похожих данных, при отборе отдается предпочтение семплам с максимальным значением quality * difficulty. алаймент для каждой из функциональных возможностей (capabilities) llama 3.1 имеет свои особенности. например, в коде есть много синтетических данных, используется execution feedback и перевод на редкие языки программирования. а для математики берут тексты из претрейна и уже к ним генерируют инстракты. что касается фактологичности, то разработчики не добавляют новых знаний поверх претрейна. для этого модель обучают отвечать только на те вопросы, на которые она может выдать ответ, согласованный с документом из претрейна. а для чувствительных тем, по которым в датасете много некорректной информации, используют ручную разметку. разбор подготовил ❣ алексей зотов душный nlp алаймент llama 3.1 возвращаемся к llama 3.1 и продолжаем разбираться, как она устроена. в этот раз речь пойдёт об алайменте модели. по сравнению с llama 2 у третьей версии изменилась разметка пар. помимо стандартных chosen и rejected добавилась ещё метка edited. она ставится в тех случаях, когда победивший объект не слишком хорош и его переписывают. ответы оцениваются по семибалльной шкале. sft происходит в шесть раундов. если в llama 2 использовался ppo, то в llama 3 — dpo. разработчики отмечают, что это связано с тем, что ppo требует больше вычислительных ресурсов, а качество выходит хуже. ещё одно важное отличие — это специализация. на претрейне модель доучивают для решения специальных задач. потом делают отдельный алаймент, полученную специализированную модель используют для генерации новых обучающих данных стадии алайнмента, а также мержат веса нескольких специализированных модель в единую модель. reward-модель обучается над претрейном. margin term, который был в llama 2, в третьей версии отсутствует, так как, по словам разработчиков, он не даёт никакого прироста в качестве. как и в dpo, оставляют только те ответы, которые помечены как «сильно лучше» и «лучше». кроме того, в reward-модели есть отдельные награды для полезности и безопасности. за один раунд sft 405b-модель суммарно проходит 576 тысяч сэмплов. в dpo используют сэмплы от моделей с последних раундов (а в reward-модели — все). служебные токены, такие как eos или токены для вызовов функций, маскируют для стабилизации обучения. кроме того, к dpo добавляют nll (ce) с коэффициентом 0,2. это нужно, чтобы повысить вероятность chosen-ответов. промпты для обучения пишут люди, а ответы — модели. на один промпт выбирают лучший ответ из 10-30 поступивших. в llama 3.1 есть четыре уровня фильтрации данных: 1. rule-based — удаляет дисклеймеры, смайлики и восклицания; 2. quality — качественными считаются 25% ответов с наибольшим скором. кроме того, используется llm-as-judge. ответы оцениваются по трём критериям для обычных данных и двум — для кодинга. ответ считается качественным, если все критерии выполнены. сэмпл попадает в обучение, если хотя бы один из методов показал, что ответ качественный; 3. difficulty — оценивается по числу интентов в запросе: чем их больше, тем сложнее запрос. также модель оценивает сложность по трёхбальной шкале; 4. semdedup — этот метод используется для удаления похожих данных, при отборе отдается предпочтение семплам с максимальным значением quality * difficulty. алаймент для каждой из функциональных возможностей (capabilities) llama 3.1 имеет свои особенности. например, в коде есть много синтетических данных, используется execution feedback и перевод на редкие языки программирования. а для математики берут тексты из претрейна и уже к ним генерируют инстракты. что касается фактологичности, то разработчики не добавляют новых знаний поверх претрейна. для этого модель обучают отвечать только на те вопросы, на которые она может выдать ответ, согласованный с документом из претрейна. а для чувствительных тем, по которым в датасете много некорректной информации, используют ручную разметку. разбор подготовил ❣ алексей зотов душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-11-01T13:51:48+00:00" href="./posts/40.html">2024-11-01 13:51 UTC</a></div>
      </div>
      <div class="post-body"><strong>Алаймент LlaMA 3.1</strong><br><br>Возвращаемся к LlaMA 3.1 и продолжаем разбираться, как она устроена. В этот раз речь пойдёт <a href="https://arxiv.org/abs/2407.21783" rel="nofollow noopener noreferrer">об алайменте модели.</a> <br><br>По сравнению с LLaMA 2 у третьей версии изменилась разметка пар. Помимо стандартных chosen и rejected добавилась ещё метка edited. Она ставится в тех случаях, когда победивший объект не слишком хорош и его переписывают. Ответы оцениваются по семибалльной шкале.<br><br>SFT происходит в шесть раундов. Если в LLaMA 2 использовался PPO, то в LlaMA 3 — DPO. Разработчики отмечают, что это связано с тем, что PPO требует больше вычислительных ресурсов, а качество выходит хуже.<br><br>Ещё одно важное отличие — это специализация. На претрейне модель доучивают для решения специальных задач. Потом делают отдельный алаймент, полученную специализированную модель используют для генерации новых обучающих данных стадии алайнмента, а также мержат веса нескольких специализированных модель в единую модель.  <br><br>Reward-модель обучается над претрейном. Margin term, который был в Llama 2, в третьей версии отсутствует, так как, по словам разработчиков, он не даёт никакого прироста в качестве. Как и в DPO, оставляют только те ответы, которые помечены как «сильно лучше» и «лучше». Кроме того, в reward-модели есть отдельные награды для полезности и безопасности.<br><br>За один раунд SFT 405B-модель суммарно проходит 576 тысяч сэмплов. В DPO используют сэмплы от моделей с последних раундов (а в reward-модели — все). Служебные токены, такие как EOS или токены для вызовов функций, маскируют для стабилизации обучения. Кроме того, к DPO добавляют NLL (CE) с коэффициентом 0,2. Это нужно, чтобы повысить вероятность chosen-ответов. Промпты для обучения пишут люди, а ответы — модели. На один промпт выбирают лучший ответ из 10-30 поступивших. <br><br>В LlaMA 3.1 есть четыре уровня фильтрации данных:<br><br><strong>1. Rule-based </strong>— удаляет дисклеймеры, смайлики и восклицания;<br><br><strong>2. Quality</strong> — качественными считаются 25% ответов с наибольшим скором. Кроме того, используется LLM-as-judge. Ответы оцениваются по трём критериям для обычных данных и двум — для кодинга. Ответ считается качественным, если все критерии выполнены. Сэмпл попадает в обучение, если хотя бы один из методов показал, что ответ качественный;<br><br><strong>3. Difficulty </strong>— оценивается по числу интентов в запросе: чем их больше, тем сложнее запрос. Также модель оценивает сложность по трёхбальной шкале;<br><br><strong>4. SemDedup</strong> — этот метод используется для удаления похожих данных, при отборе отдается предпочтение семплам с максимальным значением quality * difficulty.<br><br>Алаймент для каждой из функциональных возможностей (Capabilities) LLaMA 3.1 имеет свои особенности. Например, в коде есть много синтетических данных, используется execution feedback и перевод на редкие языки программирования. А для математики берут тексты из претрейна и уже к ним генерируют инстракты.<br><br>Что касается фактологичности, то разработчики не добавляют новых знаний поверх претрейна. Для этого модель обучают отвечать только на те вопросы, на которые она может выдать ответ, согласованный с документом из претрейна. А для чувствительных тем, по которым в датасете много некорректной информации, используют ручную разметку. <br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Алексей Зотов</em><br><br><a href="https://t.me/+rKa608VzQuozODEy" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/40_480.webp" srcset="../assets/media/thumbs/40_480.webp 480w, ../assets/media/40.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="40" data-image-index="0" /></div></div>
      <div class="actions">
        <span>5 372 просмотров · 60 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/40" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/40.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="39" data-search="ограничения instruction tuning и как их преодолеть supervised full fine-tuning (sft) — распространённая практика, но он не лишён недостатков. авторы сегодняшней статьи задаются вопросом: а может ли lora (low-rank adaptation) исправить недочёты? при использовании full fine-tuning возникает две проблемы: у моделей часто возникают сложности с извлечением новых знаний из sft-датасета, могут участиться галлюцинации. исследование показало, что модели, обученные с использованием full fine-tuning, могут генерировать неверные ответы, если берут слишком много токенов из sft-датасетов. эффект особенно заметен, если модель пытается отвечать на вопросы, требующие глубокой экспертизы. например, на вопрос «какие основные работы эйнштейн сделал после того, как в 1915 году открыл общую теорию относительности?» модель начинала выдавать не соответствующие действительности ответы — скажем, о «квантовой теории атома трития». одним из возможных решений может быть lora — это метод, который позволяет обучать модели с гораздо меньшими ресурсами, модифицируя лишь небольшую часть параметров. вместо полного тюнинга всех параметров lora использует специальные низкоранговые матрицы, что приводит к изменениям только определённых аспектов, таких как стиль ответа или инициирование фраз. при этом основная часть весов предобученной модели остаётся неизменной. первые несколько процентов токенов, сгенерированных lora-моделью, могут быть изменены (по сравнению с ответом предобученной модели), чтобы правильно начать ответ. но большая часть предложения остаётся такой же, как у предобученной модели. это позволяет уменьшить количество галлюцинаций. эксперименты показали, что lora даёт более точные ответы. lora эффективен даже при малом объёме датасета. например, модель с lora, обученная на наборе данных из 1000 инструкций, может превосходить модели с sft на датасетах по срезам фактологичености и полезности, содержащих 52 000 или даже 326 000 инструкций. в экспериментах использовались различные открытые и домен-специфичные датасеты, включая medinstruct и alpaca. модели с lora демонстрировали лучшее соответствие фактам и были менее подвержены галлюцинациям. разбор подготовил ❣ алексей шимко душный nlp ограничения instruction tuning и как их преодолеть supervised full fine-tuning (sft) — распространённая практика, но он не лишён недостатков. авторы сегодняшней статьи задаются вопросом: а может ли lora (low-rank adaptation) исправить недочёты? при использовании full fine-tuning возникает две проблемы: у моделей часто возникают сложности с извлечением новых знаний из sft-датасета, могут участиться галлюцинации. исследование показало, что модели, обученные с использованием full fine-tuning, могут генерировать неверные ответы, если берут слишком много токенов из sft-датасетов. эффект особенно заметен, если модель пытается отвечать на вопросы, требующие глубокой экспертизы. например, на вопрос «какие основные работы эйнштейн сделал после того, как в 1915 году открыл общую теорию относительности?» модель начинала выдавать не соответствующие действительности ответы — скажем, о «квантовой теории атома трития». одним из возможных решений может быть lora — это метод, который позволяет обучать модели с гораздо меньшими ресурсами, модифицируя лишь небольшую часть параметров. вместо полного тюнинга всех параметров lora использует специальные низкоранговые матрицы, что приводит к изменениям только определённых аспектов, таких как стиль ответа или инициирование фраз. при этом основная часть весов предобученной модели остаётся неизменной. первые несколько процентов токенов, сгенерированных lora-моделью, могут быть изменены (по сравнению с ответом предобученной модели), чтобы правильно начать ответ. но большая часть предложения остаётся такой же, как у предобученной модели. это позволяет уменьшить количество галлюцинаций. эксперименты показали, что lora даёт более точные ответы. lora эффективен даже при малом объёме датасета. например, модель с lora, обученная на наборе данных из 1000 инструкций, может превосходить модели с sft на датасетах по срезам фактологичености и полезности, содержащих 52 000 или даже 326 000 инструкций. в экспериментах использовались различные открытые и домен-специфичные датасеты, включая medinstruct и alpaca. модели с lora демонстрировали лучшее соответствие фактам и были менее подвержены галлюцинациям. разбор подготовил ❣ алексей шимко душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-10-25T10:48:35+00:00" href="./posts/39.html">2024-10-25 10:48 UTC</a></div>
      </div>
      <div class="post-body"><strong>Ограничения Instruction Tuning и как их преодолеть</strong><br><br>Supervised Full Fine-tuning (SFT) — распространённая практика, но он не лишён недостатков. Авторы <a href="https://arxiv.org/abs/2402.05119" rel="nofollow noopener noreferrer">сегодняшней статьи</a> задаются вопросом: а может ли LoRA (Low-Rank Adaptation) исправить недочёты? <br><br>При использовании Full Fine-tuning возникает две проблемы: у моделей часто возникают сложности с извлечением новых знаний из SFT-датасета, могут участиться галлюцинации. Исследование показало, что модели, обученные с использованием Full Fine-tuning, могут генерировать неверные ответы, если берут слишком много токенов из SFT-датасетов. Эффект особенно заметен, если модель пытается отвечать на вопросы, требующие глубокой экспертизы. <br><br>Например, на вопрос «Какие основные работы Эйнштейн сделал после того, как в 1915 году открыл Общую теорию относительности?» модель начинала выдавать не соответствующие действительности ответы — скажем, о «квантовой теории атома трития».<br><br>Одним из возможных решений может быть LoRA — это метод, который позволяет обучать модели с гораздо меньшими ресурсами, модифицируя лишь небольшую часть параметров. Вместо полного тюнинга всех параметров LoRA использует специальные низкоранговые матрицы, что приводит к изменениям только определённых аспектов, таких как стиль ответа или инициирование фраз. При этом основная часть весов предобученной модели остаётся неизменной.<br><br>Первые несколько процентов токенов, сгенерированных LoRA-моделью, могут быть изменены (по сравнению с ответом предобученной модели), чтобы правильно начать ответ. Но большая часть предложения остаётся такой же, как у предобученной модели. Это позволяет уменьшить количество галлюцинаций. Эксперименты показали, что LoRA даёт более точные ответы.<br><br>LoRA эффективен даже при малом объёме датасета. Например, модель с LoRA, обученная на наборе данных из 1000 инструкций, может превосходить модели с SFT на датасетах по срезам фактологичености и полезности, содержащих 52 000 или даже 326 000 инструкций. В экспериментах использовались различные открытые и домен-специфичные датасеты, включая MedInstruct и Alpaca. Модели с LoRA демонстрировали лучшее соответствие фактам и были менее подвержены галлюцинациям. <br><em><br>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Алексей Шимко</em><br><br><a href="https://t.me/+Ym6fFvXhvu4wMDAy" rel="nofollow noopener noreferrer">Душный NLP</a></div>
      <div class="actions">
        <span>12 180 просмотров · 57 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/39" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/39.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="38" data-search="архитектура llama 3.1 продолжаем разбирать llama 3.1. в прошлый раз речь шла о претрейн-датасете, а в этот раз — об архитектуре модели. llama 3 использует стандартную архитектуру трансформера, которая не сильно отличается от того, что было в llama и llama 2. однако отличия есть. скажем, если в llama 2 grouped query attention (gqa) с восемью ключевыми головами внимания использовались только в моделях на 34b+, то здесь gqa применяется для всех моделей llama 3.1. это позволило повысить скорость вывода и уменьшить объём данных, необходимых для кеширования во время декодирования. ещё одно важное изменение — увеличение контекстного окна до 128 тысяч токенов. это стало возможным благодаря увеличению гиперпараметра базовой частоты rope до 500 тысяч. такой подход позволяет модели эффективно решать задачи, связанные с большими объёмами текстов. модель также использует словарь на 128 тысяч токенов. разработчики внедрили четырёхмерный параллелизм (4d parallelism), который включает тензорный, пайплайновый, контекстный и параллелизм данных. этот подход позволяет значительно улучшить утилизацию ресурсов при обучении на тысячах gpu. например, для обучения модели с 405 миллиардами параметров использовалось до 16 тысяч gpu, а средняя утилизация вычислительных ресурсов составила около 41%​. контекстный параллелизм позволяет разбивать длинные строки на части. в отличие от классических методов, такой параллелизм синхронизирует только ключи и значения в attention-слое, что минимизирует задержки при обработке длинных последовательностей. помимо этого, в архитектуре llama 3.1 активно используется fp8-квантизация, которая значительно ускоряет вычисления без значительных потерь в точности. это позволяет экономить до 50% времени на вычисления по сравнению с традиционными методами, что критично для моделей с миллиардами параметров. fp8-квантизация используется не для всех слоев, потому что она может вызвать ошибки при вычислении в attention-слоях. подход доказал свою эффективность при решении большинства задач​. разбор подготовил ❣ михаил хрущев душный nlp архитектура llama 3.1 продолжаем разбирать llama 3.1. в прошлый раз речь шла о претрейн-датасете , а в этот раз — об архитектуре модели. llama 3 использует стандартную архитектуру трансформера, которая не сильно отличается от того, что было в llama и llama 2. однако отличия есть. скажем, если в llama 2 grouped query attention (gqa) с восемью ключевыми головами внимания использовались только в моделях на 34b+, то здесь gqa применяется для всех моделей llama 3.1. это позволило повысить скорость вывода и уменьшить объём данных, необходимых для кеширования во время декодирования. ещё одно важное изменение — увеличение контекстного окна до 128 тысяч токенов. это стало возможным благодаря увеличению гиперпараметра базовой частоты rope до 500 тысяч. такой подход позволяет модели эффективно решать задачи, связанные с большими объёмами текстов. модель также использует словарь на 128 тысяч токенов. разработчики внедрили четырёхмерный параллелизм (4d parallelism), который включает тензорный, пайплайновый, контекстный и параллелизм данных. этот подход позволяет значительно улучшить утилизацию ресурсов при обучении на тысячах gpu. например, для обучения модели с 405 миллиардами параметров использовалось до 16 тысяч gpu, а средняя утилизация вычислительных ресурсов составила около 41%​. контекстный параллелизм позволяет разбивать длинные строки на части. в отличие от классических методов, такой параллелизм синхронизирует только ключи и значения в attention-слое, что минимизирует задержки при обработке длинных последовательностей. помимо этого, в архитектуре llama 3.1 активно используется fp8-квантизация, которая значительно ускоряет вычисления без значительных потерь в точности. это позволяет экономить до 50% времени на вычисления по сравнению с традиционными методами, что критично для моделей с миллиардами параметров. fp8-квантизация используется не для всех слоев, потому что она может вызвать ошибки при вычислении в attention-слоях. подход доказал свою эффективность при решении большинства задач​. разбор подготовил ❣ михаил хрущев душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-10-18T10:55:22+00:00" href="./posts/38.html">2024-10-18 10:55 UTC</a></div>
      </div>
      <div class="post-body"><strong>Архитектура LLaMA 3.1</strong><br><br>Продолжаем <a href="https://arxiv.org/abs/2407.21783" rel="nofollow noopener noreferrer">разбирать LLaMA 3.1.</a> В прошлый раз речь шла о <a href="https://t.me/stuffyNLP/37" rel="nofollow noopener noreferrer">претрейн-датасете</a>, а в этот раз — об архитектуре модели. <br><br>Llama 3 использует стандартную архитектуру трансформера, которая не сильно отличается от того, что было в LLaMA и LLaMA 2. Однако отличия есть. Скажем, если в LLaMA 2 Grouped Query Attention (GQA) с восемью ключевыми головами внимания использовались только в моделях на 34B+, то здесь GQA применяется для всех моделей LLaMA 3.1. Это позволило повысить скорость вывода и уменьшить объём данных, необходимых для кеширования во время декодирования.<br><br>Ещё одно важное изменение — увеличение контекстного окна до 128 тысяч токенов. Это стало возможным благодаря увеличению гиперпараметра базовой частоты RoPE до 500 тысяч. Такой подход позволяет модели эффективно решать задачи, связанные с большими объёмами текстов. Модель также использует словарь на 128 тысяч токенов.<br><br>Разработчики внедрили четырёхмерный параллелизм (4D Parallelism), который включает тензорный, пайплайновый, контекстный и параллелизм данных. Этот подход позволяет значительно улучшить утилизацию ресурсов при обучении на тысячах GPU. Например, для обучения модели с 405 миллиардами параметров использовалось до 16 тысяч GPU, а средняя утилизация вычислительных ресурсов составила около 41%​.<br><br>Контекстный параллелизм позволяет разбивать длинные строки на части. В отличие от классических методов, такой параллелизм синхронизирует только ключи и значения в attention-слое, что минимизирует задержки при обработке длинных последовательностей.<br><br>Помимо этого, в архитектуре LLaMA 3.1 активно используется FP8-квантизация, которая значительно ускоряет вычисления без значительных потерь в точности. Это позволяет экономить до 50% времени на вычисления по сравнению с традиционными методами, что критично для моделей с миллиардами параметров. FP8-квантизация используется не для всех слоев, потому что она может вызвать ошибки при вычислении в attention-слоях. Подход доказал свою эффективность при решении большинства задач​.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Михаил Хрущев</em><br><br><a href="https://t.me/+yAHD2sNNZUw0MTRi" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/38_480.webp" srcset="../assets/media/thumbs/38_480.webp 480w, ../assets/media/38.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="38" data-image-index="0" /></div></div>
      <div class="actions">
        <span>5 516 просмотров · 81 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/38" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/38.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="37" data-search="о претрейн-датасете llama 3.1 сегодня расскажем о том, как устроен претрейн-датасет для llama 3.1. разработчики этой llm поделились множеством подробностей, поэтому будет интересно. одна из важнейших характеристик llama 3.1 — объём данных, на которых она обучалась. для этой модели использовался претрейн-датасет из 15 триллионов токенов. это один из самых больших наборов данных для языковых моделей, превосходящий, например, llama 2 с её 2 триллионами токенов. модель также включает специальный набор данных для длинных контекстов. сбор данных для llama 3.1 построен на стандартном пайплайне для обработки текста из html. один из его ключевых аспектов — это фильтрация данных на ранних стадиях. она включает как дедупликацию данных, так и использование эвристических методов для удаления нежелательных токенов. кроме того, команда разработчиков использовала кастомные парсеры для специфических доменов, таких как математика и программирование. это позволило улучшить качество обработки данных для узкоспециализированных задач. кастомный парсер способен сохранить точное форматирование математических формул, тогда как обычные парсеры теряют важную информацию. llama 3.1 демонстрирует хорошие результаты благодаря специальным пайплайнам для математических задач и задач, связанных с программированием. объем данных по коду и математике в датасете llama 3.1 превосходит весь датасет оригинальной llama. для оценки качества данных использовались различные классификаторы, полученные дистилляцией llama 2. другие классификаторы отбирали документы, опираясь на вероятность их принадлежности к определённому домену. один из интересных моментов — использование классификаторов для оценки доменов с целью дальнейшего перевзвешивания различных частей датасета. например, данные из области искусства и развлечений могут быть уменьшены в весе по сравнению с математикой. фильтрация включает несколько уровней дедупликации. во-первых, стандартная дедупликация с использованием метода minhash. есть также локальная дедупликация — метод, при котором данные разбиваются на блоки, и дубликаты удаляются только внутри этих блоков. ещё одной важной частью фильтрации данных стала проверка на безопасность. разработчики llama 3.1 реализовали специальные фильтры для удаления вредоносного или небезопасного контента. эти фильтры основаны на классификаторах и могут удалять не только нежелательные токены, но и данные с персональной информацией. llama 3.1 также выделяется своими экспериментами в области data mixer — процесса, в котором данные разделяются по доменам, а затем перевзвешиваются для оптимальной работы модели. важным этапом оптимизации стал так называемое «сведение». это метод, при котором на последних этапах обучения модель дообучается на малом наборе данных, включающем специфичные задачи. это позволяет значительно повысить качество ответов модели на конечных тестах. в ходе экспериментов установили, что даже небольшой объём высококачественных данных может существенно улучшить результаты. разбор подготовил ❣ дмитрий мокеев душный nlp о претрейн-датасете llama 3.1 сегодня расскажем о том, как устроен претрейн-датасет для llama 3.1 . разработчики этой llm поделились множеством подробностей, поэтому будет интересно. одна из важнейших характеристик llama 3.1 — объём данных, на которых она обучалась. для этой модели использовался претрейн-датасет из 15 триллионов токенов. это один из самых больших наборов данных для языковых моделей, превосходящий, например, llama 2 с её 2 триллионами токенов. модель также включает специальный набор данных для длинных контекстов. сбор данных для llama 3.1 построен на стандартном пайплайне для обработки текста из html. один из его ключевых аспектов — это фильтрация данных на ранних стадиях. она включает как дедупликацию данных, так и использование эвристических методов для удаления нежелательных токенов. кроме того, команда разработчиков использовала кастомные парсеры для специфических доменов, таких как математика и программирование. это позволило улучшить качество обработки данных для узкоспециализированных задач. кастомный парсер способен сохранить точное форматирование математических формул, тогда как обычные парсеры теряют важную информацию. llama 3.1 демонстрирует хорошие результаты благодаря специальным пайплайнам для математических задач и задач, связанных с программированием. объем данных по коду и математике в датасете llama 3.1 превосходит весь датасет оригинальной llama. для оценки качества данных использовались различные классификаторы, полученные дистилляцией llama 2. другие классификаторы отбирали документы, опираясь на вероятность их принадлежности к определённому домену. один из интересных моментов — использование классификаторов для оценки доменов с целью дальнейшего перевзвешивания различных частей датасета. например, данные из области искусства и развлечений могут быть уменьшены в весе по сравнению с математикой. фильтрация включает несколько уровней дедупликации. во-первых, стандартная дедупликация с использованием метода minhash. есть также локальная дедупликация — метод, при котором данные разбиваются на блоки, и дубликаты удаляются только внутри этих блоков. ещё одной важной частью фильтрации данных стала проверка на безопасность. разработчики llama 3.1 реализовали специальные фильтры для удаления вредоносного или небезопасного контента. эти фильтры основаны на классификаторах и могут удалять не только нежелательные токены, но и данные с персональной информацией. llama 3.1 также выделяется своими экспериментами в области data mixer — процесса, в котором данные разделяются по доменам, а затем перевзвешиваются для оптимальной работы модели. важным этапом оптимизации стал так называемое «сведение». это метод, при котором на последних этапах обучения модель дообучается на малом наборе данных, включающем специфичные задачи. это позволяет значительно повысить качество ответов модели на конечных тестах. в ходе экспериментов установили, что даже небольшой объём высококачественных данных может существенно улучшить результаты. разбор подготовил ❣ дмитрий мокеев душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-10-11T13:15:58+00:00" href="./posts/37.html">2024-10-11 13:15 UTC</a></div>
      </div>
      <div class="post-body"><strong>О претрейн-датасете LLaMA 3.1</strong><br><br>Сегодня расскажем о том, как устроен <a href="https://arxiv.org/abs/2407.21783" rel="nofollow noopener noreferrer">претрейн-датасет для LLaMA 3.1</a>. Разработчики этой LLM поделились множеством подробностей, поэтому будет интересно. <br><br>Одна из важнейших характеристик LLaMA 3.1 — объём данных, на которых она обучалась. Для этой модели использовался претрейн-датасет из 15 триллионов токенов. Это один из самых больших наборов данных для языковых моделей, превосходящий, например, LLaMA 2 с её 2 триллионами токенов. Модель также включает специальный набор данных для длинных контекстов.<br><br>Сбор данных для LLaMA 3.1 построен на стандартном пайплайне для обработки текста из HTML. Один из его ключевых аспектов — это фильтрация данных на ранних стадиях. Она включает как дедупликацию данных, так и использование эвристических методов для удаления нежелательных токенов. <br><br>Кроме того, команда разработчиков использовала кастомные парсеры для специфических доменов, таких как математика и программирование. Это позволило улучшить качество обработки данных для узкоспециализированных задач. Кастомный парсер способен сохранить точное форматирование математических формул, тогда как обычные парсеры теряют важную информацию.<br><br>LLaMA 3.1 демонстрирует хорошие результаты благодаря специальным пайплайнам для математических задач и задач, связанных с программированием. Объем данных по коду и математике в датасете LLaMA 3.1 превосходит весь датасет оригинальной LLaMA.<br><br>Для оценки качества данных использовались различные классификаторы, полученные дистилляцией LLaMA 2. Другие классификаторы отбирали документы, опираясь на вероятность их принадлежности к определённому домену. Один из интересных моментов — использование классификаторов для оценки доменов с целью дальнейшего перевзвешивания различных частей датасета. Например, данные из области искусства и развлечений могут быть уменьшены в весе по сравнению с математикой.<br><br>Фильтрация включает несколько уровней дедупликации. Во-первых, стандартная дедупликация с использованием метода MinHash. Есть также локальная дедупликация — метод, при котором данные разбиваются на блоки, и дубликаты удаляются только внутри этих блоков. <br><br>Ещё одной важной частью фильтрации данных стала проверка на безопасность. Разработчики LLaMA 3.1 реализовали специальные фильтры для удаления вредоносного или небезопасного контента. Эти фильтры основаны на классификаторах и могут удалять не только нежелательные токены, но и данные с персональной информацией.<br><br>LLaMA 3.1 также выделяется своими экспериментами в области Data Mixer — процесса, в котором данные разделяются по доменам, а затем перевзвешиваются для оптимальной работы модели. <br><br>Важным этапом оптимизации стал так называемое «сведение». Это метод, при котором на последних этапах обучения модель дообучается на малом наборе данных, включающем специфичные задачи. Это позволяет значительно повысить качество ответов модели на конечных тестах. В ходе экспериментов установили, что даже небольшой объём высококачественных данных может существенно улучшить результаты.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Дмитрий Мокеев</em><br><br><a href="https://t.me/+ufd0OK8SBSE3ZmFi" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/37_480.webp" srcset="../assets/media/thumbs/37_480.webp 480w, ../assets/media/37.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="37" data-image-index="0" /></div></div>
      <div class="actions">
        <span>6 136 просмотров · 63 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/37" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/37.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="36" data-search="remax как альтернатива ppo сегодняшняя статья — об интересном методе обучения с подкреплением, который называется remax. авторы предлагают его как замену популярному алгоритму proximal policy optimization (ppo). remax основывается на алгоритме обучения с подкреплением, который называется reinforce — отсюда и приставка re. в reinforce, в отличие от ppo, нет value-модели. она выступает в роли бейзлайна для снижения дисперсии оценки градиента и представляет собой среднюю награду, которую наберёт генератор, если будет отвечать на конкретный запрос. вместо отдельной value-модели в remax предлагают использовать другой бейзлайн — то, сколько награды набирает greedy-генерация обучаемой моделью на запросе. отсюда окончание max. такой бейзлайн тоже отлично подходит с точки зрения теории, и не требует хранения дополнительной модели в памяти — авторы сообщают, что снижение потребления gpu-памяти составляет 46%. а поскольку число моделей, необходимых алгоритму алайнмента, уменьшилось, то уменьшилось и число гиперпараметров, которые нужно подобрать. использование remax для обучения модели mistral-7b показало значительные улучшения. модель достигла 94,78% успеха на leaderboard alpacaeval и установила новый стандарт для моделей с 7 миллиардами параметров. эти результаты демонстрируют, что remax может стать отличной альтернативой ppo для rlhf-задач, значительно сокращая вычислительные затраты и повышая эффективность обучения крупных языковых моделей. разбор подготовил ❣ павел темирчев душный nlp remax как альтернатива ppo сегодняшняя статья — об интересном методе обучения с подкреплением, который называется remax. авторы предлагают его как замену популярному алгоритму proximal policy optimization (ppo). remax основывается на алгоритме обучения с подкреплением, который называется reinforce — отсюда и приставка re. в reinforce, в отличие от ppo, нет value-модели. она выступает в роли бейзлайна для снижения дисперсии оценки градиента и представляет собой среднюю награду, которую наберёт генератор, если будет отвечать на конкретный запрос. вместо отдельной value-модели в remax предлагают использовать другой бейзлайн — то, сколько награды набирает greedy-генерация обучаемой моделью на запросе. отсюда окончание max. такой бейзлайн тоже отлично подходит с точки зрения теории, и не требует хранения дополнительной модели в памяти — авторы сообщают, что снижение потребления gpu-памяти составляет 46%. а поскольку число моделей, необходимых алгоритму алайнмента, уменьшилось, то уменьшилось и число гиперпараметров, которые нужно подобрать. использование remax для обучения модели mistral-7b показало значительные улучшения. модель достигла 94,78% успеха на leaderboard alpacaeval и установила новый стандарт для моделей с 7 миллиардами параметров. эти результаты демонстрируют, что remax может стать отличной альтернативой ppo для rlhf-задач, значительно сокращая вычислительные затраты и повышая эффективность обучения крупных языковых моделей. разбор подготовил ❣ павел темирчев душный nlp">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-10-04T08:57:19+00:00" href="./posts/36.html">2024-10-04 08:57 UTC</a></div>
      </div>
      <div class="post-body"><strong>ReMax как альтернатива PPO</strong><br><br>Сегодняшняя статья — об интересном методе обучения с подкреплением, <a href="https://arxiv.org/abs/2310.10505" rel="nofollow noopener noreferrer">который называется ReMax.</a> Авторы предлагают его как замену популярному алгоритму Proximal Policy Optimization (PPO).<br><br>ReMax основывается на алгоритме обучения с подкреплением, который называется REINFORCE — отсюда и приставка Re. В REINFORCE, в отличие от PPO, нет value-модели. Она выступает в роли бейзлайна для снижения дисперсии оценки градиента и представляет собой среднюю награду, которую наберёт генератор, если будет отвечать на конкретный запрос.<br><br>Вместо отдельной value-модели в ReMax предлагают использовать другой бейзлайн — то, сколько награды набирает greedy-генерация обучаемой моделью на запросе. Отсюда окончание Max. Такой бейзлайн тоже отлично подходит с точки зрения теории, и не требует хранения дополнительной модели в памяти — авторы сообщают, что снижение потребления GPU-памяти составляет 46%. А поскольку число моделей, необходимых алгоритму алайнмента, уменьшилось, то уменьшилось и число гиперпараметров, которые нужно подобрать.<br><br>Использование ReMax для обучения модели Mistral-7B показало значительные улучшения. Модель достигла 94,78% успеха на leaderboard AlpacaEval и установила новый стандарт для моделей с 7 миллиардами параметров. Эти результаты демонстрируют, что ReMax может стать отличной альтернативой PPO для RLHF-задач, значительно сокращая вычислительные затраты и повышая эффективность обучения крупных языковых моделей.<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Павел Темирчев</em><br><br><a href="https://t.me/+HJcT-kMS_sYwNDJi" rel="nofollow noopener noreferrer">Душный NLP</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/36_480.webp" srcset="../assets/media/thumbs/36_480.webp 480w, ../assets/media/36.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="36" data-image-index="0" /></div></div>
      <div class="actions">
        <span>12 750 просмотров · 60 реакций</span>
        <span class="action-links"><a href="https://t.me/stuffyNLP/36" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/36.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    </div>
    
    <div class="pager static-pager" style="justify-content:center">
      <div class="page-links">
        <a class="nav-link" href="page-2.html">←</a>
        <a class="page-link" href="index.html">1</a> <a class="page-link" href="page-2.html">2</a> <a class="page-link current" href="page-3.html">3</a> <a class="page-link" href="page-4.html">4</a>
        <a class="nav-link" href="page-4.html">→</a>
      </div>
    </div>
    
  </main>

  <footer class="footer">
    <div class="container">
      <div class="footer-inner">
        <span>based on <a href="https://github.com/ml-brand/tg-to-gh-pages" target="_blank" rel="noopener">tg-to-gh-pages</a> (created by <a href="https://github.com/ml-brand" target="_blank" rel="noopener">ML Brand</a>)</span>
        <a id="repoLink" href="https://github.com/ml-brand/tg-to-gh-pages" target="_blank" rel="noopener">Do the same with your channel.</a>
        <span class="footer-links">
          static copy ·
          <a href="../feed.xml" target="_blank" rel="noopener">RSS</a> ·
          <a href="../atom.xml" target="_blank" rel="noopener">Atom</a>
        </span>
      </div>
    </div>
  </footer>

  <script>
    window.__STATIC_POSTS = [{"id": 85, "media": [{"kind": "photo", "path": "../assets/media/85.jpg", "thumb": "../assets/media/thumbs/85_480.webp", "size": 144180, "mime": "image/jpeg", "name": null}]}, {"id": 84, "media": []}, {"id": 81, "media": [{"kind": "photo", "path": "../assets/media/81.jpg", "thumb": "../assets/media/thumbs/81_480.webp", "size": 99274, "mime": "image/jpeg", "name": null}]}, {"id": 79, "media": []}, {"id": 78, "media": [{"kind": "photo", "path": "../assets/media/78.jpg", "thumb": "../assets/media/thumbs/78_480.webp", "size": 73381, "mime": "image/jpeg", "name": null}]}, {"id": 77, "media": []}, {"id": 75, "media": [{"kind": "photo", "path": "../assets/media/75.jpg", "thumb": "../assets/media/thumbs/75_480.webp", "size": 101151, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/76.jpg", "thumb": "../assets/media/thumbs/76_480.webp", "size": 109199, "mime": "image/jpeg", "name": null}]}, {"id": 72, "media": [{"kind": "photo", "path": "../assets/media/72.jpg", "thumb": "../assets/media/thumbs/72_480.webp", "size": 178094, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/73.jpg", "thumb": "../assets/media/thumbs/73_480.webp", "size": 92990, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/74.jpg", "thumb": "../assets/media/thumbs/74_480.webp", "size": 41482, "mime": "image/jpeg", "name": null}]}, {"id": 68, "media": [{"kind": "photo", "path": "../assets/media/68.jpg", "thumb": "../assets/media/thumbs/68_480.webp", "size": 95737, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/69.jpg", "thumb": "../assets/media/thumbs/69_480.webp", "size": 186177, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/70.jpg", "thumb": "../assets/media/thumbs/70_480.webp", "size": 188588, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/71.jpg", "thumb": "../assets/media/thumbs/71_480.webp", "size": 162038, "mime": "image/jpeg", "name": null}]}, {"id": 65, "media": [{"kind": "photo", "path": "../assets/media/65.jpg", "thumb": "../assets/media/thumbs/65_480.webp", "size": 110556, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/66.jpg", "thumb": "../assets/media/thumbs/66_480.webp", "size": 73484, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/67.jpg", "thumb": "../assets/media/thumbs/67_480.webp", "size": 211681, "mime": "image/jpeg", "name": null}]}, {"id": 64, "media": [{"kind": "photo", "path": "../assets/media/64.jpg", "thumb": "../assets/media/thumbs/64_480.webp", "size": 162859, "mime": "image/jpeg", "name": null}]}, {"id": 63, "media": []}, {"id": 61, "media": [{"kind": "photo", "path": "../assets/media/61.jpg", "thumb": "../assets/media/thumbs/61_480.webp", "size": 39008, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/62.jpg", "thumb": "../assets/media/thumbs/62_480.webp", "size": 27955, "mime": "image/jpeg", "name": null}]}, {"id": 60, "media": []}, {"id": 59, "media": [{"kind": "photo", "path": "../assets/media/59.jpg", "thumb": "../assets/media/thumbs/59_480.webp", "size": 150066, "mime": "image/jpeg", "name": null}]}, {"id": 58, "media": []}, {"id": 53, "media": [{"kind": "photo", "path": "../assets/media/53.jpg", "thumb": "../assets/media/thumbs/53_480.webp", "size": 76623, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/54.jpg", "thumb": "../assets/media/thumbs/54_480.webp", "size": 134591, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/55.jpg", "thumb": "../assets/media/thumbs/55_480.webp", "size": 114870, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/56.jpg", "thumb": "../assets/media/thumbs/56_480.webp", "size": 103976, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/57.jpg", "thumb": "../assets/media/thumbs/57_480.webp", "size": 153349, "mime": "image/jpeg", "name": null}]}, {"id": 52, "media": [{"kind": "photo", "path": "../assets/media/52.jpg", "thumb": "../assets/media/thumbs/52_480.webp", "size": 150010, "mime": "image/jpeg", "name": null}]}, {"id": 51, "media": [{"kind": "photo", "path": "../assets/media/51.jpg", "thumb": "../assets/media/thumbs/51_480.webp", "size": 83334, "mime": "image/jpeg", "name": null}]}, {"id": 49, "media": [{"kind": "video", "path": "../assets/media/49_IMG_0737.MOV.mov", "thumb": null, "size": 1420293, "mime": "video/quicktime", "name": "IMG_0737.MOV"}, {"kind": "video", "path": "../assets/media/50_video_2024-12-11_14-25-32.mp4", "thumb": null, "size": 1977479, "mime": "video/mp4", "name": "video_2024-12-11_14-25-32.mp4"}]}, {"id": 46, "media": [{"kind": "photo", "path": "../assets/media/46.jpg", "thumb": "../assets/media/thumbs/46_480.webp", "size": 15205, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/47.jpg", "thumb": "../assets/media/thumbs/47_480.webp", "size": 71696, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/48.jpg", "thumb": "../assets/media/thumbs/48_480.webp", "size": 27212, "mime": "image/jpeg", "name": null}]}, {"id": 45, "media": [{"kind": "photo", "path": "../assets/media/45.jpg", "thumb": "../assets/media/thumbs/45_480.webp", "size": 193695, "mime": "image/jpeg", "name": null}]}, {"id": 44, "media": [{"kind": "photo", "path": "../assets/media/44.jpg", "thumb": "../assets/media/thumbs/44_480.webp", "size": 86104, "mime": "image/jpeg", "name": null}]}, {"id": 43, "media": [{"kind": "photo", "path": "../assets/media/43.jpg", "thumb": "../assets/media/thumbs/43_480.webp", "size": 120981, "mime": "image/jpeg", "name": null}]}, {"id": 41, "media": [{"kind": "photo", "path": "../assets/media/41.jpg", "thumb": "../assets/media/thumbs/41_480.webp", "size": 34337, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/42.jpg", "thumb": "../assets/media/thumbs/42_480.webp", "size": 109767, "mime": "image/jpeg", "name": null}]}, {"id": 40, "media": [{"kind": "photo", "path": "../assets/media/40.jpg", "thumb": "../assets/media/thumbs/40_480.webp", "size": 44632, "mime": "image/jpeg", "name": null}]}, {"id": 39, "media": []}, {"id": 38, "media": [{"kind": "photo", "path": "../assets/media/38.jpg", "thumb": "../assets/media/thumbs/38_480.webp", "size": 79323, "mime": "image/jpeg", "name": null}]}, {"id": 37, "media": [{"kind": "photo", "path": "../assets/media/37.jpg", "thumb": "../assets/media/thumbs/37_480.webp", "size": 33633, "mime": "image/jpeg", "name": null}]}, {"id": 36, "media": [{"kind": "photo", "path": "../assets/media/36.jpg", "thumb": "../assets/media/thumbs/36_480.webp", "size": 66201, "mime": "image/jpeg", "name": null}]}];
    window.__STATIC_META = {"title": "Душный NLP", "username": "stuffyNLP", "channel": "stuffyNLP", "last_sync_utc": "2026-01-30T20:00:29Z", "posts_count": 112, "last_seen_message_id": 222, "stats": {"new": 104, "updated": 2, "media_downloaded": 104}, "avatar": "assets/channel_avatar.jpg", "meta_schema_version": "1.0.0", "posts_schema_version": "1.0.0"};
  </script>
  <script src="../common.js"></script>
  <script src="../static.js"></script>
</body>
</html>
